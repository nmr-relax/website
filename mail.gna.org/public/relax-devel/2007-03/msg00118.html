<!-- MHonArc v2.6.10 -->
<!--X-Subject: Re: relax, MPI, and Grid computing. -->
<!--X-From-R13: "tnel gubzcfba" <tnelg.naq.fnenuoNtznvy.pbz> -->
<!--X-Date: Wed, 21 Mar 2007 23:27:49 +0100 -->
<!--X-Message-Id: f001463a0703211527s52c17f0kabf2c01e91f7ae5a@mail.gmail.com -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: 7f080ed10703191030h79036e06w56912aa1d9130f48@mail.gmail.com -->
<!--X-Reference: 45FF0E84.1060007@bmb.leeds.ac.uk -->
<!--X-Reference: 1174384147.29205.20.camel@pc172&#45;31&#45;2&#45;63.biochem.unimelb.edu.au -->
<!--X-Reference: 45FFEC5C.7060205@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703200727l5fad8e72if4cf9aff74bc21@mail.gmail.com -->
<!--X-Reference: 45FFF714.7070903@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703200813x323ec212l471796641855a7ff@mail.gmail.com -->
<!--X-Reference: 45FFFDAF.8010402@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703210524tba3838ekd44b2d5cc66c41c@mail.gmail.com -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: relax, MPI, and Grid computing. -- March 21, 2007 - 23:27</title>
<link rel="stylesheet" type="text/css" href="/mail.gna.org/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="/mail.gna.org/images/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: relax, MPI, and Grid computing.</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00118" class="tabs">Index by Date</a> | <a href="threads.html#00118" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00117.html">Date Prev</a>] [<a href="msg00119.html">Date Next</a>] [<a href="msg00116.html">Thread Prev</a>] [<a href="msg00124.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: &quot;Edward d'Auvergne&quot; &lt;edward@xxxxxxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Date</em>: Wed, 21 Mar 2007 22:27:13 +0000</li>
<li class="menuitem">
<em>Cc</em>: relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Dkim-signature</em>: a=rsa-sha1; c=relaxed/relaxed; d=gmail.com; s=beta;	h=domainkey-signature:received:received:message-id:date:from:to:subject:cc:in-reply-to:mime-version:content-type:content-transfer-encoding:content-disposition:references;	b=XEdjcI4eaRwGFSrQoM9s9ivV0RNdiMgKRfNMpbO8/AHHOF5yhVSwxviYzTJDrlTk2UpA7yy22DI8zaRRzmcOJwU4TxHmSYk1XEI0MBk1apPd1XnxWx0LKwhnyZn60RgcneADcoIngacF/+Po5YVGo5oab+oyHWaQodVp2kgHeGQ=</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00118.html">f001463a0703211527s52c17f0kabf2c01e91f7ae5a@mail.gmail.com</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;<a href="msg00092.html">7f080ed10703191030h79036e06w56912aa1d9130f48@mail.gmail.com</a>&gt;	&lt;<a href="msg00093.html">45FF0E84.1060007@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00097.html">1174384147.29205.20.camel@pc172-31-2-63.biochem.unimelb.edu.au</a>&gt;	&lt;<a href="msg00107.html">45FFEC5C.7060205@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00110.html">7f080ed10703200727l5fad8e72if4cf9aff74bc21@mail.gmail.com</a>&gt;	&lt;<a href="msg00112.html">45FFF714.7070903@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00114.html">7f080ed10703200813x323ec212l471796641855a7ff@mail.gmail.com</a>&gt;	&lt;<a href="msg00115.html">45FFFDAF.8010402@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00116.html">7f080ed10703210524tba3838ekd44b2d5cc66c41c@mail.gmail.com</a>&gt;</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>gary thompson</strong> on March 21, 2007 - 23:27:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<tt>On 3/21/07, Edward d'Auvergne &lt;edward@xxxxxxxxxxxxx&gt; wrote:
</tt><blockquote class="blockquote"><pre style="margin: 0em;">On 3/21/07, Gary S. Thompson &lt;garyt@xxxxxxxxxxxxxxx&gt; wrote:
&gt; Edward d'Auvergne wrote:
&gt;
&gt; &gt;&gt; &gt;&gt; &gt;&gt; my main thought was to
&gt; &gt;&gt; &gt;&gt; &gt;&gt;effectivley add restrictions to a some commands. So consider grid
&gt; &gt;&gt; &gt;&gt; search
&gt; &gt;&gt; &gt;&gt; &gt;&gt;I would add an extra parameter at the generic  and functional
&gt; &gt;&gt; levels
&gt; &gt;&gt; &gt;&gt; &gt;&gt;which would give a range of steps within the current parameters to
&gt; &gt;&gt; &gt;&gt; &gt;&gt;calculate.... e.g here are the ranges which give a grid of
&gt; &gt;&gt; 10x10x10 ie
&gt; &gt;&gt; &gt;&gt; &gt;&gt;1000 steps. slave 1. you calculate 1-250 slave 2. 251-500 and so
&gt; &gt;&gt; &gt;&gt; on.....
&gt; &gt;&gt; &gt;&gt; &gt;&gt;is this the correct way to go?
&gt; &gt;&gt; &gt;&gt; &gt;&gt;
&gt; &gt;&gt; &gt;&gt; &gt;&gt;
&gt; &gt;&gt; &gt;&gt; &gt;
&gt; &gt;&gt; &gt;&gt; &gt;Subdividing the grid search will be an interesting problem!
&gt; &gt;&gt; Should it
&gt; &gt;&gt; &gt;&gt; &gt;be at the 'generic_fns' level, the 'specific_fns' level, or
&gt; &gt;&gt; implemented
&gt; &gt;&gt; &gt;&gt; &gt;directly into the minimisation package?  I think that the
&gt; &gt;&gt; &gt;&gt; 'specific_fns'
&gt; &gt;&gt; &gt;&gt; &gt;level, again within the 'minimise()' model-free method (copied,
&gt; &gt;&gt; &gt;&gt; modified
&gt; &gt;&gt; &gt;&gt; &gt;for MPI, and renamed to 'minimise_mpi()') would be the best place to
&gt; &gt;&gt; &gt;&gt; &gt;target.
&gt; &gt;&gt; &gt;&gt; &gt;
&gt; &gt;&gt; &gt;&gt; &gt;An algorithm to subdivide the grid would be useful.  Then an
&gt; &gt;&gt; algorithm
&gt; &gt;&gt; &gt;&gt; &gt;to collect the results and determine which subspace of the grid
&gt; &gt;&gt; has the
&gt; &gt;&gt; &gt;&gt; &gt;point with the lowest chi2 value should be used.  I.e. this will
&gt; &gt;&gt; be an
&gt; &gt;&gt; &gt;&gt; &gt;MPI-oriented grid search over a number of standard grid searches.
&gt; &gt;&gt; &gt;&gt; &gt;
&gt; &gt;&gt; &gt;&gt; &gt;However your best MPI gains are likely to be achieved by sending
&gt; &gt;&gt; each
&gt; &gt;&gt; &gt;&gt; &gt;grid search to a different node.  This higher level would be shared
&gt; &gt;&gt; &gt;&gt; with
&gt; &gt;&gt; &gt;&gt; &gt;the standard model-free optimisation code and hence you don't
&gt; &gt;&gt; need to
&gt; &gt;&gt; &gt;&gt; &gt;worry about writing separate MPI code for the grid search and for
&gt; &gt;&gt; the
&gt; &gt;&gt; &gt;&gt; &gt;minimisation.  Slight improvements may be achieved by breaking up
&gt; &gt;&gt; the
&gt; &gt;&gt; &gt;&gt; &gt;grid search, but I would personally tackle this later on.
&gt; &gt;&gt; &gt;&gt; &gt;
&gt; &gt;&gt; &gt;&gt; &gt;
&gt; &gt;&gt; &gt;&gt; &gt;
&gt; &gt;&gt; &gt;&gt;
&gt; &gt;&gt; &gt;&gt; again I need to think about this. However if this uses divisons by
&gt; &gt;&gt; model
&gt; &gt;&gt; &gt;&gt; again it will perform poorly as the different models will take
&gt; &gt;&gt; different
&gt; &gt;&gt; &gt;&gt; amounts of time to calculate so many processors will sit idle...
&gt; &gt;&gt; &gt;&gt; again if I am not undertsanding properly please accept my apologies
&gt; &gt;&gt; &gt;&gt; relax is very heavily layered and alot of names are repeated multiple
&gt; &gt;&gt; &gt;&gt; times it can be quite had to follow whatis going on in the code
&gt; &gt;&gt; base ;-)
&gt; &gt;&gt; &gt;
&gt; &gt;&gt; &gt;
&gt; &gt;&gt; &gt; Idle time will be inevitable.  Especially when it comes to the 'all'
&gt; &gt;&gt; &gt; model-free minimisation instance (the optimisation of all model-free
&gt; &gt;&gt; &gt; model parameters for all residues together with all diffusion
&gt; &gt;&gt; &gt; parameters).  That cannot be avoided.
&gt; &gt;&gt; &gt;
&gt; &gt;&gt; I agree especially if you are going for the low hanging fruit. In the
&gt; &gt;&gt; case of the all minimisation it might be possible to  parallelise it
&gt; &gt;&gt; but the code would have to work at a much lower level and many messages
&gt; &gt;&gt; would have to be passed. Whether it would work or not would depend on
&gt; &gt;&gt; how frequently nodes had to update data between themselves.  Certainly
&gt; &gt;&gt; it would only work well on a homogenous system or a fairly well balanced
&gt; &gt;&gt; hetrogenous system (i.e. you would have to know the relative speed of
&gt; &gt;&gt; each slave and weight the amount of work it did correctly)
&gt; &gt;
&gt; &gt;
&gt; &gt; By definition, minimisation algorithms are a serial process.  They
&gt; &gt; walk downhill through the space using information from the previous
&gt; &gt; and current positions.  I can't see how the 'all' model-free
&gt; &gt; minimisation instance can ever be parallelised - it will take a while
&gt; &gt; no matter what.  Unless a proven parallelised optimisation algorithm
&gt; &gt; is coded from scratch.  And even then extensive testing would be
&gt; &gt; required to validate the algorithm within the model-free space.  Where
&gt; &gt; we will get improvements with the 'all' parameter set is in the
&gt; &gt; parallelisation of the Monte Carlo simulations!
&gt;
&gt; oh definitley and also grid searches for anisotropic systems can be
&gt; quite slow</pre><br>
<pre style="margin: 0em;">The grid search for the 'diff' model - the diffusion tensor with the
model-free parameters fixed - can take a while.  That step would
benefit from the grid within a grid parallelisation approach.</pre><br>
</blockquote><pre style="margin: 0em;"><br>yes that is what Chris and I thought, unless i am misunderstanding him</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br>&gt; &gt; The parallelisation
&gt; &gt; of the processing of data pipes (the old 'runs') would most likely
&gt; &gt; give the least improvements.
&gt; &gt;
&gt; indeed ;-) It would be the last thing to do....</pre><br>
<pre style="margin: 0em;">Actually, this parallelisation would actually be quite beneficial when
optimising the 'diff' model (especially if you are running the
different diffusion model analyses simultaneously).  For your work
with optimising MPI, it might be worth profiling the execution of the
'full_analysis.py' script for the full model-free analysis.  Not real
profiling but roughly measuring the time it takes to complete various
stages of the analysis.  A graph or plot of the results, with and
without MPI, could be useful for determining which points are
bottlenecks and which steps can be improved by additional
parallelisation.  What do you think?</pre><br>
</blockquote><tt><br>yes that would be a good idea as well (one to add to the list ;-)
</tt><blockquote class="blockquote"><pre style="margin: 0em;">Bye,</pre><br>
<pre style="margin: 0em;">Edward</pre><br>
<pre style="margin: 0em;">_______________________________________________
relax (<a  href="http://nmr-relax.com">http://nmr-relax.com</a>)</pre><br>
<pre style="margin: 0em;">This is the relax-devel mailing list
relax-devel@xxxxxxx</pre><br>
<pre style="margin: 0em;">To unsubscribe from this list, get a password
reminder, or change your subscription options,
visit the list information page at
<a  href="https://mail.gna.org/listinfo/relax-devel">https://mail.gna.org/listinfo/relax-devel</a></pre><br>
</blockquote><pre style="margin: 0em;"><br></pre><br>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="00092" href="msg00092.html">relax and Grid computing.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00093" href="msg00093.html">Re: relax and Grid computing.</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00097" href="msg00097.html">relax, MPI, and Grid computing.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00107" href="msg00107.html">Re: relax, MPI, and Grid computing.</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00110" href="msg00110.html">Re: relax, MPI, and Grid computing.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00112" href="msg00112.html">Re: relax, MPI, and Grid computing.</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00114" href="msg00114.html">Re: relax, MPI, and Grid computing.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00115" href="msg00115.html">Re: relax, MPI, and Grid computing.</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00116" href="msg00116.html">Re: relax, MPI, and Grid computing.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer"></div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Fri Mar 23 16:00:45 2007</div>  
</body>
</html>
