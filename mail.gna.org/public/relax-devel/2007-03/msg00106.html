<!-- MHonArc v2.6.10 -->
<!--X-Subject: Re: r3237 &#45; in /branches/multi_processor: multi/mpi4py_processor.py	relax -->
<!--X-From-R13: "Unel E. Fubzcfba" <tnelgNozo.yrrqf.np.hx> -->
<!--X-Date: Tue, 20 Mar 2007 14:59:08 +0100 -->
<!--X-Message-Id: 45FFE881.6050703@bmb.leeds.ac.uk -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: E1HTJY1&#45;0005VF&#45;O1@subversion.gna.org -->
<!--X-Reference: 45FEAC46.8020307@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703190923s4b09fab1vd958ed6001031b9c@mail.gmail.com -->
<!--X-Reference: 45FF1AF7.2000606@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703200306q376f2120va9ab5f5fa49ec6d3@mail.gmail.com -->
<!--X-Reference: 45FFC71E.9040102@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703200601h1a9f94atd1281cc2c82254b7@mail.gmail.com -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax -- March 20, 2007 - 14:59</title>
<link rel="stylesheet" type="text/css" href="/mail.gna.org/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="/mail.gna.org/images/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00106" class="tabs">Index by Date</a> | <a href="threads.html#00106" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00105.html">Date Prev</a>] [<a href="msg00107.html">Date Next</a>] [<a href="msg00103.html">Thread Prev</a>] [<a href="msg00108.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: Edward d'Auvergne &lt;edward@xxxxxxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Date</em>: Tue, 20 Mar 2007 13:58:25 +0000</li>
<li class="menuitem">
<em>Cc</em>: relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00106.html">45FFE881.6050703@bmb.leeds.ac.uk</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;E1HTJY1-0005VF-O1@subversion.gna.org&gt;	&lt;<a href="msg00088.html">45FEAC46.8020307@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00091.html">7f080ed10703190923s4b09fab1vd958ed6001031b9c@mail.gmail.com</a>&gt;	&lt;<a href="msg00094.html">45FF1AF7.2000606@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00098.html">7f080ed10703200306q376f2120va9ab5f5fa49ec6d3@mail.gmail.com</a>&gt;	&lt;<a href="msg00101.html">45FFC71E.9040102@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00103.html">7f080ed10703200601h1a9f94atd1281cc2c82254b7@mail.gmail.com</a>&gt;</li>
<li class="menuitem">
<em>User-agent</em>: Mozilla Thunderbird 1.0 (X11/20041206)</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Gary S. Thompson</strong> on March 20, 2007 - 14:59:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre style="margin: 0em;">Edward d'Auvergne wrote:</pre><br>
<blockquote class="blockquote"><blockquote class="blockquote"><tt>&gt;&gt; &gt; As MPI<br>
&gt;&gt; &gt; is solely for those who are very serious and have access to 
clusters,<br>
&gt;&gt;<br>
&gt;&gt; not true! mpi can run over ssh as well. For example lam has an ssh<br>
&gt;&gt; backend and this is what i am using for testing on my computer!<br>
&gt;<br>
&gt;<br>
&gt; MPI can be used for grid computing but that is not what it is designed<br>
&gt; for and hence isn't optimal</tt><br>
<br>
<tt>why?
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">That what all the documentation I've read about MPI's limitations have
said.  By quickly checking on Wikipedia
(<a  href="http://en.wikipedia.org/wiki/Message_Passing_Interface">http://en.wikipedia.org/wiki/Message_Passing_Interface</a>), I just found
the following text:</pre><br>
<pre style="margin: 0em;">4. Grid computing, and virtual grid computing offer MPIs way of
handling static and dynamic process management with particular 'fits'.
While it is possible for force the MPI model into working on a grid,
the idea of a fault-free, long-running virtual machine under the MPI
program is a forced on in a grid environment. Grids may want to
instantiate MPI APIs between sets of running processes, but
multi-level middleware that addresses concurrency, faults, and message
traffic are needed. Fault tolerant MPI's and Grid MPIs have been
attempted, but the original design of MPI itself impacts what can be
done.
</pre></blockquote><tt><br>Clearly there are problems if you expect nodes on you cluster to die 
regularily then maybe not the one for you ;-) Though I haven't looked at 
mpi 2 The other thing to bear in mind is that description they give here 
for grid computing is considerably more sophisticated than is present in 
the threads model at present (and may even be overkill)</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br>My personal experience with the coding of the grid computing code, I
would assume that there are a number of differences.  For example the
algorithm relax currently uses to handle computers of different
speeds.
</pre></blockquote><tt><br>This should not make a difference you just divide more finely and weight 
the size of the job by computer (you can even send off the finer grained 
task one at a time and then send new ones as tasks are completed... the 
same way as you do in the thread code)</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">Grid computing is designed for a heterogeneous environment
whereas MPI is not.  I'm not saying one is superior to the other but
that they have different applications in different computing
environments.
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">mpi can cope with a hetrogeneous environment as explained above</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br>Cheers,</pre><br>
<pre style="margin: 0em;">Edward</pre><br>
<pre style="margin: 0em;">.</pre><br>
</blockquote><tt>Anyway we don't need to worry to much on whixh implimentation is best if 
we use a generic interface as proposed, swap and chnage what you use as 
you need it ;-)<br>
regards<br>
gary</tt><br>
<br>
<pre style="margin: 0em;">--
-------------------------------------------------------------------
Dr Gary Thompson
Astbury Centre for Structural Molecular Biology,
University of Leeds, Astbury Building,
Leeds, LS2 9JT, West-Yorkshire, UK             Tel. +44-113-3433024
email: garyt@xxxxxxxxxxxxxxx                   Fax  +44-113-2331407
-------------------------------------------------------------------</pre><br>
<pre style="margin: 0em;"><br></pre><br>
<br>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="00108" href="msg00108.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="00088" href="msg00088.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00091" href="msg00091.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00094" href="msg00094.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00098" href="msg00098.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00101" href="msg00101.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00103" href="msg00103.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer"></div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Tue Mar 20 15:20:37 2007</div>  
</body>
</html>
