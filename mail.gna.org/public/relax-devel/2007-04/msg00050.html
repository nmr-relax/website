<!-- MHonArc v2.6.16 -->
<!--X-Subject: Re: multi processor scaling -->
<!--X-From-R13: "Unel E. Fubzcfba" <tnelgNozo.yrrqf.np.hx> -->
<!--X-Date: Fri, 20 Apr 2007 18:06:30 +0200 -->
<!--X-Message-Id: 4628E4E1.9030302@bmb.leeds.ac.uk -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: 46288BD3.8060502@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10704200814o27d25a81x711502bf3c1d5dab@mail.gmail.com -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: multi processor scaling -- April 20, 2007 - 18:06</title>
<link rel="stylesheet" type="text/css" href="/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="https://gna.org/images/gna.theme/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: multi processor scaling</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00050" class="tabs">Index by Date</a> | <a href="threads.html#00050" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00049.html">Date Prev</a>] [<a href="msg00051.html">Date Next</a>] [<a href="msg00049.html">Thread Prev</a>] [<a href="msg00051.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: Edward d'Auvergne &lt;edward.dauvergne@xxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Date</em>: Fri, 20 Apr 2007 17:05:53 +0100</li>
<li class="menuitem">
<em>Cc</em>: relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00050.html">4628E4E1.9030302@bmb.leeds.ac.uk</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;46288BD3.8060502@xxxxxxxxxxxxxxx&gt;	&lt;7f080ed10704200814o27d25a81x711502bf3c1d5dab@xxxxxxxxxxxxxx&gt;</li>
<li class="menuitem">
<em>User-agent</em>: Mozilla Thunderbird 1.0 (X11/20041206)</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Gary S. Thompson</strong> on April 20, 2007 - 18:06:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre style="margin: 0em;">
Edward d'Auvergne wrote:

</pre><blockquote class="blockquote"><pre style="margin: 0em;">
The scaling is looking awesome!  Obviously the MC sims will need work
and other tests will be required.  But the functionality of the branch
is looking very promising and exciting.  I have more responses below.

Please expect to have delayed responses to the previous messages.  I
will respond to your posts Gary, but I'm three days away from leaving
</pre><tt>Australia and am flat out organising and packing.  
</tt></blockquote><pre style="margin: 0em;">


</pre><tt>I wasn't really expecting a reply for a while I know what it's like 
</tt><tt>leaving a continent (I haven't done it myself but have watched lots of 
</tt><tt>other people do it ;-)
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><pre style="margin: 0em;">
I'll then be
spending a week in London before heading to Germany.
</pre></blockquote><pre style="margin: 0em;">


</pre><tt>Ironically you and chris won't meet, but we should arrange for you to 
</tt><tt>come and visit Leeds sometime Steve (Homans) would be quite keen
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><pre style="margin: 0em;">
It could be a
few weeks before I'll be able to properly response to posts.
</pre></blockquote><pre style="margin: 0em;">


No problem

</pre><blockquote class="blockquote"><pre style="margin: 0em;">


On 4/20/07, Gary S. Thompson &lt;garyt@xxxxxxxxxxxxxxx&gt; wrote:

</pre><blockquote class="blockquote"><pre style="margin: 0em;">

 Dear All
</pre><tt>     I have now had a chance to do some true multi tasking on our 
</tt><tt>local cluster with real overhead from intreprocess communication and 
</tt><tt>the results are as follows
</tt><pre style="margin: 0em;">

 processors        min        eff    mc    eff    grid    eff
 1        18    100    80    100    134    100
 2        9    100
 4        5    90
 8        3    75
 16        1    112.5
 32        1    56.25    8    31.25    4    104.6


 and the picture that speaks 1000 words



 key top graph black line achieved runtimes
</pre><tt>         top graph red line expected runtimes with perfect scaling 
</tt><tt>efficency
</tt><pre style="margin: 0em;">
         bottom graph scaling efficiency

 some notes


</pre><tt> 0. data was collected on one of chris's small data sets containing 
</tt><tt>28 residues not all of which are active for minimisation columns
</tt><pre style="margin: 0em;">
         processors     - no slave  mpi processors
</pre><tt>         min                    - time for a minimisation of models 
</tt><tt>m1-m9 with a fixed diffusion tensor
</tt><tt>         eff                     - approximate parallel efficiency 
</tt><tt>expected runtime/ actual runtime
</tt></blockquote><pre style="margin: 0em;">


It would be interesting to see if the efficiencies all converge to
100% when a larger number of spin systems are minimised.  Maybe
duplicating the data a number of times creating an artificially large
protein would be useful in that regard.
</pre></blockquote><pre style="margin: 0em;">


good idea

</pre><blockquote class="blockquote"><pre style="margin: 0em;">


</pre><blockquote class="blockquote"><pre style="margin: 0em;">
         mc                     - 256 monte carlo calculations
         eff                     - efficiency of the above
</pre><tt>         grid                   - a grid search on a anisotropic 
</tt><tt>diffusion tensor 6 steps
</tt></blockquote><pre style="margin: 0em;">


Do you mean the spheroid (axially symmetric) or the ellipsoid, as both
</pre><tt>are anisotropic?  
</tt></blockquote><pre style="margin: 0em;">


fully anisotropic

</pre><blockquote class="blockquote"><pre style="margin: 0em;">
I would recommend increasing the number of steps in
this grid search if MPI is running.
</pre></blockquote><pre style="margin: 0em;">

yes thats true but this number of steps is good for testing ;-)

</pre><blockquote class="blockquote"><pre style="margin: 0em;">
With that type of scaling
efficiency, I would recommend 11 or 21 increments per dimension on a
32 processor cluster.  A drop from 134 min to 4 min is huge!
</pre></blockquote><pre style="margin: 0em;">

</pre><tt>indeed (I am only allowed to use 40 processors but may try and sneak in 
</tt><tt>a large run when I really go for some scaling measurements)
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><pre style="margin: 0em;">


</pre><blockquote class="blockquote"><pre style="margin: 0em;">
         eff                     - efficency of the above
</pre><tt>      tests were run on a cluster of opterons using gigabit ethernet 
</tt><tt>and mpi
</tt><tt> 1. these results are crude wall times as measured by pythons 
</tt><tt>time.time function for the master but they do not include startup and 
</tt><tt>shutdown overhead
</tt></blockquote><pre style="margin: 0em;">


They should be more than accurate enough for these types of comparison.

</pre></blockquote><pre style="margin: 0em;">
I agree but I just wanted to know what methodolgy I am using

</pre><blockquote class="blockquote"><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><pre style="margin: 0em;">
 2. these tests are single point measurements there are no statistics
</pre></blockquote><pre style="margin: 0em;">


For now statistics are unnecessary.


</pre><blockquote class="blockquote"><tt> 3. timings were rounded to 1 second, so for example we must consider 
</tt><tt>data points for  more than 16 processors for the min run to be suspect
</tt></blockquote><pre style="margin: 0em;">


That would explain the 56% efficiency for 32 processors.

</pre></blockquote><pre style="margin: 0em;">

indeed

</pre><blockquote class="blockquote"><pre style="margin: 0em;">


</pre><blockquote class="blockquote"><pre style="margin: 0em;">
 The results also highlight up some interesting considerations

</pre><tt> 1 our local cluster has very poor disk io, with the result that when 
</tt><tt>i first ran the calculations I saw no multiprocessor imporvements on 
</tt><tt>the min run (in actual fact it got worse!) I got round this for this 
</tt><tt>crude test by switching off virtually all text output from the 
</tt><tt>various minimisation commands. Now obviously this isn't a long term 
</tt><tt>solution but I can thing of other methods e.g  using an output thread 
</tt><tt>thread on the master or output batching that would improve these 
</tt><tt>results.
</tt></blockquote><pre style="margin: 0em;">


Both options sound good.  This type of threading is not very
complicated, although debugging blocked threads is hell.  Sending the
minimisation print out in one hit at the end would be very useful as
well.

</pre></blockquote><pre style="margin: 0em;">
indeed

</pre><blockquote class="blockquote"><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><tt> 2. comparison of the results from the grid calculation and the other 
</tt><tt>calculations are quite informative. clearly the grid results are 
</tt><tt>excellent. I believe this is because I am returning individual 
</tt><tt>subtask results to the master as they complete and the  resulting 
</tt><tt>overhead due to waiting for the master is a problem. To make this 
</tt><tt>(clearer?) here is an example: in  the case of the mc run I will take 
</tt><tt>the 256 mc runs and distribute a batch of 8 to each processor (in the 
</tt><tt>case of a 32 processor run) I then resturn the results individually 
</tt><tt>as they complete I believe this can lead to access to the master 
</tt><tt>being the bottleneck (this is most probably due to output ovrehead on 
</tt><tt>stdout   again, though problems with contention due to coherence of 
</tt><tt>the calculation length could also be a problem ).
</tt></blockquote><pre style="margin: 0em;">


In the Monte Carlo simulations, all of the output of the minimisations
is suppressed.  Therefore the sending of the minimisation print out
shouldn't be the issue as nothing needs to be sent.  There must be
something else at play.  Finding out what this is exactly is important
before an investment into the threading or batching is made.

</pre></blockquote><pre style="margin: 0em;">
that true I will have to look

</pre><blockquote class="blockquote"><blockquote class="blockquote"><tt>In the case of the grid there are no subtasks  as the grid is almost 
</tt><tt>ideally sub divided by processor so only one task is run on each slave.
</tt></blockquote><pre style="margin: 0em;">


Do the MC sims have the same scaling efficiency if only one simulation
is sent to each processor at once?  Does the efficiency increase or
decrease?

don't know

</pre><blockquote class="blockquote"><tt>I can see at least two answers to this. One is to batch the return of 
</tt><tt>results so all results get returned at once and the second is to have 
</tt><tt>an output thread on  the master separate from the thread ervicing mpi 
</tt><tt>calls so processing of  returned data doesn't block the master and 
</tt><tt>thus the rest of the cluster.
</tt></blockquote><pre style="margin: 0em;">


As I mentioned above, both would be useful.  However it would be good
to know which will be the most beneficial for increasing efficiency
before implementation.  It could be that the one or the other will not
result in any significant improvements.

</pre></blockquote><tt>I can impliment both as tests fairly quickly and that maybe easier than 
</tt><tt>trying to figure it out mentally or with tracing
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><tt> I have some more comments to follow on the design of the current 
</tt><tt>minimisation interface, how text output from the commands is 
</tt><tt>controlled, and unit testing but these will have to follow in another 
</tt><tt>message later on
</tt><pre style="margin: 0em;">


 regards
 gary

 n.b. if the pciture doesn't dsiplay well my apolergi
</pre></blockquote><pre style="margin: 0em;">


</pre><tt>The picture at 
</tt><tt><a  rel="nofollow" href="https://mail.gna.org/public/relax-devel/2007-04/msg00048.html">https://mail.gna.org/public/relax-devel/2007-04/msg00048.html</a>
</tt><pre style="margin: 0em;">
displays perfectly.  I've been thinking about how you could release an
MPI relax version prior to the merging of a patch into the 1.3 line.
You could release a relax-1.3.0-gt version (gt for Gary Thompson).
This could itself have a few versions associated with it (I don't know
how they would be called though).  What do you think Gary?
</pre></blockquote><pre style="margin: 0em;">



sounds good to me

</pre><blockquote class="blockquote"><pre style="margin: 0em;">

Cheers,

Edward

.

</pre></blockquote><pre style="margin: 0em;">


--
-------------------------------------------------------------------
Dr Gary Thompson
Astbury Centre for Structural Molecular Biology,
University of Leeds, Astbury Building,
Leeds, LS2 9JT, West-Yorkshire, UK             Tel. +44-113-3433024
email: garyt@xxxxxxxxxxxxxxx                   Fax  +44-113-2331407
-------------------------------------------------------------------




</pre>
<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="00048" href="msg00048.html">multi processor scaling</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00049" href="msg00049.html">Re: multi processor scaling</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer">You are on the <a href="http://gna.org">Gna!</a> mail server.</div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Tue Apr 24 11:41:02 2007</div>  
</body>
</html>
