<!-- MHonArc v2.6.16 -->
<!--X-Subject: The multi&#45;processor branch. -->
<!--X-From-R13: "tnel gubzcfba" <tnelg.naq.fnenuoNtznvy.pbz> -->
<!--X-Date: Tue, 29 May 2007 16:15:50 +0200 -->
<!--X-Message-Id: f001463a0705290715x20e7d355y5c677f6480f637ee@mail.gmail.com -->
<!--X-Content-Type: multipart/alternative -->
<!--X-Reference: E1HjkOC&#45;0007zq&#45;7e@subversion.gna.org -->
<!--X-Reference: 463B2E31.30503@bmb.leeds.ac.uk -->
<!--X-Reference: 1178838900.6484.88.camel@localhost -->
<!--X-Reference: 464D5F8B.3010009@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10705290122m275f385fx82450871973657a5@mail.gmail.com -->
<!--X-Reference: f001463a0705290340k16f1ba3eudcca0c1f5639cf3b@mail.gmail.com -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>The multi-processor branch. -- May 29, 2007 - 16:15</title>
<link rel="stylesheet" type="text/css" href="/mail.gna.org/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="/mail.gna.org/images/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />The multi-processor branch.</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00033" class="tabs">Index by Date</a> | <a href="threads.html#00033" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00032.html">Date Prev</a>] [<a href="msg00034.html">Date Next</a>] [<a href="msg00032.html">Thread Prev</a>] [<a href="msg00034.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Date</em>: Tue, 29 May 2007 15:15:14 +0100</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00033.html">f001463a0705290715x20e7d355y5c677f6480f637ee@mail.gmail.com</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;E1HjkOC-0007zq-7e@xxxxxxxxxxxxxxxxxx&gt;	&lt;463B2E31.30503@xxxxxxxxxxxxxxx&gt; &lt;<a href="msg00017.html">1178838900.6484.88.camel@localhost</a>&gt;	&lt;464D5F8B.3010009@xxxxxxxxxxxxxxx&gt;	&lt;7f080ed10705290122m275f385fx82450871973657a5@xxxxxxxxxxxxxx&gt;	&lt;f001463a0705290340k16f1ba3eudcca0c1f5639cf3b@xxxxxxxxxxxxxx&gt;</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>gary thompson</strong> on May 29, 2007 - 16:15:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
Hi Ed<br><br><div><span class="q"><span class="gmail_quote">On 5/29/07, <b class="gmail_sendername">Edward d&#39;Auvergne</b> &lt;<a rel="nofollow" href="mailto:edward.dauvergne@xxxxxxxxx" target="_blank" >

edward.dauvergne@xxxxxxxxx</a>&gt; wrote:</span><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
On 5/18/07, Gary S. Thompson &lt;<a rel="nofollow" href="mailto:garyt@xxxxxxxxxxxxxxx" target="_blank" >garyt@xxxxxxxxxxxxxxx</a>&gt; wrote:<br>&gt; Edward d&#39;Auvergne wrote:<br>

&gt;<br>&gt; &gt;On Fri, 2007-05-04 at 13:59 +0100, Gary S. Thompson wrote:
<br><br>[snip]<br><br>&gt; &gt;If using &#39;-np 6&#39;, shouldn&#39;t the number of slaves be 6?<br>&gt; &gt;<br>&gt; nope there needs to be one processor which is the master and you just<br>&gt; tell&nbsp;&nbsp;mpi how many prcoessors you want ( I will investigate running jobs
<br>&gt; on the master in a thread at some point (maybe never depending ;-)) but<br>&gt; this places extra requirements on the mpi implimentation and is thus a<br>&gt; special case, I can give more details if you want me to)
<br><br>I would personally avoid running one of the calculations on the master. </blockquote><div><br><br>&nbsp;</div><br></span>In general yes I agree, however, if you have a small cluster this is quite a waste<span class="q">
<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<br><br>&gt; &gt;&gt;when running under the threaded and mpi4py implimentations you may see
<br>&gt; &gt;&gt;long gaps with no output and the output to the terminal can be quite<br>&gt; &gt;&gt;&#39;jerky&#39;. This is because the multiprcoessor implimentation uses a<br>&gt; &gt;&gt;threaded output queue to decouple the writing of output on the master
<br>&gt; &gt;&gt;from the queuing of calculations on the slaves, as otherwise for<br>&gt; &gt;&gt;systems with slow io the rate of io on the mastewr can control the<br>&gt; &gt;&gt;rate of calculation!<br>&gt; &gt;&gt;<br>


&gt; &gt;&gt;<br>&gt; &gt;<br>&gt; &gt;I&#39;ll have to test this later and see if I can cosmetically minimise the<br>&gt; &gt;jerkyness.<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt;<br>&gt; Your can&#39;t! Well ok you can but there are &#39;implications&#39;. The jerkyness
<br>&gt; is intrinsic to the&nbsp;&nbsp;batching up of results from Slave_commands, so you<br>&gt; can switch off the batching of results from the Slave_processors but<br>&gt; this will put more stress on ther master and the interprocessor
<br>&gt; communication fabric. If you want to return string results one line at a<br>&gt; time the design also allows you to do this, but again you stress the<br>&gt; master processor and the interprocessor communication fabric so possibly
<br>&gt; slowing the overall calculation. Note also that what works well for a<br>&gt; computer with fast interprcoess interconnects will not work well on a<br>&gt; computer with a slow communication fabric. Anyway the message overall is
<br>&gt; if you block/slow the master you can end up slowing the whole<br>&gt; multiprocessor....<br><br>I was thinking of storing stdout and stderr in a buffer on the master<br>and having a thread handle the screen output. 
</blockquote><div><br><br>&nbsp;</div><br></span>Already done! <br>The output is already stored in a queue which is processed by a&nbsp; thread.&nbsp; The jerkyness here is because each individual slave_command&nbsp; stores its output and sends it to the master when it finishes. Thus we don&#39;t send individual lines just results of calculations and iostream output from calculations
<span class="q"><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><br><br>&gt; &gt;&gt;also note the std error stream is not currently used as race
<br>&gt; &gt;&gt;conditions between writing to the&nbsp;&nbsp;stderr and stdout streams can lead
<br>&gt; &gt;&gt;to garbled output.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;<br>&gt; &gt;This will definitely need to be fixed prior to merging into the 1.3<br>&gt; &gt;line.&nbsp;&nbsp;Stdout and stderr separation is quite important.
<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt;<br>&gt; Indeed this is true and what I intend to do is to reintroduce an output<br>&gt; streem that splits output on the master based on what the lines prefix<br>&gt; is. This is all down to efficiency again, i vcould retuirn each line of
<br>&gt; text as it is output to the output stream on the slave, however, so<br>&gt; there are not lots of objects to send between&nbsp;&nbsp;processors I join the<br>&gt; streams together with the tags for identification of where the lien came
<br>&gt; from.. The intention was to give the user the choice to split them again<br>&gt; at the other&nbsp;&nbsp;end but, I still haven&#39;t had time to write that code.<br><br>It&#39;s a pity MPI doesn&#39;t have the ability to keep the streams separate.
</blockquote></span><div><br><br>this isn&#39;t really an mpi thing mpi just runs code and doesn&#39;t deal with stdout and stderr in a consistent manner<br></div><span class="q"><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">


 Maybe it would be better if the slave keep its output in a buffer and<br>then sent it all at once at the end.</blockquote></span><div><br>Thats what it does, but if I save stderr and stdout in separate streams then the context of the error stream is lost so I bang them together with some tags ;-) and then send them
over to the master. the intention is to write a &#39;de-multiplexer&#39; at some point that sends things to the right stream in the correct order... However, this wasn&#39;t a priority at the time but good error logs were!
<br></div><span class="q"><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">&gt; &gt;&gt;Now some caveats<br>&gt; &gt;&gt;1. not all exceptions can be handled by this mechanism as they
<br>&gt; &gt;&gt;exceptions can only be handed back once communication between the<br>&gt; &gt;&gt;slaves has been setup. This can be a problem on some mpi<br>&gt; &gt;&gt;implimentations as they don&#39;t provide redirection of stdout back to
<br>&gt; &gt;&gt;the master contolling trerminal.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;<br>&gt; &gt;There&#39;s probably not much that can be done there.<br>&gt; &gt;<br>&gt; &gt;<br>&gt;<br>&gt; yes what I am looking at is putting output to a file one per processor
<br>&gt; in this case<br>&gt;<br>&gt;&nbsp;&nbsp;(this won&#39;t work in all cases as some clusters don&#39;t have disk storage?)<br><br>How about storing stdout and stderr in buffers in memory?</blockquote></span><div><br>yes that fine but I then can&#39;t get the data back to the master as this is a situation where our mpi communication&nbsp; has failed... Anyway in general it&#39;s an edge case most clusters are saner than this 
<br></div><span class="q"><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">&gt; &gt;&gt;2. I have had a few cases where raising an exception has wedged the
<br>&gt; &gt;&gt;whole multiproessor without any output. These can be quite hard to<br>&gt; &gt;&gt;debug as they are due to errors in the overrides I put on the io<br>&gt; &gt;&gt;streams! a pointer that may help is that&nbsp;&nbsp;using the
<br>&gt; &gt;&gt;sys.settrace(traceit)&nbsp;&nbsp;as shown in processor.py will produce copious<br>&gt; &gt;&gt;output tracing&nbsp;&nbsp;(and a very slow program)<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;<br>&gt; &gt;The sorting out and separation of the IO streams may cause this problem
<br>&gt; &gt;to disappear.<br>&gt; &gt;<br>&gt; &gt;<br>&gt; nope this may be&nbsp;&nbsp;to do with exceptions being thrown on remote<br>&gt; proceessors and the master processor waiing infinitley long for<br>&gt; communication from dea processors...
<br><br>Ah, this will be a significant issue in a grid computing setup where a<br>person in the building may turn their computer, which is part of the<br>grid, off!&nbsp;&nbsp;Fault tolerance is what made my old threading code such a
<br>nightmare.</blockquote></span><div><br><br>indeed fault tolerance is a problem and requires a rather different architecture , and anyway&nbsp; in most case&nbsp; clusters are pretty stable. However, I think this was just a bug which i have patched but it could always come back (I was really just warning people that it could happen and to lookout for it)
<br></div><div><span class="e" id="q_112d76c7a2149bab_13"><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">&gt; &gt;&gt;in future it maybe possible also parallelise the minimisation of
<br>
&gt; &gt;&gt;modelfree calculations of the &#39;all&#39; case where model fitting and the<br>&gt; &gt;&gt;tensor frame are optimised at the same time. However,this will require<br>&gt; &gt;&gt;modifications to the model free hessian gradient and cuntion
<br>&gt; &gt;&gt;calculation routines and development of a parallel newton line seach<br>&gt; &gt;&gt;which are both major undertakings.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;<br>&gt; &gt;These are possible targets for parallelisation but I would very strongly
<br>&gt; &gt;recommend against working at this position.&nbsp;&nbsp;And adding optimisation<br>&gt; &gt;algorithms would require very careful testing.&nbsp;&nbsp;From my experience with<br>&gt; &gt;optimisation in the model-free space, I would probably bet that the
<br>&gt; &gt;algorithm will fail for certain model-free motions (not many algorithms<br>&gt; &gt;find all minima in such a convoluted space).&nbsp;&nbsp;The place to target is the<br>&gt; &gt;following three functions:<br>&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; maths_fns.mf.Mf.func_all()
<br>&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; maths_fns.mf.Mf.dfunc_all()<br>&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; maths_fns.mf.Mf.d2func_all()<br>&gt; &gt;<br>&gt; &gt;Specifically the loop over all residues (to be renamed to all spin<br>&gt; &gt;systems in the 1.3 line) to create the value, gradient, and Hessian
<br>&gt; &gt;would be the ideal spot to parallelise!<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt;<br>&gt; indeed this is what I thought and neil is working on it. One side note is that (if such a thing existed) a line search which is&nbsp;&nbsp;adventitous and looked a superset of newton positions would work ;-) again it would have to be tested but all such things have to betested to some degree
<br><br>See Chapter 4 of my PhD thesis at<br><a rel="nofollow" href="http://eprints.infodiv.unimelb.edu.au/archive/00002799/" target="_blank" >http://eprints.infodiv.unimelb.edu.au/archive/00002799/
</a> to see why<br>almost all local optimisation algorithms fail in the model-free space.
<br> Out of 31 algorithms tested, only two worked sufficiently for<br>model-free analysis - Newton optimisation with the Backtraking step<br>length algorithm and GMW81 Hessian modification, and simplex<br>optimisation (which was slower).&nbsp;&nbsp;Both required constraints using the
<br>Method of Multipliers algorithm, also known as the Augmented<br>Lagrangian.</blockquote></span></div><div><br>indeed it would obviously be best to work at something that was a parallel version of  one of these rather than something else  
<br></div><span class="q"><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">&gt; note are tests for the cases for where lm failed in the test suite for
<br>

&gt; relax?<br><br>lm failed?&nbsp;&nbsp;The model-free optimisation system tests are points where<br>some algorithms struggle.</blockquote></span><div><br>sorry awful typing The question was: are their test cases for the&nbsp; data that caused the&nbsp; non newton minimisations to fail in the test suite or unit test suites
<br></div><span class="q"><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">&gt; &gt;&gt; Indeed the problem may be fine grained enough that use of c mpi and
<br>&gt; &gt;&gt;recoding of the hessian etc calculations for model free in c is
<br>&gt; &gt;&gt;required<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;<br>&gt; &gt;This conversion should significantly speed up calculations anyway.&nbsp;&nbsp;I<br>&gt; &gt;will do this one day.<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt;
<br>&gt; &gt;<br>&gt; the later we do this the better c is a bind. I still think pyrex which<br>&gt; compiles what almost lloks like python to c woould be a good thing to<br>&gt; look at ;-) I might try an prototype something for you to look at at
<br>&gt; some point<br><br>I don&#39;t like pyrex at all.&nbsp;&nbsp;I have already tried to use it on the<br>model-free function code - the output sucked!&nbsp;</blockquote></span><div><br><br>interesting, I hadn&#39;t realised you had tried it. What was wrong with the output can you give some examples? 
<br></div><span class="q"><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">&nbsp;Most of the code are<br>simple functions of less that 5 lines.&nbsp;&nbsp;The hard part would be to port
<br>&#39;maths_fns/mf.py&#39;, which shouldn&#39;t be too hard anyway, and the rest<br>would be very basic.<br><br>Cheers,</blockquote><div><br>&nbsp;</div><br></span>regards <div><br>gary <br></div><br></div><br>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="00034" href="msg00034.html">Re: The multi-processor branch.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="00000" href="msg00000.html">Re: r3280 - /branches/multi_processor/multi/</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00017" href="msg00017.html">The multi-processor branch.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00027" href="msg00027.html">Re: The multi-processor branch.</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00032" href="msg00032.html">Re: The multi-processor branch.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer"></div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Tue May 29 16:40:37 2007</div>  
</body>
</html>
