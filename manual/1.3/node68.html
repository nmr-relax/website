<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Model-free optimisation theory</TITLE>
<META NAME="description" CONTENT="Model-free optimisation theory">
<META NAME="keywords" CONTENT="relax">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="relax.css">

<LINK REL="previous" HREF="node67.html">
<LINK REL="up" HREF="node61.html">
<LINK REL="next" HREF="node69.html">
</HEAD>

<BODY >

<DIV CLASS="navigation"><!--Navigation Panel-->
<A NAME="tex2html2313"
  HREF="node69.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/lib/latex2html/icons/next.png"></A> 
<A NAME="tex2html2307"
  HREF="node61.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/lib/latex2html/icons/up.png"></A> 
<A NAME="tex2html2303"
  HREF="node67.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/lib/latex2html/icons/prev.png"></A> 
<A NAME="tex2html2309"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="file:/usr/lib/latex2html/icons/contents.png"></A> 
<A NAME="tex2html2311"
  HREF="node298.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="file:/usr/lib/latex2html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html2314"
  HREF="node69.html">Optimisation of a single</A>
<B> Up:</B> <A NAME="tex2html2308"
  HREF="node61.html">Theory</A>
<B> Previous:</B> <A NAME="tex2html2304"
  HREF="node67.html">The model-free models</A>
 &nbsp; <B>  <A NAME="tex2html2310"
  HREF="node1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html2312"
  HREF="node298.html">Index</A></B> 
<BR>
<BR></DIV>
<!--End of Navigation Panel-->
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"><STRONG>Subsections</STRONG></A>

<UL CLASS="ChildLinks">
<LI><A NAME="tex2html2315"
  HREF="node68.html#SECTION00917100000000000000">The model-free space</A>
<LI><A NAME="tex2html2316"
  HREF="node68.html#SECTION00917200000000000000">Topology of the space</A>
<LI><A NAME="tex2html2317"
  HREF="node68.html#SECTION00917300000000000000">Optimisation algorithms</A>
<LI><A NAME="tex2html2318"
  HREF="node68.html#SECTION00917400000000000000">Line search methods</A>
<LI><A NAME="tex2html2319"
  HREF="node68.html#SECTION00917500000000000000">Trust region methods</A>
<LI><A NAME="tex2html2320"
  HREF="node68.html#SECTION00917600000000000000">Conjugate gradient methods</A>
<LI><A NAME="tex2html2321"
  HREF="node68.html#SECTION00917700000000000000">Hessian modifications</A>
<LI><A NAME="tex2html2322"
  HREF="node68.html#SECTION00917800000000000000">Other methods</A>
<LI><A NAME="tex2html2323"
  HREF="node68.html#SECTION00917900000000000000">Constraint algorithms</A>
<LI><A NAME="tex2html2324"
  HREF="node68.html#SECTION009171000000000000000">Diagonal scaling</A>
</UL>
<!--End of Table of Child-Links-->
<HR>

<H2><A NAME="SECTION00917000000000000000">
Model-free optimisation theory</A>
</H2><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<H3><A NAME="SECTION00917100000000000000">
The model-free space</A>
</H3><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The optimisation of the parameters of an arbitrary model is dependent on a function <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img108.png"
 ALT="$ f$"></SPAN>
 which takes the current parameter values <!-- MATH
 $\theta \in \mathbb{R}^n$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="56" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img109.png"
 ALT="$ \theta \in \mathbb{R}^n$"></SPAN>
 and returns a single real value <!-- MATH
 $f(\theta) \in \mathbb{R}$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="71" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img110.png"
 ALT="$ f(\theta) \in \mathbb{R}$"></SPAN>
 corresponding to position <SPAN CLASS="MATH"><IMG
 WIDTH="13" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img52.png"
 ALT="$ \theta$"></SPAN>
 in the <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img111.png"
 ALT="$ n$"></SPAN>
-dimensional space.  For it is that single value which is minimised as
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\hat\theta = \arg \min_\theta f(\theta),
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="133" HEIGHT="42" ALIGN="MIDDLE" BORDER="0"
 SRC="img112.png"
 ALT="$\displaystyle \hat\theta = \arg \min_\theta f(\theta),$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">23</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">where <!-- MATH
 $\hat\theta$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="13" HEIGHT="20" ALIGN="BOTTOM" BORDER="0"
 SRC="img53.png"
 ALT="$ \hat\theta$"></SPAN>
 is the parameter vector which is equal to the argument which minimises the function <SPAN CLASS="MATH"><IMG
 WIDTH="37" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img113.png"
 ALT="$ f(\theta)$"></SPAN>
.  In model-free analysis <SPAN CLASS="MATH"><IMG
 WIDTH="37" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img113.png"
 ALT="$ f(\theta)$"></SPAN>
 is the chi-squared<A NAME="1864"></A> equation
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="eq:_chi2"></A><!-- MATH
 \begin{equation}
\chi^2(\theta) = \sum_{i=1}^n \frac{(\mathrm{R}_i- \mathrm{R}_i(\theta))^2}{\sigma_i^2},
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="206" HEIGHT="68" ALIGN="MIDDLE" BORDER="0"
 SRC="img47.png"
 ALT="$\displaystyle \chi^2(\theta) = \sum_{i=1}^n \frac{(\mathrm{R}_i- \mathrm{R}_i(\theta))^2}{\sigma_i^2},$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">24</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">where <SPAN CLASS="MATH"><IMG
 WIDTH="10" HEIGHT="18" ALIGN="BOTTOM" BORDER="0"
 SRC="img48.png"
 ALT="$ i$"></SPAN>
 is the summation index, <!-- MATH
 $\mathrm{R}_i$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="23" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img49.png"
 ALT="$ \mathrm{R}_i$"></SPAN>
 is the experimental relaxation data which belongs to the data set R and includes the <!-- MATH
 $\mathrm{R}_1$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="25" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img29.png"
 ALT="$ \mathrm{R}_1$"></SPAN>
, <!-- MATH
 $\mathrm{R}_2$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="25" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.png"
 ALT="$ \mathrm{R}_2$"></SPAN>
, and NOE values at all field strengths, <!-- MATH
 $\mathrm{R}_i(\theta)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="45" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img50.png"
 ALT="$ \mathrm{R}_i(\theta)$"></SPAN>
 is the back calculated relaxation data belonging to the set R<SPAN CLASS="MATH"><IMG
 WIDTH="27" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img51.png"
 ALT="$ (\theta)$"></SPAN>
, and <SPAN CLASS="MATH"><IMG
 WIDTH="20" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img54.png"
 ALT="$ \sigma_i$"></SPAN>
 is the experimental error.  For the optimisation of the model-free parameters while the diffusion tensor is held fixed, the summation index ranges over the relaxation data of an individual residue.  If the diffusion parameters are optimised simultaneously with the model-free parameters the summation index ranges over all relaxation data of all selected residues of the macromolecule.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">Given the current parameter values the model-free function provided to the algorithm will calculate the value of the model-free spectral density function <SPAN CLASS="MATH"><IMG
 WIDTH="41" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img15.png"
 ALT="$ J(\omega)$"></SPAN>
 at the five frequencies which induce NMR relaxation by using Equations&nbsp;(<A HREF="node65.html#eq:_J_w__model-free_generic"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) and (<A HREF="node65.html#eq:_J_w__model-free_ext_generic"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>).  The theoretical <!-- MATH
 $\mathrm{R}_1$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="25" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img29.png"
 ALT="$ \mathrm{R}_1$"></SPAN>
, <!-- MATH
 $\mathrm{R}_2$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="25" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.png"
 ALT="$ \mathrm{R}_2$"></SPAN>
, and NOE values are then back-calculated using Equations&nbsp;(<A HREF="node64.html#eq:_R1"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>), (<A HREF="node64.html#eq:_R2"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>), (<A HREF="node64.html#eq:_sigma_NOE"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>), and (<A HREF="node64.html#eq:_NOE"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>).  Finally, the chi-squared<A NAME="1877"></A> value is calculated using Equation&nbsp;(<A HREF="#eq:_chi2"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>).
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<H3><A NAME="SECTION00917200000000000000">
Topology of the space</A>
</H3><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The problem of finding the minimum is complicated by the fact that optimisation algorithms are blind to the curvature of the complete space.  Instead they rely on topological information about the current and, sometimes, the previous parameter positions in the space.  The techniques use this information to walk iteratively downhill to the minimum.  Very few optimisation algorithms rely solely on the function value, conceptually the height of the space, at the current position.  Most techniques also utilise the gradient at the current position.  Although symbolically complex in the case of model-free analysis, the gradient can simply be calculated as the vector of first partial derivatives of the chi-squared<A NAME="1880"></A> equation with respect to each model-free parameter.  The gradient is supplied as a second function to the algorithm which is then utilised in diverse ways by different optimisation techniques.  The function value together with the gradient can be combined to construct a linear or planar description of the space at the current parameter position by first-order Taylor series approximation
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="eq:_linear_model"></A><!-- MATH
 \begin{equation}
f(\theta_k + x) \approx f_k  +  x^T \nabla f_k,
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="194" HEIGHT="40" ALIGN="MIDDLE" BORDER="0"
 SRC="img114.png"
 ALT="$\displaystyle f(\theta_k + x) \approx f_k + x^T \nabla f_k,$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">25</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">where <SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img115.png"
 ALT="$ f_k$"></SPAN>
 is the function value at the current parameter position <SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img116.png"
 ALT="$ \theta_k$"></SPAN>
, <!-- MATH
 $\nabla f_k$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="36" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img117.png"
 ALT="$ \nabla f_k$"></SPAN>
 is the gradient at the same position, and <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img118.png"
 ALT="$ x$"></SPAN>
 is an arbitrary vector.  By accumulating information from previous parameter positions a more comprehensive geometric description of the curvature of the space can be exploited by the algorithm for more efficient optimisation.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The best and most comprehensive description of the space is given by the quadratic approximation of the topology which is generated from the combination of the function value, the gradient, and the Hessian.  From the second-order Taylor series expansion the quadratic model of the space is
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="eq:_quadratic_model"></A><!-- MATH
 \begin{equation}
f(\theta_k + x) \approx f_k  +  x^T \nabla f_k  +  \tfrac{1}{2} x^T \nabla^2 f_k x,
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="295" HEIGHT="40" ALIGN="MIDDLE" BORDER="0"
 SRC="img119.png"
 ALT="$\displaystyle f(\theta_k + x) \approx f_k + x^T \nabla f_k + \tfrac{1}{2} x^T \nabla^2 f_k x,$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">26</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">where <!-- MATH
 $\nabla^2 f_k$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="43" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img120.png"
 ALT="$ \nabla^2 f_k$"></SPAN>
 is the Hessian, which is the symmetric matrix of second partial derivatives of the function, at the position <SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img116.png"
 ALT="$ \theta_k$"></SPAN>
.  As the Hessian is computationally expensive a number of optimisation algorithms try to approximate it.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">To produce the gradient and Hessian required for model-free optimisation a large chain of first and second partial derivatives needs to be calculated.  Firstly the partial derivatives of the spectral density functions (<A HREF="node65.html#eq:_J_w__model-free_generic"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) and (<A HREF="node65.html#eq:_J_w__model-free_ext_generic"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) are necessary.  Then the partial derivatives of the relaxation equations&nbsp;(<A HREF="node64.html#eq:_R1"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) to&nbsp;(<A HREF="node64.html#eq:_sigma_NOE"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) followed by the NOE equation&nbsp;(<A HREF="node64.html#eq:_NOE"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) are needed.  Finally the partial derivative of the chi-squared<A NAME="1894"></A> formula&nbsp;(<A HREF="#eq:_chi2"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) is required.  These first and second partial derivatives, as well as those of the components of the Brownian diffusion correlation function for non-isotropic tumbling, are presented in Chapter&nbsp;<A HREF="node82.html#ch:_values__gradients__and_Hessians"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<H3><A NAME="SECTION00917300000000000000">
Optimisation algorithms</A>
</H3><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">Prior to minimisation, all optimisation algorithms investigated require a starting position within the model-free space.  This initial parameter vector is found by employing a coarse grid search - chi-squared<A NAME="1898"></A> values at regular positions spanning the space are calculated and the grid point with the lowest value becomes the starting position.  The grid search itself is an optimisation technique.  As it is computationally expensive the number of grid points needs to be kept to a minimum.  Hence the initial parameter values are a rough and imprecise approximation of the local minimum.  Due to the complexity of the curvature of the model-free space, the grid point with the lowest chi-squared<A NAME="1899"></A> value may in fact be on the opposite side of the space to the local minimum.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">Once the starting position has been determined by the grid search the optimisation algorithm can be executed.  The number of algorithms developed within the mathematical field of optimisation is considerable.  They can nevertheless be grouped into one of a small number of major categories based on the fundamental principles of the technique.  These include the line search methods, the trust region methods, and the conjugate gradient methods.  For more details on the algorithms described below see <A
 HREF="#NocedalWright99">()</A>.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<H3><A NAME="SECTION00917400000000000000">
Line search methods</A>
</H3><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The defining characteristic of a line search algorithm is to choose a search direction <SPAN CLASS="MATH"><IMG
 WIDTH="22" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img121.png"
 ALT="$ p_k$"></SPAN>
 and then to find the minimum along that vector starting from <SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img116.png"
 ALT="$ \theta_k$"></SPAN>
 (<A
 HREF="#NocedalWright99">, </A>).  The distance travelled along <SPAN CLASS="MATH"><IMG
 WIDTH="22" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img121.png"
 ALT="$ p_k$"></SPAN>
 is the step length <SPAN CLASS="MATH"><IMG
 WIDTH="24" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img122.png"
 ALT="$ \alpha_k$"></SPAN>
 and the parameter values for the next iteration are
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\theta_{k+1} = \theta_k + \alpha_k p_k.
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="140" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img123.png"
 ALT="$\displaystyle \theta_{k+1} = \theta_k + \alpha_k p_k.$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">27</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The line search algorithm determines the search direction <SPAN CLASS="MATH"><IMG
 WIDTH="22" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img121.png"
 ALT="$ p_k$"></SPAN>
 whereas the value of <SPAN CLASS="MATH"><IMG
 WIDTH="24" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img122.png"
 ALT="$ \alpha_k$"></SPAN>
 is found using an auxiliary step-length selection algorithm.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">One of the simplest line search methods is the steepest descent<A NAME="1906"></A> algorithm.  The search direction is simply the negative gradient, <!-- MATH
 $p_k = -\nabla f_k$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="89" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img124.png"
 ALT="$ p_k = -\nabla f_k$"></SPAN>
, and hence the direction of maximal descent is always followed.  This method is inefficient - the linear rate of convergence requires many iterations of the algorithm to reach the minimum and it is susceptible to being trapped on saddle points within the space.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The coordinate descent<A NAME="1907"></A> algorithms are a simplistic group of line search methods whereby the search directions alternate between vectors parallel to the parameter axes.  For the back-and-forth coordinate descent the search directions cycle in one direction and then back again.  For example for a three parameter model the search directions cycle <!-- MATH
 $\theta_1, \theta_2, \theta_3, \theta_2, \theta_1, \theta_2, \hdots$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="166" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img125.png"
 ALT="$ \theta_1, \theta_2, \theta_3, \theta_2, \theta_1, \theta_2, \hdots$"></SPAN>
, which means that each parameter of the model is optimised one by one.  The method becomes less efficient when approaching the minimum as the step length <SPAN CLASS="MATH"><IMG
 WIDTH="24" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img122.png"
 ALT="$ \alpha_k$"></SPAN>
 continually decreases (ibid.).
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The quasi-Newton methods begin with an initial guess of the Hessian and update it at each iteration using the function value and gradient.  Therefore the benefits of using the quadratic model of (<A HREF="#eq:_quadratic_model"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) are obtained without calculating the computationally expensive Hessian.  The Hessian approximation <SPAN CLASS="MATH"><IMG
 WIDTH="26" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img126.png"
 ALT="$ B_k$"></SPAN>
 is updated using various formulae, the most common being the BFGS<A NAME="1909"></A> formula (<A NAME="tex2html2325" target="contents"
  HREF="#Broyden70">, </A>,,,).  The search direction is given by the equation <!-- MATH
 $p_k = -B_k^{-1} \nabla f_k$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="122" HEIGHT="41" ALIGN="MIDDLE" BORDER="0"
 SRC="img127.png"
 ALT="$ p_k = -B_k^{-1} \nabla f_k$"></SPAN>
.  The quasi-Newton algorithms can attain a superlinear rate of convergence, being superior to the steepest descent or coordinate descent methods.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The most powerful line search method when close to the minimum is the Newton<A NAME="1912"></A> search direction
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="eq:_Newton_dir"></A><!-- MATH
 \begin{equation}
p_k = - \nabla^2 f_k^{-1} \nabla f_k.
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="145" HEIGHT="41" ALIGN="MIDDLE" BORDER="0"
 SRC="img128.png"
 ALT="$\displaystyle p_k = - \nabla^2 f_k^{-1} \nabla f_k.$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">28</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">This direction is obtained from the derivative of (<A HREF="#eq:_quadratic_model"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) which is assumed to be zero at the minimum of the quadratic model.  The vector <SPAN CLASS="MATH"><IMG
 WIDTH="22" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img121.png"
 ALT="$ p_k$"></SPAN>
 points from the current position to the exact minimum of the quadratic model of the space.  The rate of convergence is quadratic, being superior to both linear and superlinear convergence.  The technique is computationally expensive due to the calculation of the Hessian.  It is also susceptible to failure when optimisation commences from distant positions in the space as the Hessian may not be positive definite and hence not convex, a condition required for the search direction both to point downhill and to be reasonably oriented.  In these cases the quadratic model is a poor description of the space.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">A practical Newton algorithm which is robust for distant starting points is the Newton conjugate gradient method (Newton-CG<A NAME="1918"></A>).  This line search method, which is also called the truncated Newton algorithm, finds an approximate solution to Equation&nbsp;(<A HREF="#eq:_Newton_dir"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) by using a conjugate gradient (CG) sub-algorithm.  Retaining the performance of the pure Newton algorithm, the CG sub-algorithm guarantees that the search direction is always downhill as the method terminates when negative curvature is encountered.  This algorithm is similar to the Newton-Raphson-CG algorithm implemented within Dasha<A NAME="1920"></A>.  Newton optimisation is sometimes also known as the Newton-Raphson algorithm and, as documented in the source code, the Newton algorithm in Dasha is coupled to a conjugate gradient algorithm.  The auxiliary step-length selection algorithm in Dasha<A NAME="1922"></A> is undocumented and may not be employed.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">Once the search direction has been determined by the above algorithms the minimum along that direction needs to be determined.  Not to be confused with the methodology for determining the search direction <SPAN CLASS="MATH"><IMG
 WIDTH="22" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img121.png"
 ALT="$ p_k$"></SPAN>
, the line search itself is performed by an auxiliary step-length selection algorithm to find the value <SPAN CLASS="MATH"><IMG
 WIDTH="24" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img122.png"
 ALT="$ \alpha_k$"></SPAN>
.  A number of step-length selection methods can be used to find a minimum along the line <!-- MATH
 $\theta_k + \alpha_k p_k$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="79" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img129.png"
 ALT="$ \theta_k + \alpha_k p_k$"></SPAN>
, although only two will be investigated.  The first is the backtracking line search of <A
 HREF="#NocedalWright99">()</A>.  This method is inexact - it takes a starting step length <SPAN CLASS="MATH"><IMG
 WIDTH="24" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img122.png"
 ALT="$ \alpha_k$"></SPAN>
 and decreases the value until a sufficient decrease in the function is found.  The second is the line search method of <A
 HREF="#MoreThuente94">()</A>.  Designed to be robust, the MT algorithm finds the exact minimum along the search direction and guarantees sufficient decrease.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<H3><A NAME="SECTION00917500000000000000">
Trust region methods</A>
</H3><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">In the trust region class of algorithms the curvature of the space is modelled quadratically by (<A HREF="#eq:_quadratic_model"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>).  This model is assumed to be reliable only within a region of trust defined by the inequality <!-- MATH
 $\lVert p \rVert \leqslant \Delta_k$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="77" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img130.png"
 ALT="$ \lVert p \rVert \leqslant \Delta_k$"></SPAN>
 where <SPAN CLASS="MATH"><IMG
 WIDTH="13" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img131.png"
 ALT="$ p$"></SPAN>
 is the step taken by the algorithm and <SPAN CLASS="MATH"><IMG
 WIDTH="27" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img132.png"
 ALT="$ \Delta_k$"></SPAN>
 is the radius of the <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img111.png"
 ALT="$ n$"></SPAN>
-dimensional sphere of trust (<A
 HREF="#NocedalWright99">, </A>).  The solution sought for each iteration of the algorithm is
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="eq:_trust_region"></A><!-- MATH
 \begin{equation}
\min_{p \in \mathbb{R}^n} m_k(p) = f_k  +  p^{T} \nabla f_k  +  \tfrac{1}{2} p^{T} B_k p,  \qquad \textrm{s.t. } \lVert p \rVert \leqslant \Delta_k,
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="430" HEIGHT="40" ALIGN="MIDDLE" BORDER="0"
 SRC="img133.png"
 ALT="$\displaystyle \min_{p \in \mathbb{R}^n} m_k(p) = f_k + p^{T} \nabla f_k + \tfrac{1}{2} p^{T} B_k p, \qquad \textrm{s.t. } \lVert p \rVert \leqslant \Delta_k,$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">29</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">where <SPAN CLASS="MATH"><IMG
 WIDTH="50" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img134.png"
 ALT="$ m_k(p)$"></SPAN>
 is the quadratic model, <SPAN CLASS="MATH"><IMG
 WIDTH="26" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img126.png"
 ALT="$ B_k$"></SPAN>
 is a positive definite matrix which can be the true Hessian as in the Newton model or an approximation such as the BFGS<A NAME="1937"></A> matrix, and <!-- MATH
 $\lVert p \rVert$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="31" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img135.png"
 ALT="$ \lVert p \rVert$"></SPAN>
 is the Euclidean norm of <SPAN CLASS="MATH"><IMG
 WIDTH="13" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img131.png"
 ALT="$ p$"></SPAN>
.  The trust region radius <SPAN CLASS="MATH"><IMG
 WIDTH="27" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img132.png"
 ALT="$ \Delta_k$"></SPAN>
 is modified dynamically during optimisation - if the quadratic model is found to be a poor representation of the space the radius is decreased whereas if the quadratic model is found to be reasonable the radius is increased to allow larger, more efficient steps to be taken.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The Cauchy point<A NAME="1938"></A> algorithm is similar in concept to the steepest descent<A NAME="1939"></A> line search algorithm.  The Cauchy point is the point lying on the gradient which minimises the quadratic model subject to the step being within the trust region.  By iteratively finding the Cauchy point the local minimum can be found.  The convergence of the technique is inefficient, being similar to that of the steepest descent algorithm.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">In changing the trust region radius the exact solutions to (<A HREF="#eq:_trust_region"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) map out a curved trajectory which starts parallel to the gradient for small radii.  The end of the trajectory, which occurs for radii greater than the step length, is the bottom of the quadratic model.  The dogleg<A NAME="1941"></A> algorithm attempts to follow a similar path by first finding the minimum along the gradient and then finding the minimum along a trajectory from the current point to the bottom of the quadratic model.  The minimum along the second path is either the trust region boundary or the quadratic solution.  The matrix <SPAN CLASS="MATH"><IMG
 WIDTH="26" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img126.png"
 ALT="$ B_k$"></SPAN>
 of (<A HREF="#eq:_trust_region"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) can be the BFGS matrix, the unmodified Hessian, or a Hessian modified to be positive definite.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">Another trust region algorithm is Steihaug's<A NAME="1943"></A> modified conjugate gradient approach (<A
 HREF="#Steihaug83">, </A>).  For each step <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img81.png"
 ALT="$ k$"></SPAN>
 an iterative technique is used which is almost identical to the standard conjugate gradient procedure except for two additional termination conditions.  The first is if the next step is outside the trust region, the second is if a direction of zero or negative curvature is encountered.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">An almost exact solution to (<A HREF="#eq:_trust_region"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) can be found using an algorithm described in <A
 HREF="#NocedalWright99">()</A>.  This exact trust region<A NAME="1947"></A> algorithm aims to precisely find the minimum of the quadratic model <SPAN CLASS="MATH"><IMG
 WIDTH="28" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img136.png"
 ALT="$ m_k$"></SPAN>
 of the space within the trust region <SPAN CLASS="MATH"><IMG
 WIDTH="27" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img132.png"
 ALT="$ \Delta_k$"></SPAN>
.  Any matrix <SPAN CLASS="MATH"><IMG
 WIDTH="26" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img126.png"
 ALT="$ B_k$"></SPAN>
 can be used to construct the quadratic model.  However, the technique is computationally expensive.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<H3><A NAME="SECTION00917600000000000000">
Conjugate gradient methods</A>
</H3><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The conjugate gradient algorithm (CG) was originally designed as a mathematical technique for solving a large system of linear equations <A
 HREF="#HestenesStiefel52">()</A>, but was later adapted to solving nonlinear optimisation problems (<A
 HREF="#FletcherReeves64">, </A>).  The technique loops over a set of directions <SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img137.png"
 ALT="$ p_0$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img138.png"
 ALT="$ p_1$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="25" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img139.png"
 ALT="$ \hdots$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="22" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img140.png"
 ALT="$ p_n$"></SPAN>
 which are all conjugate to the Hessian (<A
 HREF="#NocedalWright99">, </A>), a property defined as
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
p_i^T \nabla^2 f_k p_j = 0,  \qquad \textrm{for all } i \ne j.
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="245" HEIGHT="40" ALIGN="MIDDLE" BORDER="0"
 SRC="img141.png"
 ALT="$\displaystyle p_i^T \nabla^2 f_k p_j = 0, \qquad \textrm{for all } i \ne j.$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">30</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">By performing line searches over all directions <SPAN CLASS="MATH"><IMG
 WIDTH="20" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img142.png"
 ALT="$ p_j$"></SPAN>
 the solution to the quadratic model (<A HREF="#eq:_quadratic_model"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) of the position <SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img116.png"
 ALT="$ \theta_k$"></SPAN>
 will be found in <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img111.png"
 ALT="$ n$"></SPAN>
 or less iterations of the CG algorithm where <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img111.png"
 ALT="$ n$"></SPAN>
 is the total number of parameters in the model.  The technique performs well on large problems with many parameters as no matrices are calculated or stored.  The algorithms perform better than the steepest descent method and preconditioning of the system is used to improve optimisation.  A number of preconditioned techniques will be investigated including the Fletcher-Reeves<A NAME="1956"></A> algorithm which was the original conjugate gradient optimisation technique (<A
 HREF="#FletcherReeves64">, </A>), the Polak-Ribi&#232;re<A NAME="1958"></A> method (<A
 HREF="#PolakRibiere69">, </A>), a modified Polak-Ribi&#232;re method called the Polak-Ribi&#232;re +<A NAME="1960"></A> method (<A
 HREF="#NocedalWright99">, </A>), and the Hestenes-Stiefel<A NAME="1962"></A> algorithm which originates from a formula in <A
 HREF="#HestenesStiefel52">()</A>.  As a line search is performed to find the minimum along each conjugate direction both the backtracking and Mor&#233; and Thuente auxiliary step-length selection algorithms will be tested with the CG algorithms.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<H3><A NAME="SECTION00917700000000000000">
Hessian modifications</A>
</H3><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The Newton search direction, used in both the line search and trust region methods, is dependent on the Hessian being positive definite for the quadratic model to be convex so that the search direction points sufficiently downhill.  This is not always the case as saddle points and other non-quadratic features of the space can be problematic.  Two classes of algorithms can be used to handle this situation.  The first involves using the conjugate gradient method as a sub-algorithm for solving the Newton problem for the step <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img81.png"
 ALT="$ k$"></SPAN>
.  The Newton-CG<A NAME="1965"></A> line search algorithm described above is one such example.  The second class involves modifying the Hessian prior to, or at the same time as, finding the Newton step to guarantee that the replacement matrix <SPAN CLASS="MATH"><IMG
 WIDTH="26" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img126.png"
 ALT="$ B_k$"></SPAN>
 is positive definite.  The convexity of <SPAN CLASS="MATH"><IMG
 WIDTH="26" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img126.png"
 ALT="$ B_k$"></SPAN>
 is ensured by its eigenvalues all being positive.  The performance of two of these methods within the model-free space will be investigated.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The first modification uses the Cholesky factorisation of the matrix <SPAN CLASS="MATH"><IMG
 WIDTH="26" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img126.png"
 ALT="$ B_k$"></SPAN>
, initialised to the true Hessian, to test for convexity (Algorithm 6.3 of <A
 HREF="#NocedalWright99">()</A>).  If factorisation fails the matrix is not positive definite and a constant <SPAN CLASS="MATH"><IMG
 WIDTH="20" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img143.png"
 ALT="$ \tau_k$"></SPAN>
 times the identity matrix <SPAN CLASS="MATH"><IMG
 WIDTH="13" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img144.png"
 ALT="$ I$"></SPAN>
 is then added to <SPAN CLASS="MATH"><IMG
 WIDTH="26" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img126.png"
 ALT="$ B_k$"></SPAN>
.  The constant originates from the Robbins norm of the Hessian <!-- MATH
 $\lVert \nabla^2 f_k \rVert_F$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="72" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img145.png"
 ALT="$ \lVert \nabla^2 f_k \rVert_F$"></SPAN>
 and is steadily increased until the factorisation is successful.  The resultant Cholesky lower triangular matrix <SPAN CLASS="MATH"><IMG
 WIDTH="16" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img146.png"
 ALT="$ L$"></SPAN>
 can then be used to find the approximate Newton direction.  If <SPAN CLASS="MATH"><IMG
 WIDTH="20" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img143.png"
 ALT="$ \tau_k$"></SPAN>
 is too large the convergence of this technique can approach that of the steepest descent<A NAME="1967"></A> algorithm.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The second method is the Gill, Murray, and Wright (GMW) algorithm (<A
 HREF="#GMW81">, </A>) which modifies the Hessian during the execution of the Cholesky factorisation <!-- MATH
 $\nabla^2 f_k = LIL^T$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="110" HEIGHT="40" ALIGN="MIDDLE" BORDER="0"
 SRC="img147.png"
 ALT="$ \nabla^2 f_k = LIL^T$"></SPAN>
, where <SPAN CLASS="MATH"><IMG
 WIDTH="16" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img146.png"
 ALT="$ L$"></SPAN>
 is a lower triangular matrix and <SPAN CLASS="MATH"><IMG
 WIDTH="19" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img148.png"
 ALT="$ D$"></SPAN>
 is a diagonal matrix.  Only a single factorisation is required.  As rows and columns are interchanged during the algorithm the technique may be slow for large problems such as the optimisation of the model-free parameters of all residues together with the diffusion tensor parameters.  The rate of convergence of the technique is quadratic.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<H3><A NAME="SECTION00917800000000000000">
Other methods</A>
</H3><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">Two other optimisation algorithms which cannot be classified within line search, trust region, or conjugate gradient categories will also be investigated.  The first is the well known simplex<A NAME="1970"></A> optimisation algorithm.  The technique is often used as the only the function value is employed and hence the derivation of the gradient and Hessian can be avoided.  The simplex is created as an <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img111.png"
 ALT="$ n$"></SPAN>
-dimensional geometric object with <SPAN CLASS="MATH"><IMG
 WIDTH="45" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img149.png"
 ALT="$ n+1$"></SPAN>
 vertices.  The first vertex is the starting position.  Each of the other <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img111.png"
 ALT="$ n$"></SPAN>
 vertices are created by shifting the starting position by a small amount parallel to one of unit vectors defining the coordinate system of the space.  Four simple rules are used to move the simplex through the space: reflection, extension, contraction, and a shrinkage of the entire simplex.  The result of these movements is that the simplex moves in an ameoboid-like fashion downhill, shrinking to pass through tight gaps and expanding to quickly move through non-convoluted space, eventually finding the minimum.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">Key to these four movements is the pivot point, the centre of the face created by the <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img111.png"
 ALT="$ n$"></SPAN>
 vertices with the lowest function values.  The first movement is a reflection - the vertex with the greatest function value is reflected through the pivot point on the opposite face of the simplex.  If the function value at this new position is less than all others the simplex is allowed to extend - the point is moved along the line to twice the distance between the current position and the pivot point.  Otherwise if the function value is greater than the second highest value but less than the highest value, the reflected simplex is contracted.  The reflected point is moved to be closer to the simplex, its position being half way between the reflected position and the pivot point.  Otherwise if the function value at the reflected point is greater than all other vertices, then the original simplex is contracted - the highest vertex is moved to a position half way between the current position and the pivot point.  Finally if none of these four movements yield an improvement, then the simplex is shrunk halfway towards the vertex with the lowest function value.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The other algorithm is the commonly used Levenberg-Marquardt<A NAME="1971"></A> algorithm (<A NAME="tex2html2329" target="contents"
  HREF="#Marquardt63">, </A>,) which is implemented in Modelfree4<A NAME="1973"></A>, Dasha<A NAME="1974"></A>, and Tensor2<A NAME="1975"></A>.  This technique is designed for least-squares problems to which the chi-squared<A NAME="1976"></A> equation (<A HREF="#eq:_chi2"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) belongs.  The key to the algorithm is the replacement of the Hessian with the Levenberg-Marquardt matrix <!-- MATH
 $J^T J + \lambda I$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="78" HEIGHT="40" ALIGN="MIDDLE" BORDER="0"
 SRC="img150.png"
 ALT="$ J^T J + \lambda I$"></SPAN>
, where <SPAN CLASS="MATH"><IMG
 WIDTH="16" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img151.png"
 ALT="$ J$"></SPAN>
 is the Jacobian of the system calculated as the matrix of partial derivatives of the residuals, <!-- MATH
 $\lambda > 0$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="47" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img152.png"
 ALT="$ \lambda &gt; 0$"></SPAN>
 is a factor related to the trust-region radius, and <SPAN CLASS="MATH"><IMG
 WIDTH="13" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img144.png"
 ALT="$ I$"></SPAN>
 is the identity matrix.  The algorithm is conceptually allied to the trust region methods and its performance varies finely between that of the steepest descent and the pure Newton step.  When far from the minimum <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img153.png"
 ALT="$ \lambda$"></SPAN>
 is large and the algorithm takes steps close to the gradient; when in vicinity of the minimum <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img153.png"
 ALT="$ \lambda$"></SPAN>
 heads towards zero and the steps taken approximate the Newton direction.  Hence the algorithm avoids the problems of the Newton<A NAME="1978"></A> algorithm when non-convex curvature is encountered and approximates the Newton step in convex regions of the space.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<H3><A NAME="SECTION00917900000000000000">
Constraint algorithms</A>
</H3><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">To guarantee that the minimum will still be reached the implementation of constraints limiting the parameter values together with optimisation algorithms is not a triviality.  For this to occur the space and its boundaries must remain smooth thereby allowing the algorithm to move along the boundary to either find the minimum along the limit or to slide along the limit and then move back into the centre of the constrained space once the curvature allows it.  One of the most powerful approaches is the Method of Multipliers (<A
 HREF="#NocedalWright99">, </A>), also known as the Augmented Lagrangian.  Instead of a single optimisation the algorithm is iterative with each iteration consisting of an independent unconstrained minimisation on a sequentially modified space.  When inside the limits the function value is unchanged but when outside a penalty, which is proportional to the distance outside the limit, is added to the function value.  This penalty, which is based on the Lagrange multipliers, is smooth and hence the gradient and Hessian are continuous at and beyond the constraints.  For each iteration of the Method of Multipliers the penalty is increased until it becomes impossible for the parameter vector to be in violation of the limits.  This approach allows the parameter vector <SPAN CLASS="MATH"><IMG
 WIDTH="13" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img52.png"
 ALT="$ \theta$"></SPAN>
 outside the limits yet the successive iterations ensure that the final results will not be in violation of the constraint.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">For inequality constraints, each iteration of the Method of Multipliers attempts to solve the quadratic sub-problem
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="eq:_Augmented_Lagrangian"></A><!-- MATH
 \begin{equation}
\min_\theta \mathfrak{L}_A(\theta, \lambda^k; \mu_k) \stackrel{\mathrm{def}}{=} f(\theta) + \sum_{i \in \mathfrak{I}} \Psi(c_i(\theta), \lambda_i^k; \mu_k),
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="364" HEIGHT="56" ALIGN="MIDDLE" BORDER="0"
 SRC="img154.png"
 ALT="$\displaystyle \min_\theta \mathfrak{L}_A(\theta, \lambda^k; \mu_k) \stackrel{\m...
...=} f(\theta) + \sum_{i \in \mathfrak{I}} \Psi(c_i(\theta), \lambda_i^k; \mu_k),$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">31</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">where the function <SPAN CLASS="MATH"><IMG
 WIDTH="18" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img155.png"
 ALT="$ \Psi$"></SPAN>
 is defined as
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\Psi(c_i(\theta), \lambda^k; \mu_k) = \begin{cases}
        -\lambda^k c_i(\theta) + \frac{1}{2\mu_k} c_i^2(\theta) & \textrm{if } c_i(\theta) - \mu_k \lambda^k \leqslant 0, \\
        -\frac{\mu_k}{2} (\lambda^k)^2 & \textrm{otherwise}.
    \end{cases}
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="474" HEIGHT="70" ALIGN="MIDDLE" BORDER="0"
 SRC="img156.png"
 ALT="$\displaystyle \Psi(c_i(\theta), \lambda^k; \mu_k) = \begin{cases}-\lambda^k c_i...
...eqslant 0, \\  -\frac{\mu_k}{2} (\lambda^k)^2 &amp; \textrm{otherwise}. \end{cases}$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">32</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">In (<A HREF="#eq:_Augmented_Lagrangian"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>), <SPAN CLASS="MATH"><IMG
 WIDTH="13" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img52.png"
 ALT="$ \theta$"></SPAN>
 is the parameter vector; <!-- MATH
 $\mathfrak{L}_A$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="27" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img157.png"
 ALT="$ \mathfrak{L}_A$"></SPAN>
 is the Augmented Lagrangian function; <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img81.png"
 ALT="$ k$"></SPAN>
 is the current iteration of the Method of Multipliers; <SPAN CLASS="MATH"><IMG
 WIDTH="23" HEIGHT="19" ALIGN="BOTTOM" BORDER="0"
 SRC="img158.png"
 ALT="$ \lambda^k$"></SPAN>
 are the Lagrange multipliers which are positive factors such that, at the minimum <!-- MATH
 $\hat\theta$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="13" HEIGHT="20" ALIGN="BOTTOM" BORDER="0"
 SRC="img53.png"
 ALT="$ \hat\theta$"></SPAN>
, <!-- MATH
 $\nabla f(\hat\theta) = \lambda_i \nabla c_i(\hat\theta)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="140" HEIGHT="42" ALIGN="MIDDLE" BORDER="0"
 SRC="img159.png"
 ALT="$ \nabla f(\hat\theta) = \lambda_i \nabla c_i(\hat\theta)$"></SPAN>
; <SPAN CLASS="MATH"><IMG
 WIDTH="55" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img160.png"
 ALT="$ \mu_k &gt; 0$"></SPAN>
 is the penalty parameter which decreases to zero as <!-- MATH
 $k \to \infty$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="59" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img161.png"
 ALT="$ k \to \infty$"></SPAN>
; <!-- MATH
 $\mathfrak{I}$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img162.png"
 ALT="$ \mathfrak{I}$"></SPAN>
 is the set of inequality constraints; and <!-- MATH
 $c_i(\theta)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="40" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img163.png"
 ALT="$ c_i(\theta)$"></SPAN>
 is an individual constraint value.  The Lagrange multipliers are updated using the formula
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\lambda_i^{k+1} = \max(\lambda_i^k - c_i(\theta)/\mu_k, 0), \qquad \textrm{for all } i \in \mathfrak{I}.
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="363" HEIGHT="41" ALIGN="MIDDLE" BORDER="0"
 SRC="img164.png"
 ALT="$\displaystyle \lambda_i^{k+1} = \max(\lambda_i^k - c_i(\theta)/\mu_k, 0), \qquad \textrm{for all } i \in \mathfrak{I}.$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">33</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The gradient of the Augmented Lagrangian is
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\nabla \mathfrak{L}_A(\theta, \lambda^k; \mu_k) =
        \nabla f(\theta)
        - \sum_{i \in \mathfrak{I} | c_i(\theta) \leqslant \mu_k \lambda_i^k}
            \left( \lambda_i^k - \frac{c_i(\theta)}{\mu_k} \right) \nabla c_i(\theta),
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="467" HEIGHT="69" ALIGN="MIDDLE" BORDER="0"
 SRC="img165.png"
 ALT="$\displaystyle \nabla \mathfrak{L}_A(\theta, \lambda^k; \mu_k) = \nabla f(\theta...
...i^k} \left( \lambda_i^k - \frac{c_i(\theta)}{\mu_k} \right) \nabla c_i(\theta),$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">34</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">and the Hessian is
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\nabla^2 \mathfrak{L}_A(\theta, \lambda^k; \mu_k) =
        \nabla^2 f(\theta)
        + \sum_{i \in \mathfrak{I} | c_i(\theta) \leqslant \mu_k \lambda_i^k}
            \left[
                \frac{1}{\mu_k} \nabla c_i^2(\theta)
                - \left( \lambda_i^k - \frac{c_i(\theta)}{\mu_k} \right) \nabla^2 c_i(\theta)
            \right].
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="603" HEIGHT="67" ALIGN="MIDDLE" BORDER="0"
 SRC="img166.png"
 ALT="$\displaystyle \nabla^2 \mathfrak{L}_A(\theta, \lambda^k; \mu_k) = \nabla^2 f(\t...
...( \lambda_i^k - \frac{c_i(\theta)}{\mu_k} \right) \nabla^2 c_i(\theta) \right].$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">35</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The Augmented Lagrangian algorithm can accept any set of three arbitrary constraint functions <SPAN CLASS="MATH"><IMG
 WIDTH="34" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img167.png"
 ALT="$ c(\theta)$"></SPAN>
, <!-- MATH
 $\nabla c(\theta)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="49" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img168.png"
 ALT="$ \nabla c(\theta)$"></SPAN>
, and <!-- MATH
 $\nabla^2 c(\theta)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="57" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img169.png"
 ALT="$ \nabla^2 c(\theta)$"></SPAN>
.  When given the current parameter values <SPAN CLASS="MATH"><IMG
 WIDTH="34" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img167.png"
 ALT="$ c(\theta)$"></SPAN>
 returns a vector of constraint values whereby each position corresponds to one of the model parameters.  The constraint is defined as <!-- MATH
 $c_i \geqslant 0$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="49" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img170.png"
 ALT="$ c_i \geqslant 0$"></SPAN>
.  The function <!-- MATH
 $\nabla c(\theta)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="49" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img168.png"
 ALT="$ \nabla c(\theta)$"></SPAN>
 returns the matrix of constraint gradients and <!-- MATH
 $\nabla^2 c(\theta)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="57" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img169.png"
 ALT="$ \nabla^2 c(\theta)$"></SPAN>
 is the constraint Hessian function which should return the 3D matrix of constraint Hessians.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">A more specific set of constraints accepted by the Method of Multipliers are bound constraints.  These are defined by the function
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
l \leqslant \theta \leqslant u,
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="80" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img171.png"
 ALT="$\displaystyle l \leqslant \theta \leqslant u,$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">36</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">where <SPAN CLASS="MATH"><IMG
 WIDTH="10" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img172.png"
 ALT="$ l$"></SPAN>
 and <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img173.png"
 ALT="$ u$"></SPAN>
 are the vectors of lower and upper bounds respectively and <SPAN CLASS="MATH"><IMG
 WIDTH="13" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img52.png"
 ALT="$ \theta$"></SPAN>
 is the parameter vector.  For example for model-free model <SPAN CLASS="MATH"><IMG
 WIDTH="28" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img174.png"
 ALT="$ m4$"></SPAN>
 to place lower and upper bounds on the order parameter and lower bounds on the correlation time and chemical exchange parameters, the vectors are
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\begin{pmatrix}
        0 \\
        0 \\
        0 \\
    \end{pmatrix}
    \leqslant
    \begin{pmatrix}
        S^2 \\
        \tau_e \\
        R_{ex} \\
    \end{pmatrix}
    \leqslant
    \begin{pmatrix}
        1 \\
        \infty \\
        \infty \\
    \end{pmatrix}.
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="204" HEIGHT="82" ALIGN="MIDDLE" BORDER="0"
 SRC="img175.png"
 ALT="$\displaystyle \begin{pmatrix}0 \\  0 \\  0 \\  \end{pmatrix} \leqslant \begin{p...
...nd{pmatrix} \leqslant \begin{pmatrix}1 \\  \infty \\  \infty \\  \end{pmatrix}.$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">37</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The default setting in the program relax<A NAME="2031"></A> is to use linear constraints which are defined as
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="eq:_linear_constraint"></A><!-- MATH
 \begin{equation}
A \cdot \theta \geqslant b,
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="74" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img176.png"
 ALT="$\displaystyle A \cdot \theta \geqslant b,$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">38</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">where <SPAN CLASS="MATH"><IMG
 WIDTH="18" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img177.png"
 ALT="$ A$"></SPAN>
 is an <!-- MATH
 $m \times n$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="51" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img178.png"
 ALT="$ m \times n$"></SPAN>
 matrix where the rows are the transposed vectors <SPAN CLASS="MATH"><IMG
 WIDTH="19" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img179.png"
 ALT="$ a_i$"></SPAN>
 of length <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img111.png"
 ALT="$ n$"></SPAN>
; the elements of <SPAN CLASS="MATH"><IMG
 WIDTH="19" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img179.png"
 ALT="$ a_i$"></SPAN>
 are the coefficients of the model parameters; <SPAN CLASS="MATH"><IMG
 WIDTH="13" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img52.png"
 ALT="$ \theta$"></SPAN>
 is the vector of model parameters of dimension <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img111.png"
 ALT="$ n$"></SPAN>
; <SPAN CLASS="MATH"><IMG
 WIDTH="12" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img180.png"
 ALT="$ b$"></SPAN>
 is the vector of scalars of dimension <SPAN CLASS="MATH"><IMG
 WIDTH="20" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img181.png"
 ALT="$ m$"></SPAN>
; <SPAN CLASS="MATH"><IMG
 WIDTH="20" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img181.png"
 ALT="$ m$"></SPAN>
 is the number of constraints; and <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img111.png"
 ALT="$ n$"></SPAN>
 is the number of model parameters.  For model-free analysis, linear constraints are the most useful type of constraint as the correlation time <SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img24.png"
 ALT="$ \tau_f$"></SPAN>
 can be restricted to being less than <SPAN CLASS="MATH"><IMG
 WIDTH="19" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img25.png"
 ALT="$ \tau_s$"></SPAN>
 by using the inequality <!-- MATH
 $\tau_s - \tau_f \geqslant 0$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="89" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img182.png"
 ALT="$ \tau_s - \tau_f \geqslant 0$"></SPAN>
.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">In rearranging (<A HREF="#eq:_linear_constraint"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) the linear constraint function <SPAN CLASS="MATH"><IMG
 WIDTH="34" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img167.png"
 ALT="$ c(\theta)$"></SPAN>
 returns the vector <!-- MATH
 $A \cdot \theta - b$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="68" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img183.png"
 ALT="$ A \cdot \theta - b$"></SPAN>
.  Because of the linearity of the constraints the gradient and Hessian are greatly simplified.  The gradient <!-- MATH
 $\nabla c(\theta)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="49" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img168.png"
 ALT="$ \nabla c(\theta)$"></SPAN>
 is simply the matrix <SPAN CLASS="MATH"><IMG
 WIDTH="18" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img177.png"
 ALT="$ A$"></SPAN>
 and the Hessian <!-- MATH
 $\nabla^2 c(\theta)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="57" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img169.png"
 ALT="$ \nabla^2 c(\theta)$"></SPAN>
 is zero.  For the parameters specific to individual residues the linear constraints in the notation of (<A HREF="#eq:_linear_constraint"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) are
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\begin{pmatrix}
        1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 &-1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 &-1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
        1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 &-1 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 &-1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &-1 \\
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        S^2 \\
        S^2_f \\
        S^2_s \\
        \tau_e \\
        \tau_f \\
        \tau_s \\
        R_{ex} \\
         r \\
        CSA \\
    \end{pmatrix}
    \geqslant
    \begin{pmatrix}
        0 \\
        -1 \\
        0 \\
        -1 \\
        0 \\
        -1 \\
        0 \\
        0 \\
        0 \\
        0 \\
        0 \\
        0 \\
        0 \\
        0.9e^{-10} \\
        2e^{-10} \\
        300e^{-6} \\
        0 \\
    \end{pmatrix}.
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="507" HEIGHT="385" ALIGN="MIDDLE" BORDER="0"
 SRC="img184.png"
 ALT="$\displaystyle \begin{pmatrix}1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\  1 &amp; 0 &amp; 0 &amp; ...
... \\  0 \\  0 \\  0.9e^{-10} \\  2e^{-10} \\  300e^{-6} \\  0 \\  \end{pmatrix}.$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">39</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">Through the isolation of each individual element, the constraints can be see to be equivalent to
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SPAN>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{subequations}
\begin{gather}
    0 \leqslant S^2 \leqslant 1, \\
    0 \leqslant S^2_f \leqslant 1, \\
    0 \leqslant S^2_s \leqslant 1, \\
    S^2 \leqslant S^2_f, \\
    S^2 \leqslant S^2_s, \\
    \tau_e \geqslant 0, \\
    \tau_f \geqslant 0, \\
    \tau_s \geqslant 0, \\
    \tau_s \geqslant 0, \\
    \tau_f \leqslant \tau_s, \\
    R_{ex} \geqslant 0, \\
    0.9e^{-10} \leqslant r \leqslant 2e^{-10}, \\
    -300e^{-6} \leqslant CSA \leqslant 0.
\end{gather} 
\end{subequations}
 -->
<TABLE CLASS="subequations" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="556" HEIGHT="377" ALIGN="BOTTOM" BORDER="0"
 SRC="img185.png"
 ALT="\begin{subequations}\begin{gather}0 \leqslant S^2 \leqslant 1, \\ 0 \leqslant S^...
...e^{-10}, \\ -300e^{-6} \leqslant CSA \leqslant 0. \end{gather}\end{subequations}"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
&nbsp;&nbsp;&nbsp;</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><SPAN CLASS="MATH"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">To prevent the computationally expensive optimisation of failed models in which the internal correlation times minimise to infinity (<A
 HREF="#dAuvergneGooley06">, </A>), the constraint <!-- MATH
 $\tau_e, \tau_f, \tau_s \leqslant 2\tau_m$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="118" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img186.png"
 ALT="$ \tau_e, \tau_f, \tau_s \leqslant 2\tau_m$"></SPAN>
 was implemented.  When the global correlation time is fixed the constraints in the matrix notation of (<A HREF="#eq:_linear_constraint"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) are
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\begin{pmatrix}
        -1 &  0 &  0 \\
         0 & -1 &  0 \\
         0 &  0 & -1 \\
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        \tau_e \\
        \tau_f \\
        \tau_s \\
    \end{pmatrix}
    \geqslant
    \begin{pmatrix}
        -2\tau_m \\
        -2\tau_m \\
        -2\tau_m \\
    \end{pmatrix}.
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="297" HEIGHT="82" ALIGN="MIDDLE" BORDER="0"
 SRC="img187.png"
 ALT="$\displaystyle \begin{pmatrix}-1 &amp; 0 &amp; 0 \\  0 &amp; -1 &amp; 0 \\  0 &amp; 0 &amp; -1 \\  \end{...
... \geqslant \begin{pmatrix}-2\tau_m \\  -2\tau_m \\  -2\tau_m \\  \end{pmatrix}.$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">41</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">However when the global correlation time <SPAN CLASS="MATH"><IMG
 WIDTH="25" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img26.png"
 ALT="$ \tau_m$"></SPAN>
 is one of the parameters being optimised the constraints become
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\begin{pmatrix}
        2 & -1 &  0 &  0 \\
        2 &  0 & -1 &  0 \\
        2 &  0 &  0 & -1 \\
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        \tau_m \\
        \tau_e \\
        \tau_f \\
        \tau_s \\
    \end{pmatrix}
    \geqslant
    \begin{pmatrix}
        0 \\
        0 \\
        0 \\
    \end{pmatrix}.
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="292" HEIGHT="105" ALIGN="MIDDLE" BORDER="0"
 SRC="img188.png"
 ALT="$\displaystyle \begin{pmatrix}2 &amp; -1 &amp; 0 &amp; 0 \\  2 &amp; 0 &amp; -1 &amp; 0 \\  2 &amp; 0 &amp; 0 &amp; ...
...u_s \\  \end{pmatrix} \geqslant \begin{pmatrix}0 \\  0 \\  0 \\  \end{pmatrix}.$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">42</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">For the parameters of the diffusion tensor the constraints utilised are
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SPAN>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{subequations}
\begin{gather}
    0 \leqslant \tau_m \leqslant 200.0e^{-9}, \\
    \mathfrak{D}_a \geqslant 0, \\
    0 \leqslant \mathfrak{D}_r \leqslant 1,
\end{gather} 
\end{subequations}
 -->
<TABLE CLASS="subequations" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="556" HEIGHT="101" ALIGN="BOTTOM" BORDER="0"
 SRC="img189.png"
 ALT="\begin{subequations}\begin{gather}0 \leqslant \tau_m \leqslant 200.0e^{-9}, \\ \...
...ant 0, \\ 0 \leqslant \mathfrak{D}_r \leqslant 1, \end{gather}\end{subequations}"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
&nbsp;&nbsp;&nbsp;</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><SPAN CLASS="MATH"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">which in the matrix notation of (<A HREF="#eq:_linear_constraint"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>) become
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\begin{pmatrix}
         1 &  0 &  0 \\
        -1 &  0 &  0 \\
         0 &  1 &  0 \\
         0 &  0 &  1 \\
         0 &  0 & -1 \\
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        \tau_m \\
        \mathfrak{D}_a \\
        \mathfrak{D}_r \\
    \end{pmatrix}
    \geqslant
    \begin{pmatrix}
        0 \\
        -200.0e^{-9} \\
        0 \\
        0 \\
        -1 \\
    \end{pmatrix}.
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="326" HEIGHT="125" ALIGN="MIDDLE" BORDER="0"
 SRC="img190.png"
 ALT="$\displaystyle \begin{pmatrix}1 &amp; 0 &amp; 0 \\  -1 &amp; 0 &amp; 0 \\  0 &amp; 1 &amp; 0 \\  0 &amp; 0 &amp;...
...eqslant \begin{pmatrix}0 \\  -200.0e^{-9} \\  0 \\  0 \\  -1 \\  \end{pmatrix}.$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">44</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">The upper limit of 200&nbsp;ns on <SPAN CLASS="MATH"><IMG
 WIDTH="25" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img26.png"
 ALT="$ \tau_m$"></SPAN>
 prevents the parameter from heading towards infinity when model failure occurs (see Chapter&nbsp;<A HREF="#ch:_model_elimination"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="file:/usr/lib/latex2html/icons/crossref.png"></A>).  This can significantly decrease the computation time.  To isolate the prolate spheroid<A NAME="2091"></A> the constraint
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\begin{pmatrix}
         1 \\
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        \mathfrak{D}_a \\
    \end{pmatrix}
    \geqslant
    \begin{pmatrix}
        0 \\
    \end{pmatrix},
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="136" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img191.png"
 ALT="$\displaystyle \begin{pmatrix}1 \\  \end{pmatrix} \cdot \begin{pmatrix}\mathfrak{D}_a \\  \end{pmatrix} \geqslant \begin{pmatrix}0 \\  \end{pmatrix},$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">45</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">is used whereas to isolate the oblate spheroid<A NAME="2100"></A> the constraint used is
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\begin{pmatrix}
         -1 \\
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        \mathfrak{D}_a \\
    \end{pmatrix}
    \geqslant
    \begin{pmatrix}
        0 \\
    \end{pmatrix}.
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="149" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img192.png"
 ALT="$\displaystyle \begin{pmatrix}-1 \\  \end{pmatrix} \cdot \begin{pmatrix}\mathfrak{D}_a \\  \end{pmatrix} \geqslant \begin{pmatrix}0 \\  \end{pmatrix}.$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">46</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">Dependent on the model optimised, the matrix <SPAN CLASS="MATH"><IMG
 WIDTH="18" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img177.png"
 ALT="$ A$"></SPAN>
 and vector <SPAN CLASS="MATH"><IMG
 WIDTH="12" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img180.png"
 ALT="$ b$"></SPAN>
 are constructed from combinations of the above linear constraints.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<H3><A NAME="SECTION009171000000000000000"></A> <A NAME="sect:_diagonal_scaling"></A>
<BR>
Diagonal scaling
</H3><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">Model scaling can have a significant effect on the optimisation algorithm - a poorly scaled model can cause certain techniques to fail.  When two parameters of the model lie on very different numeric scales the model is said to be poorly scaled.  For example in model-free analysis the order of magnitude of the order parameters is one whereas for the internal correlation times the order of magnitude is between <SPAN CLASS="MATH"><IMG
 WIDTH="46" HEIGHT="21" ALIGN="BOTTOM" BORDER="0"
 SRC="img193.png"
 ALT="$ 1e^{-12}$"></SPAN>
 to <SPAN CLASS="MATH"><IMG
 WIDTH="39" HEIGHT="21" ALIGN="BOTTOM" BORDER="0"
 SRC="img194.png"
 ALT="$ 1e^{-8}$"></SPAN>
.  Most effected are the trust region algorithms - the multidimensional sphere of trust will either be completely ineffective against the correlation time parameters or severely restrict optimisation in the order parameter dimensions.  In model-free analysis the significant scaling disparity can even cause failure of optimisation due to amplified effects of machine precision.  Therefore the model parameters need to be scaled.
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">This can be done by supplying the optimisation algorithm with the scaled rather than unscaled parameters.  When the chi-squared<A NAME="2113"></A> function, gradient<A NAME="2114"></A>, and Hessian<A NAME="2115"></A> are called the vector is then premultiplied with a diagonal matrix in which the diagonal elements are the scaling factors.  For the model-free analysis the scaling factor of one was used for the order parameter and a scaling factor of <SPAN CLASS="MATH"><IMG
 WIDTH="46" HEIGHT="21" ALIGN="BOTTOM" BORDER="0"
 SRC="img193.png"
 ALT="$ 1e^{-12}$"></SPAN>
 was used for the correlation times.  The <SPAN CLASS="MATH"><IMG
 WIDTH="33" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img19.png"
 ALT="$ R_{ex}$"></SPAN>
 parameter was scaled to be the chemical exchange rate of the first field strength.  The scaling matrix for the parameters {<SPAN CLASS="MATH"><IMG
 WIDTH="24" HEIGHT="21" ALIGN="BOTTOM" BORDER="0"
 SRC="img20.png"
 ALT="$ S^2$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="24" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img21.png"
 ALT="$ S^2_f$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="24" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img22.png"
 ALT="$ S^2_s$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="19" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img23.png"
 ALT="$ \tau_e$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img24.png"
 ALT="$ \tau_f$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="19" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img25.png"
 ALT="$ \tau_s$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="33" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img19.png"
 ALT="$ R_{ex}$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="13" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img16.png"
 ALT="$ r$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="43" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img195.png"
 ALT="$ CSA$"></SPAN>
} of individual residues is
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\begin{pmatrix}
        1 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 \\
        0 &  1 &  0 &  0 &  0 &  0 &  0 &  0 &  0 \\
        0 &  0 &  1 &  0 &  0 &  0 &  0 &  0 &  0 \\
        0 &  0 &  0 &  1e^{-12} &  0 &  0 &  0 &  0 &  0 \\
        0 &  0 &  0 &  0 &  1e^{-12} &  0 &  0 &  0 &  0 \\
        0 &  0 &  0 &  0 &  0 &  1e^{-12} &  0 &  0 &  0 \\
        0 &  0 &  0 &  0 &  0 &  0 &  (2\pi \omega_H)^{-2} &  0 &  0 \\
        0 &  0 &  0 &  0 &  0 &  0 &  0 &  1e^{-10} &  0 \\
        0 &  0 &  0 &  0 &  0 &  0 &  0 &  0 &  1e^{-4} \\
    \end{pmatrix}.
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="472" HEIGHT="212" ALIGN="MIDDLE" BORDER="0"
 SRC="img196.png"
 ALT="$\displaystyle \begin{pmatrix}1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\  0 &amp; 1 &amp; 0 &amp; ...
...0 &amp; 1e^{-10} &amp; 0 \\  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1e^{-4} \\  \end{pmatrix}.$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">47</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">For the ellipsoidal<A NAME="2129"></A> diffusion parameters {<SPAN CLASS="MATH"><IMG
 WIDTH="25" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img26.png"
 ALT="$ \tau_m$"></SPAN>
, <!-- MATH
 $\mathfrak{D}_a$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="27" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img7.png"
 ALT="$ \mathfrak{D}_a$"></SPAN>
, <!-- MATH
 $\mathfrak{D}_r$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="26" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img9.png"
 ALT="$ \mathfrak{D}_r$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="16" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img78.png"
 ALT="$ \alpha$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img79.png"
 ALT="$ \beta$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img80.png"
 ALT="$ \gamma$"></SPAN>
} the scaling matrix is
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\begin{pmatrix}
        1e^{-12} &  0 &  0 &  0 &  0 &  0 \\
        0 &  1e^7 &  0 &  0 &  0 &  0 \\
        0 &  0 &  1 &  0 &  0 &  0 \\
        0 &  0 &  0 &  1 &  0 &  0 \\
        0 &  0 &  0 &  0 &  1 &  0 \\
        0 &  0 &  0 &  0 &  0 &  1 \\
    \end{pmatrix}.
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="223" HEIGHT="147" ALIGN="MIDDLE" BORDER="0"
 SRC="img197.png"
 ALT="$\displaystyle \begin{pmatrix}1e^{-12} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\  0 &amp; 1e^7 &amp; 0 &amp; 0 ...
... &amp; 0 &amp; 0 \\  0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\  \end{pmatrix}.$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">48</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE">For the spheroidal<A NAME="2135"></A> diffusion parameters {<SPAN CLASS="MATH"><IMG
 WIDTH="25" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img26.png"
 ALT="$ \tau_m$"></SPAN>
, <!-- MATH
 $\mathfrak{D}_a$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="27" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img7.png"
 ALT="$ \mathfrak{D}_a$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="13" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img52.png"
 ALT="$ \theta$"></SPAN>
, <SPAN CLASS="MATH"><IMG
 WIDTH="15" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img94.png"
 ALT="$ \phi$"></SPAN>
} the scaling matrix is
</SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL><P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\begin{pmatrix}
        1e^{-12} &  0 &  0 &  0 \\
        0 &  1e^7 &  0 &  0 \\
        0 &  0 &  1 &  0 \\
        0 &  0 &  0 &  1 \\
    \end{pmatrix}.
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="174" HEIGHT="105" ALIGN="MIDDLE" BORDER="0"
 SRC="img198.png"
 ALT="$\displaystyle \begin{pmatrix}1e^{-12} &amp; 0 &amp; 0 &amp; 0 \\  0 &amp; 1e^7 &amp; 0 &amp; 0 \\  0 &amp; 0 &amp; 1 &amp; 0 \\  0 &amp; 0 &amp; 0 &amp; 1 \\  \end{pmatrix}.$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(theparentequation.<SPAN CLASS="arabic">49</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<P>
<SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"><SMALL CLASS="FOOTNOTESIZE"></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL></SMALL>
<DIV CLASS="navigation"><HR>
<!--Navigation Panel-->
<A NAME="tex2html2313"
  HREF="node69.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/lib/latex2html/icons/next.png"></A> 
<A NAME="tex2html2307"
  HREF="node61.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/lib/latex2html/icons/up.png"></A> 
<A NAME="tex2html2303"
  HREF="node67.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/lib/latex2html/icons/prev.png"></A> 
<A NAME="tex2html2309"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="file:/usr/lib/latex2html/icons/contents.png"></A> 
<A NAME="tex2html2311"
  HREF="node298.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="file:/usr/lib/latex2html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html2314"
  HREF="node69.html">Optimisation of a single</A>
<B> Up:</B> <A NAME="tex2html2308"
  HREF="node61.html">Theory</A>
<B> Previous:</B> <A NAME="tex2html2304"
  HREF="node67.html">The model-free models</A>
 &nbsp; <B>  <A NAME="tex2html2310"
  HREF="node1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html2312"
  HREF="node298.html">Index</A></B> </DIV>
<!--End of Navigation Panel-->
<ADDRESS>
Edward d'Auvergne
2007-02-19
</ADDRESS>
</BODY>
</HTML>
