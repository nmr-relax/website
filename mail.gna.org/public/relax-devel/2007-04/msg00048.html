<!-- MHonArc v2.6.16 -->
<!--X-Subject: multi processor scaling -->
<!--X-From-R13: "Unel E. Fubzcfba" <tnelgNozo.yrrqf.np.hx> -->
<!--X-Date: Fri, 20 Apr 2007 11:46:46 +0200 -->
<!--X-Message-Id: 46288BD3.8060502@bmb.leeds.ac.uk -->
<!--X-Content-Type: multipart/alternative -->
<!--X-Derived: jpg0Omb21ZZF9.jpg -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>multi processor scaling -- April 20, 2007 - 11:46</title>
<link rel="stylesheet" type="text/css" href="/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="https://gna.org/images/gna.theme/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />multi processor scaling</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00048" class="tabs">Index by Date</a> | <a href="threads.html#00048" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00047.html">Date Prev</a>] [<a href="msg00049.html">Date Next</a>] [<a href="msg00046.html">Thread Prev</a>] [<a href="msg00049.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: relax-devel@xxxxxxx, gary thompson scaling of min test case	&lt;garyt.and.sarahb@xxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Date</em>: Fri, 20 Apr 2007 10:45:55 +0100</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00048.html">46288BD3.8060502@bmb.leeds.ac.uk</a>&gt;</li>
<li class="menuitem">
<em>User-agent</em>: Mozilla Thunderbird 1.0 (X11/20041206)</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Gary S. Thompson</strong> on April 20, 2007 - 11:46:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<table width="100%"><tr><td bgcolor="#ffffff" style="background-color: #ffffff; color: #000000; "><font color="#000000">



Dear All<br>
&nbsp;&nbsp;&nbsp; I have now had a chance to do some true multi tasking on our local
cluster with real overhead from intreprocess communication and the
results are as follows<br>
<br>
<pre>processors&nbsp;&nbsp;&nbsp; 	min    	eff	mc	eff	grid	eff</pre>
<pre>1		18	100	80	100	134	100</pre>
<pre>2		9	100</pre>
<pre>4		5	90</pre>
<pre>8		3	75</pre>
<pre>16		1	112.5</pre>
<pre>32		1	56.25	8	31.25	4	104.6

</pre>
and the picture that speaks 1000 words<br>
<br>
<img alt="scaling of min test case"
 src="jpg0Omb21ZZF9.jpg" height="502"
 width="738"><br>
<br>
key top graph black line achieved runtimes<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; top graph red line expected runtimes with perfect scaling
efficency<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bottom graph scaling efficiency<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>
some notes<br>
<br>
<br>
0. data was collected on one of chris's small data sets containing 28
residues not all of which are active for minimisation columns<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; processors &nbsp;&nbsp;&nbsp; - no slave&nbsp; mpi processors <br>
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; min &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; - time for a minimisation of models
m1-m9 with a fixed diffusion tensor<br>
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; eff &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; - approximate parallel efficiency
expected runtime/ actual runtime<br>
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; mc &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; - 256 monte carlo calculations<br>
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; eff &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; - efficiency of the above<br>
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; grid &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; - a grid search on a anisotropic
diffusion tensor 6 steps<br>
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; eff &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; - efficency of the above<br>
&nbsp;&nbsp;&nbsp;&nbsp; tests were run on a cluster of opterons using gigabit ethernet and
mpi<br>
1. these results are crude wall times as measured by pythons time.time
function for the master but they do not include startup and shutdown
overhead<br>
2. these tests are single point measurements there are no statistics<br>
3. timings were rounded to 1 second, so for example we must consider
data points for&nbsp; more than 16 processors for the min run to be suspect<br>
<br>
<br>
The results also highlight up some interesting considerations<br>
<br>
1 our local cluster has very poor disk io, with the result that when i
first ran the calculations I saw no multiprocessor imporvements on the
min run (in actual fact it got worse!) I got round this for this crude
test by switching off virtually all text output from the various
minimisation commands. Now obviously this isn't a long term solution
but I can thing of other methods e.g&nbsp; using an output thread thread on
the master or output batching that would improve these results.<br>
2. comparison of the results from the grid calculation and the other
calculations are quite informative. clearly the grid results are
excellent. I believe this is because I am returning individual subtask
results to the master as they complete and the&nbsp; resulting overhead due
to waiting for the master is a problem. To make this (clearer?) here is
an example: in&nbsp; the case of the mc run I will take the 256 mc runs and
distribute a batch of 8 to each processor (in the case of a 32
processor run) I then resturn the results individually as they complete
I believe this can lead to access to the master being the bottleneck
(this is most probably due to output ovrehead on stdout&nbsp;&nbsp; again, though
problems with contention due to coherence of the calculation length
could also be a problem ). In the case of the grid there are no
subtasks&nbsp; as the grid is almost ideally sub divided by processor so
only one task is run on each slave. I can see at least two answers to
this. One is to batch the return of results so all results get returned
at once and the second is to have an output thread on&nbsp; the master
separate from the thread ervicing mpi calls so processing of&nbsp; returned
data doesn't block the master and thus the rest of the cluster.<br>
<br>
<br>
I have some more comments to follow on the design of the current
minimisation interface, how text output from the commands is
controlled, and unit testing but these will have to follow in another
message later on<br>
<br>
<br>
regards<br>
gary<br>
<br>
n.b. if the pciture doesn't dsiplay well my apolergi<br>
<br>
<br>
<pre class="moz-signature" cols="72">-- 
-------------------------------------------------------------------
Dr Gary Thompson
Astbury Centre for Structural Molecular Biology,
University of Leeds, Astbury Building,
Leeds, LS2 9JT, West-Yorkshire, UK             Tel. +44-113-3433024
email: <a rel="nofollow" class="moz-txt-link-abbreviated" href="mailto:garyt@xxxxxxxxxxxxxxx">garyt@xxxxxxxxxxxxxxx</a>                   Fax  +44-113-2331407
-------------------------------------------------------------------

</pre>


</font></td></tr></table>
<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="00049" href="msg00049.html">Re: multi processor scaling</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer">You are on the <a href="http://gna.org">Gna!</a> mail server.</div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Tue Apr 24 11:41:02 2007</div>  
</body>
</html>
