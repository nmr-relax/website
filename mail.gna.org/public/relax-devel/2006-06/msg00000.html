<!-- MHonArc v2.6.10 -->
<!--X-Subject: Re: [Fwd: Re: multi processing] -->
<!--X-From-R13: "Unel E. Fubzcfba" <tnelgNozo.yrrqf.np.hx> -->
<!--X-Date: Tue, 20 Jun 2006 11:58:50 +0200 -->
<!--X-Message-Id: 4497C69F.6040701@bmb.leeds.ac.uk -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: 44463C74.7050108@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10604222117l7320c264la84902aefe8ec299@mail.gmail.com -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: [Fwd: Re: multi processing] -- June 20, 2006 - 11:58</title>
<link rel="stylesheet" type="text/css" href="/mail.gna.org/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="/mail.gna.org/images/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: [Fwd: Re: multi processing]</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00000" class="tabs">Index by Date</a> | <a href="threads.html#00000" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[Date Prev] [<a href="msg00001.html">Date Next</a>] [Thread Prev] [<a href="msg00001.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: Edward d'Auvergne &lt;edward.dauvergne@xxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Date</em>: Tue, 20 Jun 2006 10:57:51 +0100</li>
<li class="menuitem">
<em>Cc</em>: relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00000.html">4497C69F.6040701@bmb.leeds.ac.uk</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;44463C74.7050108@bmb.leeds.ac.uk&gt;	&lt;7f080ed10604222117l7320c264la84902aefe8ec299@mail.gmail.com&gt;</li>
<li class="menuitem">
<em>User-agent</em>: Mozilla Thunderbird 1.0 (X11/20041206)</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Gary S. Thompson</strong> on June 20, 2006 - 11:58:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre style="margin: 0em;">Edward d'Auvergne wrote:</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">For an email which was accidentally not sent to the mailing lists it
may be better to resend the email rather than forwarding it as your
forwarded post started a new thread in
(<a  href="http://www.nmr-relax.com/mail.gna.org/public/relax-devel/2006-04/threads.html">https://mail.gna.org/public/relax-devel/2006-04/threads.html</a>).  It
may be possible to just remove the forwarding junk in the email, I'm
not sure how the 'Message ID' tag in the headers work.</pre><br>
<tt> </tt><br>
<br>
<blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">Whoa, that's a big supercomputer.  You are most welcome to give it a
go, it should speed up your model-free runs using relax.  The changes
will necessarily be extensive and will cause breakages while
development occurs, so Gary if you decide to go forwards with it, I
will probably fork relax and create an unstable development branch
called 1.3 where all new developments will go.  It might even be a
good idea to create a private branch for your changes from 1.3.  I
will then reserve 1.2 for bug fixes only.</pre><br>
<tt><br>     </tt><br>
<br>
</blockquote><pre style="margin: 0em;">Yep that seems like a good idea, however, read on;-)</pre><br>
<tt>   </tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">I've always planned on adding support for clusters and I have a basic
framework in place which might be a good platform to start from.  The
other idea I've had in the back of my mind is the conversion of the
all the model-free function code in the directory 'maths_fns' to C
(while still retaining the Python code as an option),</pre><br>
<tt>     </tt><br>
<br>
</blockquote><pre style="margin: 0em;">This seems reasonable, when I do a wc | sort -nr on maths_fns I get</pre><br>
<pre style="margin: 0em;"><br>12149  48347 493475 total
 3857  20572 174665 jw_mf.py
 2966  10359 153396 mf.py
 1314   3520  39824 ri_comps.py
  924   2434  22280 correlation_time.py
  836   2937  23114 weights.py
  732   2476  24964 jw_mf_comps.py
  599   2748  24435 direction_cosine.py
  470   1269  12150 ri_prime.py
  175    700   6129 ri.py
  109    519   4614 chi2.py
  106    448   4185 jw_mapping.py
   33    177   1922 __init__.py
   28    188   1797 test.c_chi.py</pre><br>
<tt>and I guess mf.py would be the one to hit first... The  questions are<br>
   </tt><br>
<br>
</blockquote><tt><br>The translation to C was just a suggestion as, computationally wise,<br>
the change would be a significant improvement.  It would decrease the<br>
computation time on each node of the cluster however it is a lot of<br>
work and is inessential for clustering.  Please don't fell obliged to<br>
even start this mammoth task.<br>
 </tt><br>
<br>
</blockquote><pre style="margin: 0em;"><br>yep I think I will skip this at the moment...</pre><br>
<blockquote class="blockquote"><tt> </tt><br>
<br>
<blockquote class="blockquote"><tt>1. do we need to do all of it or could we just wrap the maths intensive<br>
parts and leave the object creation and management in python<br>
   </tt><br>
<br>
</blockquote><pre style="margin: 0em;"><br>If the 'profile_flag' at the end of the 'relax' file in the base
directory is changed to 1, you can see the relative computational
requirements of the various bits of code.  To obtain the full benefits
of C, it would all need to be translated.</pre><br>
<tt> </tt><br>
<br>
<blockquote class="blockquote"><tt>2. Is there a low level test suite so conformity of python and C code<br>
can be verified<br>
   </tt><br>
<br>
</blockquote><pre style="margin: 0em;"><br>The test suite is very primitive and basic at the moment.  A large
number of tests would need to be added to cover all parameter
combinations.  These would need to cover all four types of model-free
minimisation:  the model-free parameters for one residue, the
model-free parameters together with a local tm parameter, the
diffusion parameters for all residues, and all parameters
simultaneously.</pre><br>
<tt> </tt><br>
<br>
<blockquote class="blockquote"><tt>3. would it be better to do it in pytrex rather than straight C? I guess<br>
the thing to do would be to test it out and see what the quality of the<br>
C code is like<br>
   </tt><br>
<br>
</blockquote><tt><br>I would prefer to stay with proper C using the standard Python/C API. 
I've played with<br>
Pyrex (pytrex is XML I think), Swig, and a few other interfaces but I<br>
don't believe that these will give the full speed ups of the raw<br>
interface.  The number crunching is very low level and using these<br>
high level interfaces is an overkill.<br>
 </tt><br>
<br>
</blockquote><tt><br>sorry, my typo, I mean't pytrex! Pyrex is really rather different from 
the others as it is not an interface but a reimplementation of a large 
subset of python to produce c source code not byte code with some 
extensions which allow direct access to C structures.  To quote from the 
author* 'Pyrex is Python with C data types' *(his emphasis)*<br>
*</tt><br>
<br>
<blockquote class="blockquote"><tt> </tt><br>
<br>
<blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">which may give
potential gains of 10 to 20 times increased performance.  This code is
by far the most CPU intensive, the minimisation code isn't anywhere
near as expensive.</pre><br>
<tt><br>     </tt><br>
<br>
</blockquote><tt>yep seems logical, the only question is have you profiled? Chris was<br>
trying to do some before the break and there didn't seem to be any<br>
really hot spots.. but I maybe misreading the rumour mill (He is of<br>
course a gargantuan 5 feet way much of the time ;-) Chris any comments?<br>
   </tt><br>
<br>
</blockquote><tt><br>The profile flag at the bottom of the file 'relax' will do it. 
Although a line by line translation will almost produce functional<br>
code (when mixed with the concepts in the relaxation curve-fitting C<br>
code together with the creation of a large struct called 'data'), it<br>
is still a huge effort so only play with it if you really want to.</tt><br>
<br>
<tt> </tt><br>
<br>
<blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">The framework currently in place is the threading code.  The way the
threading code works is through SSH tunnels.  It starts a new instance
of relax on the remote machine (or local if there are a number of CPUs
or CPU cores), that instance gets data sent to it, does the
calculation, and returns the result.  It does work, although it's not
very good at catching failures.  I haven't used it lately so I don't
know if it's broken.</pre><br>
<tt><br>     </tt><br>
<br>
</blockquote><tt>Thats generally the idea I had, i.e. a fairly course grained approach.<br>
My thought was to add constructs to the top level commands (if needed)<br>
to allow allow subsets of a set of calculations to be run from a script.<br>
i.e. part of a grid search or a few monte carlo runs or a subset of<br>
minimisations for a set of residues. Then the real script would generate<br>
the required subscripts plus embedded data on the fly. I think this<br>
provides a considerable degree of flexibility. Thus for instance our<br>
cluster which runs grid engine needs a master script to start all the<br>
sub processes rather than a set of separate password less ssh logons<br>
which a cluster of workstations would require. In general I thought that<br>
catching failures other than a failure to start is not required...<br>
   </tt><br>
<br>
</blockquote><tt><br>Is your idea similar to having the runs themselves threaded so instead<br>
of looping over them you run them simultaneously?  I don't know too<br>
much about clustering.  What is the interface by which data and<br>
instructions are sent and returned from the nodes?  And do you know if<br>
there are python wrappings?<br>
 </tt><br>
<br>
</blockquote><tt><br>so the idea is to take the low hanging fruit for the moment and only 
parallelise the things that will naturally run for the same amounts of time</tt><br>
<br>
<tt>e.g. divide up sets of monte carlo simulations into parts, run 
minimisations on subsets of residues that share the same model and 
tensor frame etc</tt><br>
<br>
<tt>as to how to send data, scripts and results: I would write an interface 
class and then allow differnt instances of the class to deal with 
communication differently to support different transport mehtods e.g. 
ssh logins vs mpi sessions (or something which hasn't been invented yet)</tt><br>
<br>
<tt>transfer of data will use cpickles in my case with an mpi backend to 
keep compute nodes available to prevent queing problems (you don't want 
to resubmit to the batch queue each time you calculate a subpart of the 
problem....)</tt><br>
<br>
<br>
<blockquote class="blockquote"><tt> </tt><br>
<br>
<blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">SSH tunnels is probably not the best option for your system.  Do you
know anything about MPI?</pre><br>
<tt>     </tt><br>
<br>
</blockquote><tt>I have read about MPI but have not implimented anything __YET__;-). Also<br>
I have compiled some MPI based programs. It seems to a bit of a pig and<br>
I don't think the low hanging fruit necessarily require that degree of<br>
fine grained distribution...<br>
   </tt><br>
<br>
</blockquote><tt><br>I haven't used MPI either.  There may be much better protocols<br>
implemented for Python.<br>
 </tt><br>
<br>
</blockquote><tt><br>actually after looking at the problem in our local implementation we 
will need mpi and I have the mpi from  from scientific working on my 
computer.   However,  as alluded to above mpi will only be a  dependancy 
for  a particular transport methods not the overall scheme</tt><br>
<br>
<blockquote class="blockquote"><tt> </tt><br>
<br>
<blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">There are a number of options available for
distributed calculations, but it will need to have a clean and stable
Python interface.</pre><br>
<tt>     </tt><br>
<br>
</blockquote><pre style="margin: 0em;">obviously a stable interface with as little change to the current top
level functions and as little suprise as possible is to be desired. I
thought it might be a good idea  to have some form of facade, so  that
the various forms of coarse grained multi processing looks the same,
whichever one you are using. The idea would be only to have the setup
and dispach code different.</pre><br>
<tt>   </tt><br>
<br>
</blockquote><tt><br>It would probably be best to use some existing standard protocol<br>
rather than inventing a relax specific system.<br>
 </tt><br>
<br>
</blockquote><tt><br>I think the interface of scripts plus data provides all you need,  the  
actual methodology  in the   transport method can be private...</tt><br>
<br>
<pre style="margin: 0em;">so for example:</pre><br>
<pre style="margin: 0em;">1. create a clustre with a transport layer</pre><br>
<pre style="margin: 0em;">top level script:</pre><br>
<tt>init_parallel()                                                         
                                                  # override relax 
commands as needed<br>
cluster= create_cluster(name='test')                                    
                                    # the cluster to use you can have 
more than one...<br>
mpi-transport=create_transport(name='name',method='mpi-local',....)      
             # a transport layer all extra keyword arguments are for 
configurateion<br>
processor-set=create_processor(transport=mpi-transport,nprocessors=30,...)   
    # a particular set of processors using a partuicular transport 
method, with a particular weight<br>
cluster_add_processor(processor-set, weight=1.0)                        
                       # add it to the pool of available processors</tt><br>
<br>
<pre style="margin: 0em;">normal relax setup ...</pre><br>
<tt>minimise('newton',run=name,cluster=cluster)                            
                          # one extra argument</tt><br>
<br>
<pre style="margin: 0em;">2. internally</pre><br>
<tt>class transport(object):                                            # 
just knows howto setup a connection to a bunch ot prosessors and 
communicate with them</tt><br>
<br>
<tt> def __init__(self):                                                
   pass</tt><br>
<br>
<tt> def start(self,nprocessors,**kw):                           # setup 
for calculation, returns processor-set for this particuar connection<br>
    
pass                                                                       
# kw arguments from create_processor</tt><br>
<br>
<tt>def shutdown(self,aprocessor-set):                                    
        # end all calculations and shutdown<br>
   pass</tt><br>
<br>
<tt> def setupData(self,processor-set,data,nodes=None):                # 
send setup data, in my case I would pickle it to an in memory file and 
then put it in a<br>
                                                                                           
# numpy byte array for transport over numerics mpi layer,  if node is 
None send it to everyone<br>
    pass</tt><br>
<br>
<tt> def calculate(self,processor-set,node,script,callback, tag):         
                              # run the script on the node x and call 
completion callback with tag when complete<br>
   pass</tt><br>
<br>
<pre style="margin: 0em;"> def getData(self,processor-set,node=None):
   pass</pre><br>
<tt> def status(self,processor-set,node=None):                              
                       # test for status of  a particular calculation<br>
   pass</tt><br>
<br>
<tt> def 
cancel(self,processor-set,node=None):                                                      
# give up calculation on a particular node<br>
    pass</tt><br>
<br>
<pre style="margin: 0em;"><br>class cluster(object):</pre><br>
<tt>  def __init__(self):                                                
    pass</tt><br>
<br>
<pre style="margin: 0em;"> def start(self):
     pass</pre><br>
<tt> def getDivisions(self,nproblems):       # get a list of of size for  
'divisions' of  the problems to send to each element of each processor 
set based on weights and  number of processors<br>
    pass</tt><br>
<br>
<pre style="margin: 0em;"> def shutdown(self):
     pass</pre><br>
<pre style="margin: 0em;"><br> def setupData(self,data):                             # send setup data
     pass</pre><br>
<tt><br> def calculate(self,division,scripts):                           # run 
the script on all nodes<br>
   pass</tt><br>
<br>
<tt><br> def getData(self,division)                                          # 
get results<br>
    pass</tt><br>
<br>
<pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">.... anyway i think the idea is fairly clear</pre><br>
<blockquote class="blockquote"><tt> </tt><br>
<br>
<blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">Which ever system is decided upon, threading inside
the program will probably be necessary so that each thread can be sent
to a different machine.  This requires calculations which can be
parallelised.  As minimisation is an iterative process with each
iteration requiring the results of the previous, and as it's not the
most CPU intensive part anyway, I can't see too many gains in
modifying that code.</pre><br>
<tt>     </tt><br>
<br>
</blockquote><pre style="margin: 0em;">Agreed</pre><br>
<tt>   </tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">I've already parallelised the Monte Carlo
simulations for the threading code as those calculations are the most
obvious target.</pre><br>
<tt>     </tt><br>
<br>
</blockquote><tt>They are a time hog<br>
   </tt><br>
<br>
</blockquote><pre style="margin: 0em;"><br>Grid searching model m8 {S2, tf, S2f, ts, Rex} probably beats the
total of the MC sims (unless the data is dodgy).</pre><br>
<tt> </tt><br>
<br>
<blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">But all residue specific calculations could be
parellelised as well.  This is probably where you can get the best
speed ups.</pre><br>
<tt>     </tt><br>
<br>
</blockquote><pre style="margin: 0em;">Yes that and grid searches seem obvious candidates</pre><br>
<tt>   </tt><br>
<br>
</blockquote><tt><br>I was thinking more along the lines of splitting the residues rather<br>
than the grid search increments.  These increments could be threaded<br>
however the approach would need to be conservative.  I'm planning on<br>
eventually splitting out the minimisation code as a separate project<br>
on Gna! as a Python optimisation library.  The optimisers in Scipy are<br>
useless!<br>
 </tt><br>
<br>
</blockquote><tt><br>I think whichever divisons are equal and fit the best are what is 
required, though residues would be the obvious first candidate followed 
by grid steps</tt><br>
<br>
<blockquote class="blockquote"><tt> </tt><br>
<br>
<blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">I have a few more comments below.</pre><br>
<pre style="margin: 0em;">On 4/13/06, Gary S. Thompson &lt;garyt@xxxxxxxxxxxxxxx&gt; wrote:</pre><br>
<tt><br>     </tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">Dear Ed
  I was we have a 148 processor beowolf cluster ;-) I was thinking of
having a go at developing a distributed version of relax... are you ok
with that or do you have plans of your own?</pre><br>
<pre style="margin: 0em;">The general idea was to have scripts look almost as they are but</pre><br>
<pre style="margin: 0em;">1. have  a command to register multi processor handlers</pre><br>
<tt><br>       </tt><br>
<br>
</blockquote><pre style="margin: 0em;">The user function class 'threading' is probably close to what you want.</pre><br>
<tt><br>     </tt><br>
<br>
</blockquote><pre style="margin: 0em;">I shall have a look at it</pre><br>
<tt>   </tt><br>
<br>
</blockquote><pre style="margin: 0em;"><br>Actually it's called 'thread'.</pre><br>
<tt> </tt><br>
<br>
<blockquote class="blockquote"><blockquote class="blockquote"><tt>     </tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">2. have a command to add machines and parameters to the multi processor pool</pre><br>
<tt><br>       </tt><br>
<br>
</blockquote><pre style="margin: 0em;">threading.add() is probably a good template.</pre><br>
<pre style="margin: 0em;"><br></pre><br>
<tt><br>     </tt><br>
<br>
</blockquote><pre style="margin: 0em;">again I shall have a read</pre><br>
<tt>   </tt><br>
<br>
</blockquote><pre style="margin: 0em;"><br>I got the wrong name again.  It's 'threading.read', 'threading.add'
hasn't been written yet!</pre><br>
<tt> </tt><br>
<br>
<blockquote class="blockquote"><blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">3. add code to the generic functions/or replace the generic funcntions
if the multiprocessing is setup to batch up components of calculations
and pass them out to the compute  servers</pre><br>
<tt><br>       </tt><br>
<br>
</blockquote><pre style="margin: 0em;">'generic/minimise.py' is the best bet.  Otherwise there is
'maths_fns/mf.py' which can be hacked.</pre><br>
<tt><br>     </tt><br>
<br>
</blockquote><pre style="margin: 0em;">more reading ;-)</pre><br>
<tt>   </tt><br>
<br>
<blockquote class="blockquote"><tt>     </tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">4. add  code to multiplex the results back together again</pre><br>
<tt><br>       </tt><br>
<br>
</blockquote><pre style="margin: 0em;">That should be pretty straight forward.</pre><br>
<pre style="margin: 0em;"><br></pre><br>
<tt>     </tt><br>
<br>
<blockquote class="blockquote"><tt>obviously this would just be a prototype at first but it could be rather<br>
useful<br>
       </tt><br>
<br>
</blockquote></blockquote></blockquote><pre style="margin: 0em;"><br>The use of published standards and low level protocols would be best
to keep the calculations bug free and fast.  For debugging, it might
be worth considering adding threading tests to the test suite.</pre><br>
<pre style="margin: 0em;">Edward</pre><br>
<pre style="margin: 0em;">.</pre><br>
<tt> </tt><br>
<br>
</blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">anyway i intend to branch now to a provate branch;-)</pre><br>
<pre style="margin: 0em;">regards
gary</pre><br>
<pre style="margin: 0em;">--
-------------------------------------------------------------------
Dr Gary Thompson
Astbury Centre for Structural Molecular Biology,
University of Leeds, Astbury Building,
Leeds, LS2 9JT, West-Yorkshire, UK             Tel. +44-113-3433024
email: garyt@xxxxxxxxxxxxxxx                   Fax  +44-113-2331407
-------------------------------------------------------------------</pre><br>
<pre style="margin: 0em;"><br></pre><br>
<br>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<!--X-Follow-Ups-End-->
<!--X-References-->
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer"></div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Tue Jun 20 12:02:11 2006</div>  
</body>
</html>
