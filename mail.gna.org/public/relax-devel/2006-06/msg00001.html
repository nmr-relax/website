<!-- MHonArc v2.6.10 -->
<!--X-Subject: Re: multi processing -->
<!--X-From-R13: "Unel E. Fubzcfba" <tnelgNozo.yrrqf.np.hx> -->
<!--X-Date: Tue, 20 Jun 2006 12:00:53 +0200 -->
<!--X-Message-Id: 4497C6FF.3050407@bmb.leeds.ac.uk -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: 445A0001.9020608@bmb.leeds.ac.uk -->
<!--X-Reference: c225763e0605041842t6b8287a0l543344ebc5514375@mail.gmail.com -->
<!--X-Reference: 7f080ed10605042156j1595407m18e4be8079b1784c@mail.gmail.com -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: multi processing -- June 20, 2006 - 12:00</title>
<link rel="stylesheet" type="text/css" href="/mail.gna.org/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="/mail.gna.org/images/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: multi processing</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00001" class="tabs">Index by Date</a> | <a href="threads.html#00001" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00000.html">Date Prev</a>] [Date Next] [<a href="msg00000.html">Thread Prev</a>] [Thread Next]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: Edward d'Auvergne &lt;edward.dauvergne@xxxxxxxxx&gt;, relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Date</em>: Tue, 20 Jun 2006 10:59:27 +0100</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00001.html">4497C6FF.3050407@bmb.leeds.ac.uk</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;445A0001.9020608@bmb.leeds.ac.uk&gt;	&lt;c225763e0605041842t6b8287a0l543344ebc5514375@mail.gmail.com&gt;	&lt;7f080ed10605042156j1595407m18e4be8079b1784c@mail.gmail.com&gt;</li>
<li class="menuitem">
<em>User-agent</em>: Mozilla Thunderbird 1.0 (X11/20041206)</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Gary S. Thompson</strong> on June 20, 2006 - 12:00:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre style="margin: 0em;">Edward d'Auvergne wrote:</pre><br>
<tt>Hi thanks for this info, as you can see from my previous e-mail I am not 
going to build in a dependance on mpi in relax directly as I want to 
have multiple backends. But this is till useful as I will use mpi as my 
first transport for testing ;-)</tt><br>
<br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">On 5/5/06, Andrew Perry &lt;ajperry@xxxxxxxxxxxxxx&gt; wrote:</pre><br>
<blockquote class="blockquote"><tt><br>&gt;&gt;SSH tunnels is probably not the best option for your system.  Do you<br>
&gt;&gt;know anything about MPI?<br>
&gt;<br>
&gt;I have read about MPI but have not implimented anything __YET__;-). 
&gt;Also<br>
&gt;I have compiled some MPI based programs. It seems to a bit of a pig 
&gt;and<br>
&gt;I don't think the low hanging fruit necessarily require that degree of<br>
&gt;fine grained distribution...</tt><br>
<br>
<pre style="margin: 0em;">If this is any help, I've done what I think is some fairly exhaustive
searching for python+mpi implementations recently. Note that I've never
_actually_ used any of them for a project yet.</pre><br>
</blockquote><pre style="margin: 0em;"><br>Thanks, the info should help.</pre><br>
<blockquote class="blockquote"><tt>Scientific Python has an MPI interface, which is handy since it is 
already a<br>
relax dependancy. The drawback is that its documentation seems very 
geared<br>
toward those who already understand MPI reasonably well. The other 
drawback<br>
is that is seems to be only able to pass Numpy arrays and strings 
between<br>
nodes, which would mean some relax data structures would probably 
need to be<br>
'repackaged' for sending via MPI.<br>
<a  href="http://starship.python.net/~hinsen/ScientificPython/ScientificPythonManual/Scientific_27.html">http://starship.python.net/~hinsen/ScientificPython/ScientificPythonManual/Scientific_27.html</a> </tt><br>
<br>
<br>
</blockquote><br>
</blockquote><tt>I have got this one up and this is what I am going to use make my 
default implementation.  The good news is that the apparent drawbacks 
don't seem to bad as you can dump arbitary data to in memory files via 
cpickle and then send them over the mpi link  as a byte array</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">If the code is implemented is implemented at the analysis specific
level, for example the minimise() function in the file
'specific_fns/model_free.py', then almost all of the data structures
are already converted to Numeric arrays.</pre><br>
</blockquote><pre style="margin: 0em;">that neat ;-) and useful information</pre><br>
<blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">Another one is:
 MYMPI - <a  href="http://peloton.sdsc.edu/~tkaiser/mympi/">http://peloton.sdsc.edu/~tkaiser/mympi/</a> (and
<a  href="http://grid-devel.sdsc.edu/gridsphere/gridsphere?cid=mympi">http://grid-devel.sdsc.edu/gridsphere/gridsphere?cid=mympi</a>
) -  syntax intended to match C MPI API closely, and much like
Scientific.MPI only has direct support for some basic data types, not
arbitrary python objects.</pre><br>
<tt>Most other implementations (below) support transmission of any python 
object<br>
that can be pickled, and so may take less code to implement in relax.<br>
However, sending the whole data object when only select parts of it are<br>
required for the calculation could be more inefficient than you would 
like,<br>
and so 'repackaging' and sending just what is needed may be better 
anyway. I<br>
wonder which is worse in this case .. the network overhead of sending a<br>
large-ish python object, or the extra load on the 'master' node as it<br>
repackages it to smaller Numpy array ..?? Guess it all depends on 
whether<br>
things are carved up 'batchwise' or more fine-grained (inner 
loop/function<br>
level).</tt><br>
<br>
</blockquote><br>
</blockquote><pre style="margin: 0em;">certainly for a first implementation I am going to go batchwise</pre><br>
<blockquote class="blockquote"><tt>What data needs to be sent depends on what level the threading will be<br>
implemented on.  If each call to minimise() in<br>
'specific_fns/model_free.py' is threaded, then only the data which is<br>
packaged within that function will need to be sent.  The node can then<br>
return solely the minimisation results (parameter vector, iteration<br>
count, function count, gradient count, hessian count, and warnings). 
My threading code is a little higher up in the chain within the<br>
minimise() function of the generic code (generic_fns/minimise.py)<br>
which calls the specific model-free minimise() function.  This code<br>
currently only works for Monte Carlo simulations.
</tt></blockquote><br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br>The repackaging overhead by the master node should be tiny compared to
the calculation time.  The cost of sending data could become quite
high if the threading is fine grained enough.  What really needs to be
determined is what will be threaded.  Will individual model-free
minimisation instances be threaded?  If the diffusion tensor is fixed
then individual residue minimisations will be threaded.  If the
diffusion tensor parameters are optimised, either with or without the
model-free parameters, then there is one single instance.  If the
local tm parameter is included, then again individual residues are
optimised.  Using this fine grained approach communication to and from
the nodes will likely be expensive.
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>yep I don't intend to go fine grain at the moment, though my general 
design could be expanded that way if needed</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br>The second thing which could be threaded is the runs themselves.  For
example if models m1 to m9 are optimised normally using a Python loop
these could be threaded so that, assuming individual residue
minimisations are threaded, then model m2 calculations could start
while instances of model m1 are still being calculated on nodes.  This
could cause significant speed ups if the protein has more residues
than the cluster has nodes.  Otherwise each run could be sent to a
different node (the amount of data sent would be much larger).
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>again  I would prefer not to optimise by models, but by residues at the 
moment, as the management is just so much easier</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br>Finally Monte Carlo simulations are the highest level and most obvious
target.  This is the part of model-free analysis which takes the
longest.</pre><br>
<blockquote class="blockquote"><tt>MMPI - <a  href="http://www.penzilla.net/mmpi/">http://www.penzilla.net/mmpi/</a> - looks to be actively 
developed, good<br>
documentation with examples, including sending of python objects via<br>
pickling.</tt><br>
<br>
<tt> pyPar -<br>
<a  href="http://datamining.anu.edu.au/~ole/work/software/pypar/">http://datamining.anu.edu.au/~ole/work/software/pypar/</a> -<br>
sends abitrary python objects, only two GPL licensed files so would 
be very<br>
easy to package directly with relax rather than make users chase<br>
dependancies.</tt><br>
<br>
</blockquote><pre style="margin: 0em;"><br>We could import the dependancy with a 'try:' statement so that MPI is
only a dependency for those wishing to use multiple machines.  It
looks like Pypar is dependent on a C MPI library anyway.
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>again the use of backends (which would be classes on the pythonpath) 
will solve this problem. Instantion of plugins which lack the required c 
libraries should obviously raise useful exceptions...</tt><br>
<br>
<blockquote class="blockquote"><br>
<blockquote class="blockquote"><pre style="margin: 0em;">There are also two which are parallel python interpreters that require
recompilation, and seem to work a bit differently (still getting my head
around exactly how these are meant to be used).</pre><br>
<tt> <a  href="http://www.cimec.org.ar/python/">http://www.cimec.org.ar/python/</a> - a parallel interpreter as well as 
also<br>
some MPI bindings for python. I tested the interpreter with relax and<br>
LAM/MPI, seemed to spawn off lots of processes and run.</tt><br>
<br>
<pre style="margin: 0em;"> pyMPI - <a  href="http://pympi.sourceforge.net/index.html">http://pympi.sourceforge.net/index.html</a> - a
parallel python interpreter, decent docs at (
<a  href="http://heanet.dl.sourceforge.net/sourceforge/pympi/pyMPI.pdf">http://heanet.dl.sourceforge.net/sourceforge/pympi/pyMPI.pdf</a>
), seems mature despite out of data website.</pre><br>
<pre style="margin: 0em;">There is also:</pre><br>
<pre style="margin: 0em;">MPY - <a  href="http://mpy.sourceforge.net/index.html">http://mpy.sourceforge.net/index.html</a> (seems
abandoned since 2004)</pre><br>
<pre style="margin: 0em;">Hope this helps ...</pre><br>
<pre style="margin: 0em;">Andrew</pre><br>
</blockquote><pre style="margin: 0em;"><br>Would you know which of these implementations are the most mature or
the most used?  Stability would be better than fancy features.
</pre></blockquote><tt><br>if we use transport backends  as i discuss in my previous message we can 
change the implementation relativley trivially and so avoid longterm 
dependancy problems</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br>Thanks,</pre><br>
<pre style="margin: 0em;">Edward</pre><br>
<pre style="margin: 0em;">_______________________________________________
relax (<a  href="http://nmr-relax.com">http://nmr-relax.com</a>)</pre><br>
<pre style="margin: 0em;">This is the relax-devel mailing list
relax-devel@xxxxxxx</pre><br>
<pre style="margin: 0em;">To unsubscribe from this list, get a password
reminder, or change your subscription options,
visit the list information page at
<a  href="/mail.gna.org/listinfo/relax-devel">https://mail.gna.org/listinfo/relax-devel</a></pre><br>
<pre style="margin: 0em;">.</pre><br>
</blockquote><pre style="margin: 0em;">regards
gary</pre><br>
<pre style="margin: 0em;">--
-------------------------------------------------------------------
Dr Gary Thompson
Astbury Centre for Structural Molecular Biology,
University of Leeds, Astbury Building,
Leeds, LS2 9JT, West-Yorkshire, UK             Tel. +44-113-3433024
email: garyt@xxxxxxxxxxxxxxx                   Fax  +44-113-2331407
-------------------------------------------------------------------</pre><br>
<pre style="margin: 0em;"><br></pre><br>
<br>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<!--X-Follow-Ups-End-->
<!--X-References-->
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer"></div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Tue Jun 20 12:02:11 2006</div>  
</body>
</html>
