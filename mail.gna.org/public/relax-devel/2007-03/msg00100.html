<!-- MHonArc v2.6.10 -->
<!--X-Subject: Re: r3237 &#45; in /branches/multi_processor: multi/mpi4py_processor.py	relax -->
<!--X-From-R13: "Unel E. Fubzcfba" <tnelgNozo.yrrqf.np.hx> -->
<!--X-Date: Tue, 20 Mar 2007 12:31:16 +0100 -->
<!--X-Message-Id: 45FFC5DC.8000208@bmb.leeds.ac.uk -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: E1HTJY1&#45;0005VF&#45;O1@subversion.gna.org -->
<!--X-Reference: 45FEAC46.8020307@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703190923s4b09fab1vd958ed6001031b9c@mail.gmail.com -->
<!--X-Reference: 45FF1AF7.2000606@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703200322ua51bfcexc4e26556a623a0d0@mail.gmail.com -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax -- March 20, 2007 - 12:31</title>
<link rel="stylesheet" type="text/css" href="/mail.gna.org/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="/mail.gna.org/images/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00100" class="tabs">Index by Date</a> | <a href="threads.html#00100" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00099.html">Date Prev</a>] [<a href="msg00101.html">Date Next</a>] [<a href="msg00099.html">Thread Prev</a>] [<a href="msg00102.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: Edward d'Auvergne &lt;edward@xxxxxxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Date</em>: Tue, 20 Mar 2007 11:30:36 +0000</li>
<li class="menuitem">
<em>Cc</em>: relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00100.html">45FFC5DC.8000208@bmb.leeds.ac.uk</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;E1HTJY1-0005VF-O1@subversion.gna.org&gt;	&lt;<a href="msg00088.html">45FEAC46.8020307@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00091.html">7f080ed10703190923s4b09fab1vd958ed6001031b9c@mail.gmail.com</a>&gt;	&lt;<a href="msg00094.html">45FF1AF7.2000606@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00099.html">7f080ed10703200322ua51bfcexc4e26556a623a0d0@mail.gmail.com</a>&gt;</li>
<li class="menuitem">
<em>User-agent</em>: Mozilla Thunderbird 1.0 (X11/20041206)</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Gary S. Thompson</strong> on March 20, 2007 - 12:31:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre style="margin: 0em;">Edward d'Auvergne wrote:</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">On 3/20/07, Gary S. Thompson &lt;garyt@xxxxxxxxxxxxxxx&gt; wrote:</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">Edward d'Auvergne wrote:</pre><br>
<pre style="margin: 0em;">&gt; Hi,
&gt;
&gt; Just a quick point, it would be good to either start a new thread for
&gt; these types of questions or changing the subject field (unless you use
&gt; the Gmail web interface like I am at the moment).  People may miss
&gt; important discussions with such scary subject lines!
&gt;
&gt;
&gt; On 3/20/07, Gary S. Thompson &lt;garyt@xxxxxxxxxxxxxxx&gt; wrote:
&gt;
&gt;&gt; garyt@xxxxxxxxxxxxxxx wrote:
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">[snip]</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">&gt;&gt; Now a question what is the best way to get an etrenally running relax
&gt;&gt; interpreter I can just fire commands at (for the slaves)?
&gt;
&gt;
&gt; The prompt based interface (as well as the script interface) is only
&gt; one way of invoking relax.  An important question is how should we
&gt; present relax to the user when using MPI.  Should the parent process
&gt; present a functional interpreter or should operation be solely
&gt; dictated by a script?</pre><br>
<pre style="margin: 0em;"><br>I already have a prompt running on the master, my idea is that the relax
use should see no difference (apart from perfomance) when using he
parallel version
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">I've just played around with that and it does look like the best
option for user flexibility.</pre><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">&gt; Or should a completely different mechanism of
&gt; operation be devised for the interface of the parent.  For the grid
&gt; computing code the parent UI is either the prompt or the script while
&gt; the slaves use the interface started by 'relax --thread'.  The slave
&gt; use none of the user functions and only really invoke the number
&gt; crunching code.</pre><br>
<pre style="margin: 0em;">now this is what I couldn't follow. How much of the relax environment is
essential for a slave/thread and if we don't want to do it all by
compete pickles of relax data structures (how big is a complete relax
data structure for a typical set of runs?) where shoudl i start
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>My post at <a  href="/mail.gna.org/public/relax-devel/2007-03/msg00097.html">https://mail.gna.org/public/relax-devel/2007-03/msg00097.html</a><br>
(Message-id: 
&lt;1174384147.29205.20.camel@xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&gt;)<br>
hopefully answers these questions.</tt><br>
<br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">&gt; For the MPI slaves (I'm assuming these are separate processes with
&gt; different PIDs running on different nodes) we should avoid the
&gt; standard UI interfaces as these are superfluous.  In this case a
&gt; simple MPI interface should probably be devised - it accepts the MPI
&gt; commands and returns data back to the parent.  Is this though stdin,
&gt; stdout, and stderr in MPI?  My knowledge of MPI is very limited.
&gt;</pre><br>
<pre style="margin: 0em;"><br>1. via picked objects  which will integrate the results back into the
master copy or
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">If implemented at the 'minimise_mpi()' model-free method, then this
would probably be the best option.</pre><br>
<blockquote class="blockquote"><tt>2. as text strings which are printed with a  process number in front (so<br>
that  complete parallel  log files can be grepped out of the main 
output).
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>However if you see advantages with this option, then maybe this is 
better.</tt><br>
<br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;"> I don't currently assume that the compute node has available or useable
or shared  disk space.  everything comes back to the master
</pre></blockquote><br>
</blockquote><br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br>I expected MPI to operate in this way.  This means that none of the
grid computing code will be of use to you.
</pre></blockquote><tt><br>I only assumed that this  would be a restraint for the mpi stuff other 
code can fit in the framework and access disks via nfs or in other ways  
if needed ;-)</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br></pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">the place to look at in mpi4py_processor is</pre><br>
<pre style="margin: 0em;">lines 59-71  which sends a command from the master and either prints or
exceutes the resulting object (checks for exceptions, repeated feedback,
and command completion still to come)</pre><br>
<pre style="margin: 0em;">        for i in range(1,MPI.size):
            if i != 0:
                MPI.COMM_WORLD.Send(buf=command,dest=i)
        for i in range(1,MPI.size):
            buf=[]
            if i !=0:
                elem = MPI.COMM_WORLD.Recv(source=i)
                if type(elem) == 'object':
                    elem.run(relax_instance, relax_instance.processor)
                else:
                    #FIXME can't cope with multiple lines
                    print i,elem</pre><br>
<pre style="margin: 0em;">and lines 92-94 where all the command are recieved and executes on the
slaves</pre><br>
<pre style="margin: 0em;">         while not self.do_quit:
                command = MPI.COMM_WORLD.Recv(source=0)
                command.run(self.relax_instance,
self.relax_instance.processor)</pre><br>
<pre style="margin: 0em;">it is the intention that the command protocol is very minimal</pre><br>
<pre style="margin: 0em;">0. send a command plus data to run on the remote slave as an object
which will get its run method executed with the local relax and
processor as aruments master:communicator.run_command
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>Maybe the run method should be part of the 'minimise()' model-free 
method?
</tt></blockquote><tt><br>well actually in the end the model free method would in multiprocesszor 
mode create objects and send them the commands at the moment appear to 
be part of the implimentation  of mpi4py_processor but they aren't they 
are generic objects</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br></pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">1. the slave then writes a series of objects to the processsor method
return_object
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>The model-free parameter vector and optimisation stats?
</tt></blockquote><pre style="margin: 0em;"><br>yep so this is outgoing</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br></pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">2. master recieves data either
   a. objects to execute on the master which will also be given the
master relax and processor instances
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">Unpack the minimisation data and place it into the appropriate
location in the relax data storage object?</pre><br>
</blockquote><pre style="margin: 0em;">and this is incoming</pre><br>
<blockquote class="blockquote"><br>
<blockquote class="blockquote"><tt>   b.string objects to print
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">The optimisation print outs?</pre><br>
</blockquote><pre style="margin: 0em;">yes</pre><br>
<blockquote class="blockquote"><br>
<blockquote class="blockquote"><pre style="margin: 0em;">   c. a command completion indicator back from the slave (a well known
object)
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">Does the slave then die?</pre><br>
</blockquote><pre style="margin: 0em;">no!</pre><br>
<blockquote class="blockquote"><br>
<blockquote class="blockquote"><pre style="margin: 0em;">   d. an exception (raising of a local exception ont master which will
do stack trace printing for the master and the slave)
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">Ah, I didn't think of that one!</pre><br>
<br>
<blockquote class="blockquote"><tt>   e. None  a void  return
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>???
</tt></blockquote><tt><br>a return it there is o result but I guess I could also just use c. a 
command completion indicato</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br></pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">&gt;&gt; do i modify interpreter.run to take a quit varaiable set to False  so
&gt;&gt; that run_script can be
&gt;&gt; run with quit = false?
&gt;
&gt;
&gt; Avoid the interpreter and wait at the optimisation steps - the only
&gt; serious number crunching code in relax.</pre><br>
<pre style="margin: 0em;"><br>I agree!  interpeters are not required on the slave just the relax data
structures in a clean and useable state
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">If you're working at the level of the model-free 'minimise()'
function, don't bother with the relax data structures!  See my
previous post mentioned above.
</pre></blockquote><tt><br>I follow now, my only worry is that the processing will be fairly fine 
grained so causing a greater ratio of network traffic to processing</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br></pre><br>
<blockquote class="blockquote"><tt>One other quetion: how well behaved are the relax functions with not 
gratuitously modifying global state. e.g could I share one relax 
instance between several threads? The reason I ask is that in the 
case that they are well behaved  many of the data transfer operations 
in a threaded environment with a single  memory space would become 
noops ;-) nice!
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">I wouldn't share the state.  Again if you work at the 'minimise()'
model-free method level, copying it and renaming it to
'minimise_mpi()', that new function could be made to not touch the
relax data storage object.  Maybe there should be a
'minimise_mpi_master()' that contains the setup code and a
'minimise_mpi_slave()' which contains the optimisation code and the
unpacking code.  This should be very simple to copy and modify from
the current code!</pre><br>
<pre style="margin: 0em;">Cheers,</pre><br>
<pre style="margin: 0em;">Edward</pre><br>
</blockquote><tt><br>actually there shouldn't be anything labelled mpi outside the specific 
instance of a processor (that uses mpi) everything else whould be generic</tt><br>
<br>
<pre style="margin: 0em;">create remote command
send remote command
work with results from remote command...</pre><br>
<pre style="margin: 0em;">.</pre><br>
<blockquote class="blockquote"><br>
</blockquote><pre style="margin: 0em;"><br>regards
gary</pre><br>
<pre style="margin: 0em;">--
-------------------------------------------------------------------
Dr Gary Thompson
Astbury Centre for Structural Molecular Biology,
University of Leeds, Astbury Building,
Leeds, LS2 9JT, West-Yorkshire, UK             Tel. +44-113-3433024
email: garyt@xxxxxxxxxxxxxxx                   Fax  +44-113-2331407
-------------------------------------------------------------------</pre><br>
<pre style="margin: 0em;"><br></pre><br>
<br>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="00102" href="msg00102.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="00088" href="msg00088.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00091" href="msg00091.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00094" href="msg00094.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00099" href="msg00099.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer"></div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Tue Mar 20 13:40:16 2007</div>  
</body>
</html>
