<!-- MHonArc v2.6.16 -->
<!--X-Subject: Re: The multi&#45;processor branch. -->
<!--X-From-R13: "Unel E. Fubzcfba" <tnelgNozo.yrrqf.np.hx> -->
<!--X-Date: Fri, 18 May 2007 10:11:46 +0200 -->
<!--X-Message-Id: 464D5F8B.3010009@bmb.leeds.ac.uk -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: E1HjkOC&#45;0007zq&#45;7e@subversion.gna.org -->
<!--X-Reference: 463B2E31.30503@bmb.leeds.ac.uk -->
<!--X-Reference: 1178838900.6484.88.camel@localhost -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: The multi-processor branch. -- May 18, 2007 - 10:11</title>
<link rel="stylesheet" type="text/css" href="/mail.gna.org/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="/mail.gna.org/images/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: The multi-processor branch.</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00027" class="tabs">Index by Date</a> | <a href="threads.html#00027" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00026.html">Date Prev</a>] [<a href="msg00028.html">Date Next</a>] [<a href="msg00017.html">Thread Prev</a>] [<a href="msg00032.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: Edward d'Auvergne &lt;edward.dauvergne@xxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Date</em>: Fri, 18 May 2007 09:10:51 +0100</li>
<li class="menuitem">
<em>Cc</em>: relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00027.html">464D5F8B.3010009@bmb.leeds.ac.uk</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;E1HjkOC-0007zq-7e@xxxxxxxxxxxxxxxxxx&gt;	&lt;463B2E31.30503@xxxxxxxxxxxxxxx&gt;	&lt;<a href="msg00017.html">1178838900.6484.88.camel@localhost</a>&gt;</li>
<li class="menuitem">
<em>User-agent</em>: Mozilla Thunderbird 1.0 (X11/20041206)</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Gary S. Thompson</strong> on May 18, 2007 - 10:11:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre style="margin: 0em;">
Edward d'Auvergne wrote:

</pre><blockquote class="blockquote"><pre style="margin: 0em;">
On Fri, 2007-05-04 at 13:59 +0100, Gary S. Thompson wrote:

[cut]

</pre><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
output:

the processor implementation gives some  feedback as to what prcoessor
you are running:

M S&gt; script
M S&gt;
M S&gt;
M S&gt;
M S&gt;                                      relax repository checkout
M S&gt;
M S&gt;                           Protein dynamics by NMR relaxation data
analysis
M S&gt;
M S&gt;                              Copyright (C) 2001-2006 Edward
d'Auvergne
M S&gt;
M S&gt; This is free software which you are welcome to modify and
redistribute under the conditions of the
M S&gt; GNU General Public License (GPL).  This program, including all
modules, is licensed under the GPL
M S&gt; and comes with absolutely no warranty.  For details type 'GPL'.
Assistance in using this program
M S&gt; can be accessed by typing 'help'.
M S&gt;
M S&gt; processor = MPI running via mpi4py with 5 slave processors &amp; 1
master, mpi version = 1.2
M S&gt;
M S&gt; script = 'test_small.py'
M S&gt;
----------------------------------------------------------------------------------------------------

</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">
If using '-np 6', shouldn't the number of slaves be 6?


</pre><tt> 
</tt><tt>
</tt></blockquote><tt>nope there needs to be one processor which is the master and you just 
</tt><tt>tell  mpi how many prcoessors you want ( I will investigate running jobs 
</tt><tt>on the master in a thread at some point (maybe never depending ;-)) but 
</tt><tt>this places extra requirements on the mpi implimentation and is thus a 
</tt><tt>special case, I can give more details if you want me to)
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">
note the processor =  line
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

The processor line is in line with the 'script = ...' notation and that
information would be useful if the output is caught and stored in a log
file.  I like that touch.


</pre><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
another couple of things to note are that the output from the program
is prepended with some text indicating which stream and which
processors the output is coming from: The output prefix is divided
into two parts

'processor' 'stream'&gt;  [normal output line]

</pre><tt>where 
</tt><tt>processor is either a number to identify the rank of the processor, or
</tt><pre style="margin: 0em;">
a series of M's to indicate the master
stream is either E or S for the error or output streams

so here is another fragment

1 S&gt; Hessian calls:    0
1 S&gt; Warning:          None
1 S&gt;
M S&gt; idle set set([1, 2])
M S&gt; running_set set([2, 3, 4, 5])
M S&gt;
2 S&gt;
2 S&gt;
2 S&gt; Fitting to residue: 24 ALA
2 S&gt; ~~~~~~~~~~~~~~~~~~~~~~~~~~
2 S&gt;
2 S&gt; Grid search
2 S&gt; ~~~~~~~~~~~
2 S&gt;
2 S&gt; Searching the grid.
2 S&gt; k: 0       xk: array([ 0.


in this case we finish a minimisation on processor 1 '1 S&gt;'
then have some output from the master processor  'M S&gt;'
and then some output from prcoessor 2 '2 S&gt;'
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

This output is very useful.  I would prefer though if the notation:

[M S] relax&gt;

rather than:

M S&gt; relax&gt;

The text '1 S&gt;' looks like a prompt when it is not whereas [1 S] is less
ambiguous.

</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

thats fine I will do it

</pre><blockquote class="blockquote"><pre style="margin: 0em;">
On another note, I strongly believe that for ordinary operation this
output is not necessary.  The user doesn't need to know which slave
process the code has executed on.  All the user will care about is that
the calculation has occurred successfully.  Nevertheless this output is
very useful for programming and debugging purposes.  I propose that it
is shown when the --debug flag is passed to relax and suppressed
otherwise.  Importantly, that way the MPI or threading mode of operation
will look very similar to the normal uni-processor operation.  Or maybe
a verbosity flag to relax could be added to activate this printout?

</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

</pre><tt>certainly I think it is quite useful to have myself, because it assures 
</tt><tt>the user that work is actually being distributed. However, I was 
</tt><tt>planning to add a flag to switch it off anyway. Its just a case of taste 
</tt><tt>I guess;-)
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
when running under the threaded and mpi4py implimentations you may see
long gaps with no output and the output to the terminal can be quite
'jerky'. This is because the multiprcoessor implimentation uses a
threaded output queue to decouple the writing of output on the master
from the queuing of calculations on the slaves, as otherwise for
systems with slow io the rate of io on the mastewr can control the
rate of calculation!
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

I'll have to test this later and see if I can cosmetically minimise the
jerkyness.

</pre><tt> 
</tt><tt>
</tt></blockquote><tt>Your can't! Well ok you can but there are 'implications'. The jerkyness 
</tt><tt>is intrinsic to the  batching up of results from Slave_commands, so you 
</tt><tt>can switch off the batching of results from the Slave_processors but 
</tt><tt>this will put more stress on ther master and the interprocessor 
</tt><tt>communication fabric. If you want to return string results one line at a 
</tt><tt>time the design also allows you to do this, but again you stress the 
</tt><tt>master processor and the interprocessor communication fabric so possibly 
</tt><tt>slowing the overall calculation. Note also that what works well for a 
</tt><tt>computer with fast interprcoess interconnects will not work well on a 
</tt><tt>computer with a slow communication fabric. Anyway the message overall is 
</tt><tt>if you block/slow the master you can end up slowing the whole 
</tt><tt>multiprocessor....
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
also note the std error stream is not currently used as race
conditions between writing to the  stderr and stdout streams can lead
to garbled output.
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

This will definitely need to be fixed prior to merging into the 1.3
line.  Stdout and stderr separation is quite important.


</pre><tt> 
</tt><tt>
</tt></blockquote><tt>Indeed this is true and what I intend to do is to reintroduce an output 
</tt><tt>streem that splits output on the master based on what the lines prefix 
</tt><tt>is. This is all down to efficiency again, i vcould retuirn each line of 
</tt><tt>text as it is output to the output stream on the slave, however, so 
</tt><tt>there are not lots of objects to send between  processors I join the 
</tt><tt>streams together with the tags for identification of where the lien came 
</tt><tt>from.. The intention was to give the user the choice to split them again 
</tt><tt>at the other  end but, I still haven't had time to write that code.
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">
futher note that the implimentation includes a simple timer that gives
some bench marking as to the speed of calculation, this is the total
time that it takes for the master process to run

M S&gt; relax&gt; state.save(file='save', dir=None, force=1,
compress_type=1)
M S&gt; Opening the file 'save.bz2' for writing.
M S&gt;
M S&gt; overall runtime: 0:00:24
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

What triggers the time printout?  Does it occur at a specific location?
Does it occur multiple times during execution?

</pre><tt> 
</tt><tt>
</tt></blockquote><tt>no only once on completion, it is the last thing the processor does on 
</tt><tt>normal termination. See multi.prccessor.prerun and 
</tt><tt>multi.processor.postrun and multi.Application_callback
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
Interactive terminals: the multi implementation still has an
interactive terminal. Tis maybe started by typing mpiexec -np
6 ../relax --multi mpi4py      for example in the case of an mpi4py
session All io to the treminal takes place on the master processor,
but commands that are parallel still run across the whole cluster.
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

Perfect (although I already knew this)!
</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">
;-)
</pre><tt>note I think there maybe a problem with command completion etc I think 
</tt><tt>this is because the streams aren't notifying the system that they are 
</tt><tt>conencted to terminals as opposed to a file eg istty is not correctly 
</tt><tt>passed through. I will investigate sometime soon but it isn't my higest 
</tt><tt>priority ... laos the output can also look a bit manky but i will work 
</tt><tt>at this
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
Exceptions: exceptions from slave  processors appear with slightly
different stack traces compared to normal exceptions:


Traceback (most recent call last):
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 351, in run
   self.callback.init_master(self)
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/processor.py&quot;, 
line 75, in default_init_master
   self.master.run()
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/relax_tests_chris/../relax&quot;,
 line 177, in run
   self.interpreter.run()
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/prompt/interpreter.py&quot;, 
line 216, in run
   run_script(intro=self.relax.intro_string, local=self.local,
script_file=self.relax.script_file, quit=1)
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/prompt/interpreter.py&quot;, 
line 392, in run_script
   console.interact(intro, local, script_file, quit)
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/prompt/interpreter.py&quot;, 
line 343, in interact_script
   execfile(script_file, local)
 File &quot;test_small.py&quot;, line 54, in ?
   grid_search(name, inc=11)
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/prompt/minimisation.py&quot;,
 line 147, in grid_search
   self.relax.processor.run_queue()
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 270, in run_queue
   self.run_command_queue(lqueue)
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 335, in run_command_queue
   result_queue.put(result)
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 109, in put
   super(Threaded_result_queue,self).put(job)
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 76, in put
   self.processor.process_result(job)
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 221, in process_result
   result.run(self,memo)
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/processor.py&quot;, 
line 276, in run
   raise self.exception
Capturing_exception:

------------------------------------------------------------------------------------------------------------------------

 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 381, in run
   command.run(self,completed)
 File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/commands.py&quot;, 
line 297, in run
   raise 'dummy'


Nested Exception from sub processor
Rank: 1  Name: fbsdpcu156-pid31522
Exception type: dummy (legacy string exception)
Message: dummy

------------------------------------------------------------------------------------------------------------------------


here we have an exception 'dummy' which was raised at line 297, in the
run function /multi/commands.py on  slave 1 processor node  fbsdpcu156
process id 31522 and transferred back to line 276 of  function run in
</pre><tt>multi/processor.py on the master where it was raised again. 
</tt><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

I think that the error printout should be made to resemble the standard
Python printout.  For example if you raise a RelaxError with the text
'hello', the printout could look like:

   raise RelaxError, 'hello'
Nested Exception from sub processor
Rank: 1  Name: fbsdpcu156-pid31522
RelaxError: hello

Hence the exception type is not separated from its message.

</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">
Ok I can do that its only a question of munging  a format string

</pre><blockquote class="blockquote"><blockquote class="blockquote"><tt>Now some caveats 
</tt><tt>1. not all exceptions can be handled by this mechanism as they
</tt><pre style="margin: 0em;">
exceptions can only be handed back once communication between the
slaves has been setup. This can be a problem on some mpi
implimentations as they don't provide redirection of stdout back to
the master contolling trerminal.
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

There's probably not much that can be done there.
</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

</pre><tt>yes what I am looking at is putting output to a file one per processor 
</tt><tt>in this case
</tt><pre style="margin: 0em;">

(this won't work in all cases as some clusters don't have disk storage?)

</pre><blockquote class="blockquote"><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
2. I have had a few cases where raising an exception has wedged the
whole multiproessor without any output. These can be quite hard to
debug as they are due to errors in the overrides I put on the io
streams! a pointer that may help is that  using the
sys.settrace(traceit)  as shown in processor.py will produce copious
output tracing  (and a very slow program)
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

The sorting out and separation of the IO streams may cause this problem
to disappear.
</pre><tt> 
</tt><tt>
</tt></blockquote><tt>nope this may be  to do with exceptions being thrown on remote 
</tt><tt>proceessors and the master processor waiing infinitley long for 
</tt><tt>communication from dea processors...
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><pre style="margin: 0em;">

</pre><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
3. not all exception states seem to be leading to an exit from the
program currently so you should monitor output from the program
carefully
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

Do you know why this is happening?
</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">
no I am investigating

</pre><blockquote class="blockquote"><pre style="margin: 0em;">

</pre><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
Speedups
-----------

the following calculations are currently parallelised

1. model free minimisations across sets of residues with a  fixed
</pre><tt>difffusion tensor frame 
</tt><tt>2. model free grid searches for the difffusion tensor frame
</tt><pre style="margin: 0em;">
3. monte carlo simulations
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

This is great work!

</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
in future it maybe possible also parallelise the minimisation of
modelfree calculations of the 'all' case where model fitting and the
tensor frame are optimised at the same time. However,this will require
modifications to the model free hessian gradient and cuntion
calculation routines and development of a parallel newton line seach
which are both major undertakings.
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

These are possible targets for parallelisation but I would very strongly
recommend against working at this position.  And adding optimisation
algorithms would require very careful testing.  From my experience with
optimisation in the model-free space, I would probably bet that the
algorithm will fail for certain model-free motions (not many algorithms
find all minima in such a convoluted space).  The place to target is the
following three functions:
        maths_fns.mf.Mf.func_all()
        maths_fns.mf.Mf.dfunc_all()
        maths_fns.mf.Mf.d2func_all()

Specifically the loop over all residues (to be renamed to all spin
systems in the 1.3 line) to create the value, gradient, and Hessian
would be the ideal spot to parallelise!

</pre><tt> 
</tt><tt>
</tt></blockquote><tt>indeed this is what I thought and neil is working on it. One side note is that (if such a thing existed) a line search which is  adventitous and looked a superset of newton positions would work ;-) again it would have to be tested but all such things have to betested to some degree 
</tt><tt>
</tt><tt>note are tests for the cases for where lm failed in the test suite for 
</tt><tt>relax?
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">
Indeed the problem may be fine grained enough that use of c mpi and
recoding of the hessian etc calculations for model free in c is
required
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

This conversion should significantly speed up calculations anyway.  I
will do this one day.


</pre><tt> 
</tt><tt>
</tt></blockquote><tt>the later we do this the better c is a bind. I still think pyrex which 
</tt><tt>compiles what almost lloks like python to c woould be a good thing to 
</tt><tt>look at ;-) I might try an prototype something for you to look at at 
</tt><tt>some point
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">
speedups on all calculations with increasing numbers of processors
should be near perfect as alluded to in message
<a  rel="nofollow" href="https://mail.gna.org/public/relax-devel/2007-04/msg00048.html">https://mail.gna.org/public/relax-devel/2007-04/msg00048.html</a> more
benchmarks will follow soon

processors      min     eff     mc      eff     grid    eff
1               18      100     80      100     134     100
2               9       100
4               5       90
8               3       75
16              1       112.5
32              1       56.25   8       31.25   4       104.6

and the picture that speaks 1000 words

processors      min     eff     mc      eff     grid    eff
1               18      100     80      100     134     100
2               9       100
4               5       90
8               3       75
16              1       112.5
32              1       56.25   8       31.25   4       104.6

and the picture that speaks 1000 words




key top graph black line achieved runtimes
       top graph red line expected runtimes with perfect scaling
efficency
       bottom graph scaling efficiency
</pre><tt>       
</tt><tt>some notes
</tt><pre style="margin: 0em;">


0. data was collected on one of chris's small data sets containing 28
residues not all of which are active for minimisation columns
</pre><tt>       processors     - no slave  mpi processors 
</tt><tt>       min                    - time for a minimisation of models
</tt><pre style="margin: 0em;">
m1-m9 with a fixed diffusion tensor
       eff                     - approximate parallel efficiency
expected runtime/ actual runtime
       mc                     - 256 monte carlo calculations
       eff                     - efficiency of the above
       grid                   - a grid search on a anisotropic
diffusion tensor 6 steps
       eff                     - efficency of the above
    tests were run on a cluster of opterons using gigabit ethernet
and mpi
1. these results are crude wall times as measured by pythons time.time
function for the master but they do not include startup and shutdown
overhead
2. these tests are single point measurements there are no statistics
3. timings were rounded to 1 second, so for example we must consider
data points for  more than 16 processors for the min run to be suspect

key top graph black line achieved runtimes
       top graph red line expected runtimes with perfect scaling
efficency
       bottom graph scaling efficiency

note if you watch the output carefully you will see one difference
between the multiprocessor and uniprocessor runs of the grid search.
The grid search reports all cases of the search where the target
function has improved for each processor, rather than for the whole
grid search....
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

I wonder what is causing the Monte Carlo simulations to not be 100%
efficient.  Of all the code in relax, I would see this as the most
amenable for parallelisation.  Each simulation can be queued to a
different slave.
</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

</pre><tt>I agree this may be an artefact of they way our cluster is setup... or a 
</tt><tt>oneoff problem due to contention on its ethernet fabric (I said I didn't 
</tt><tt>do any statistics
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><pre style="margin: 0em;">

</pre><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
Bugs missing freatures todos etc:
-------------------------------------

1. There is very little commenting
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

This is quite important for the future maintainability of the code.

</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">
well its on the list

</pre><blockquote class="blockquote"><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
2. some exceptions do not stop the interpreter properly and there may
still be some bugs that cause lockups on throwing exceptions
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

Yikes.


</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">
well I have sorted everying I can find but... we need more tests

</pre><blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">
3. there are no unit tests (though the amount of code that can be unit
tested is rather limited as for example writing mock objects for mpi
could be fun!)
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

Most parts of the code could be tested relatively easily.  The unit test
framework makes this job quite easy.

</pre><tt> 
</tt><tt>
</tt></blockquote><tt>Indeed ;-) I can test everything except the comnmunication which would 
</tt><tt>require me to write  a mock mpi object (ouch) though I do now have some 
</tt><tt>thoughts on this...
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
4. there are no documentation strings
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

I would recommend compiling the API documentation using scons and then
looking at the HTML output.  That output should very clearly show what
docstrings are missing or deficient.


</pre><tt> 
</tt><tt>
</tt></blockquote><tt>well until last weeknothing wa documented  so I didn't bother! Also I 
</tt><tt>note that epydoc has a checking mode that will note which interfaces are 
</tt><tt>undocumented...
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">
5. the command line handling need to be improved: we need to find the
current processor implimentation, load it and then ask it what command
line options it needs (this will also allow the simplification of the
handling of setting up the number of processors and allow
multiprocessor that need more command line arguments such as ssh
tunnels to get extra arguments) I will also have to design a way of
getting the help text for all the processor command line options
whether they are loaded or not
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

I don't follow.  The user shouldn't be asked a question by relax.
</pre><tt> 
</tt><tt>
</tt></blockquote><tt>when I say 'ask the user' I really mean interrogate the command line I 
</tt><tt>was just being a bit loose in my terminology
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><pre style="margin: 0em;">

</pre><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
6. there are many task comments littered around the code FIXME: TODO:
etc all of these except the ones labelled PY3K: will need to be
reviewed resolved and removed
7. the relax class still has much code for the slave command setup
which needs to be removed as the multi module replaces it
8. The Get_name_command hasn't been tested recently especially across
all of the current processor fabrics
9. there needs to be a way of running the relax system test suite
againnst a list of processor fabrics
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

I disagree.  The unit tests should test all functions (except those
requiring missing dependancies).  The system/functional tests should be
run in the current mode of operation.  Therefore to test the operation
of relax against the different processor fabrics you would run the test
suite multiple times in those different modes.


</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

</pre><tt>what I was thinking of was allowing --multi=mpi4py,threads,uni for the 
</tt><tt>system tests which would repatedly run the system tests with the 
</tt><tt>different implimentations
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">
10. code to control the use of batched command queueing and returning,
and the threaded output queue  has been implimented but hasn't got an
interface to turn it on and off yet
11.  the command queuing code has an idea of  how many grains there
should be per processor. This isn't under use control  at the moment
(the grainyness contols how many batches of commands each processor
should see , take for example 3 slaves and 18 commands with a
grainyness of 1  .  On the task queue they  would be divided up into 3
batched commands one for each processor with each batched command
containing 6 sub commands. With a grainyness of 3 there would be 9
batched commands with each batched command containing 2 commands).
This allow for some load balancing on more hetrogenous systems as the
batched commands are held in a queue and handed out to the slave
processors as the slaves become available.
12. some of the output prefixing has off by 1 errors
13. re segregation of  the stdout and  stderr streams back out into
their correct streams is not implimented; everything is reported on
stdout. This will require work for the uni_processor as well
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

</pre><tt>I think that the two should never be combined. 
</tt><tt>
</tt></blockquote><tt>see my comments above about why I combine stout and stderr and how its a 
</tt><tt>requirement for accounable efficiency ;-) since they are tagged 
</tt><tt>recombinig them is not a problem....
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><pre style="margin: 0em;">
This makes it
challenging with the treading, but it is doable (I did it with my
ancient threading code).


</pre><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
14.  parellisation of hessian calculations and the all minimisation
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

See above.


</pre><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
15 . it would be good to give users control of which parts of the
program are parallelised during a run
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

I don't know if this is important or useful from the user's perspective.


</pre><tt> 
</tt><tt>
</tt></blockquote><tt>It can be important for speed. Consider if you have a cluster with slow 
</tt><tt>communication you might well want to run the monte carlos in parallel 
</tt><tt>but not the (currently unimplimented) all hessian  optimistaion as the 
</tt><tt>overhead of sending out the parts of the hessian could well overwhelm 
</tt><tt>the gains you make if you communication is slow
</tt><pre style="margin: 0em;">

</pre><blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">
16 . uni processor could be implimented as a s subclass of
multi_processor
17.  true virtual classes are not implimented
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

What is a virtual class?

</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

sorry me being sleep I meant abstract class

</pre><blockquote class="blockquote"><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
18.  the stdio stream interceptors should be implimented as delegates
to StringIO rather than inheriting from StringIO which would also
allow for the use of cStringIO
19. The master processor only does io and no calculations
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

Is that not how it currently works?

</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">
yep but the question is are we wasting resources and can it be improved?

</pre><blockquote class="blockquote"><tt> 
</tt><tt>
</tt><blockquote class="blockquote"><pre style="margin: 0em;">
anyway thats it for now
</pre><tt>   
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

Again I have to say that this code is looking really good.  This should
be released as a proper relax release.  I would look at the instructions
for creating a relax release.  A tag needs to be created and the source
and binary packages created.  For that you will need to create a GPG key
pair specifically for relax and then send me your public key.  Then you
will be able to package and upload signed files to the download site.  I
can then check the packages and sign them with the relax key.
</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">

</pre><tt>I will look into it soon and try to make a release in the next couple of 
</tt><tt>weeks, I certainly need to iron out this bug of neils first
</tt><pre style="margin: 0em;">


regards
gary

</pre><blockquote class="blockquote"><pre style="margin: 0em;">
Cheers,

Edward


_______________________________________________
relax (<a  rel="nofollow" href="http://nmr-relax.com">http://nmr-relax.com</a>)

This is the relax-devel mailing list
relax-devel@xxxxxxx

To unsubscribe from this list, get a password
reminder, or change your subscription options,
visit the list information page at
<a  rel="nofollow" href="https://mail.gna.org/listinfo/relax-devel">https://mail.gna.org/listinfo/relax-devel</a>

.

</pre><tt> 
</tt><tt>
</tt></blockquote><pre style="margin: 0em;">


--
-------------------------------------------------------------------
Dr Gary Thompson
Astbury Centre for Structural Molecular Biology,
University of Leeds, Astbury Building,
Leeds, LS2 9JT, West-Yorkshire, UK             Tel. +44-113-3433024
email: garyt@xxxxxxxxxxxxxxx                   Fax  +44-113-2331407
-------------------------------------------------------------------




</pre>
<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="00032" href="msg00032.html">Re: The multi-processor branch.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="00000" href="msg00000.html">Re: r3280 - /branches/multi_processor/multi/</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00017" href="msg00017.html">The multi-processor branch.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer"></div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Tue May 29 10:40:51 2007</div>  
</body>
</html>
