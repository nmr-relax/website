<!-- MHonArc v2.6.10 -->
<!--X-Subject: Re: relax, MPI, and Grid computing. -->
<!--X-From-R13: "Unel E. Fubzcfba" <tnelgNozo.yrrqf.np.hx> -->
<!--X-Date: Tue, 20 Mar 2007 16:01:12 +0100 -->
<!--X-Message-Id: 45FFF714.7070903@bmb.leeds.ac.uk -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: 7f080ed10703191030h79036e06w56912aa1d9130f48@mail.gmail.com -->
<!--X-Reference: 45FF0E84.1060007@bmb.leeds.ac.uk -->
<!--X-Reference: 1174384147.29205.20.camel@pc172&#45;31&#45;2&#45;63.biochem.unimelb.edu.au -->
<!--X-Reference: 45FFEC5C.7060205@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703200727l5fad8e72if4cf9aff74bc21@mail.gmail.com -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: relax, MPI, and Grid computing. -- March 20, 2007 - 16:01</title>
<link rel="stylesheet" type="text/css" href="/mail.gna.org/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="/mail.gna.org/images/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: relax, MPI, and Grid computing.</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00112" class="tabs">Index by Date</a> | <a href="threads.html#00112" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00111.html">Date Prev</a>] [<a href="msg00113.html">Date Next</a>] [<a href="msg00110.html">Thread Prev</a>] [<a href="msg00114.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: Edward d'Auvergne &lt;edward@xxxxxxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Date</em>: Tue, 20 Mar 2007 15:00:36 +0000</li>
<li class="menuitem">
<em>Cc</em>: relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00112.html">45FFF714.7070903@bmb.leeds.ac.uk</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;<a href="msg00092.html">7f080ed10703191030h79036e06w56912aa1d9130f48@mail.gmail.com</a>&gt;	&lt;<a href="msg00093.html">45FF0E84.1060007@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00097.html">1174384147.29205.20.camel@pc172-31-2-63.biochem.unimelb.edu.au</a>&gt;	&lt;<a href="msg00107.html">45FFEC5C.7060205@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00110.html">7f080ed10703200727l5fad8e72if4cf9aff74bc21@mail.gmail.com</a>&gt;</li>
<li class="menuitem">
<em>User-agent</em>: Mozilla Thunderbird 1.0 (X11/20041206)</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Gary S. Thompson</strong> on March 20, 2007 - 16:01:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre style="margin: 0em;">Edward d'Auvergne wrote:</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">On 3/21/07, Gary S. Thompson &lt;garyt@xxxxxxxxxxxxxxx&gt; wrote:</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">Edward d'Auvergne wrote:</pre><br>
<pre style="margin: 0em;">&gt;On Mon, 2007-03-19 at 22:28 +0000, Gary S. Thompson wrote:
&gt;
&gt;
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<blockquote class="blockquote"><tt>&gt;&gt; I also had a look at save state in state.py and this<br>
&gt;&gt;seems quite heavy I presume that it dumps the complete program 
state to<br>
&gt;&gt;a pickle and then rejuvenates it at the other side?<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;<br>
&gt;This pickles solely the relax data storage object (which is now a<br>
&gt;singleton) as this is the program state.  All permanent data and<br>
&gt;settings in relax must be stored in this object.  This object in a<br>
&gt;pickled state is not what you'd need for MPI though - the object is too<br>
&gt;big for inter-node communication.<br>
&gt;<br>
not necessarily it depends on how coarse grained your calculations are<br>
and how long they run for. What I am doing with the multi frameworks is<br>
effectivley just a wrapper round something similar to the therad code<br>
plus at the moment an implimentation and that uses complete state dumps<br>
(obviously if I can use more focused dumps things become faster)
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">This structure is BIG!  Especially if Monte Carlo simulations are run.
I'd avoid sending it at all costs!  Unless sending many, many
megabytes between the nodes is acceptable.</pre><br>
<br>
</blockquote><pre style="margin: 0em;">no not good ;-)</pre><br>
<blockquote class="blockquote"><blockquote class="blockquote"><pre style="margin: 0em;">&gt;Maybe there should be a model-free specific method called
&gt;'minimise_mpi()' which is copied from the current 'minimise()' method.
&gt;I think this is the area of relax which should be targeted.
&gt;
yes but not mpi just  a multiprocessor specific version
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>Ah.  I get what you are trying to do!
</tt></blockquote><pre style="margin: 0em;"><br>;-) sorry half baked code can be confusing to those eating it!</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br></pre><br>
<blockquote class="blockquote"><tt>&gt;&gt;One thing to note  here is that I will at some stage try and rewrite<br>
&gt;&gt;commands to keep the slave states in sync as we run so we don't 
have to<br>
&gt;&gt;save the whole state. But that is for a later day, or never if you<br>
&gt;&gt;consider that to not be the way to go...<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;<br>
&gt;If you are working at the model-free 'minimise()' method level I think<br>
&gt;you will get the best efficiency out of a cluster!  The granulatity<br>
&gt;would be perfect - not too fine that inter-node communication is the<br>
&gt;limiting factor and not too coarse that the used nodes of the cluster<br>
&gt;will be underutilised.<br>
&gt;<br>
&gt;At this low level the program state and the relax data storage 
object do<br>
&gt;not even come into play.  Hence the slave program state will be<br>
&gt;untouched and remain at the initial state for as long as it exists.<br>
&gt;This will probably be the simplest solution to implement as well.  This<br>
&gt;is what I eventually plan to do for the grid computing but you are<br>
&gt;welcome to beat me to it.<br>
&gt;<br>
&gt;ok i wll go for it<br>
&gt;<br>
&gt;<br>
&gt;&gt;more questions<br>
&gt;&gt;<br>
&gt;&gt;where should I be attacking the division problem?<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;<br>
&gt;Unless you see a point of using threads in the MPI code, don't attack<br>
&gt;the division problem.  That one is my problem!<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
sorry I don't follow I need to divide upr the jobs otherwise I have<br>
nothing to work with??
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">Sorry, I though you meant separating the grid computing code from the
threading code.</pre><br>
<br>
<blockquote class="blockquote"><tt>&gt;&gt; my main thought was to<br>
&gt;&gt;effectivley add restrictions to a some commands. So consider grid 
search<br>
&gt;&gt;I would add an extra parameter at the generic  and functional levels<br>
&gt;&gt;which would give a range of steps within the current parameters to<br>
&gt;&gt;calculate.... e.g here are the ranges which give a grid of 10x10x10 ie<br>
&gt;&gt;1000 steps. slave 1. you calculate 1-250 slave 2. 251-500 and so 
on.....<br>
&gt;&gt;is this the correct way to go?<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;<br>
&gt;Subdividing the grid search will be an interesting problem!  Should it<br>
&gt;be at the 'generic_fns' level, the 'specific_fns' level, or implemented<br>
&gt;directly into the minimisation package?  I think that the 
'specific_fns'<br>
&gt;level, again within the 'minimise()' model-free method (copied, 
modified<br>
&gt;for MPI, and renamed to 'minimise_mpi()') would be the best place to<br>
&gt;target.<br>
&gt;<br>
&gt;An algorithm to subdivide the grid would be useful.  Then an algorithm<br>
&gt;to collect the results and determine which subspace of the grid has the<br>
&gt;point with the lowest chi2 value should be used.  I.e. this will be an<br>
&gt;MPI-oriented grid search over a number of standard grid searches.<br>
&gt;<br>
&gt;However your best MPI gains are likely to be achieved by sending each<br>
&gt;grid search to a different node.  This higher level would be shared 
with<br>
&gt;the standard model-free optimisation code and hence you don't need to<br>
&gt;worry about writing separate MPI code for the grid search and for the<br>
&gt;minimisation.  Slight improvements may be achieved by breaking up the<br>
&gt;grid search, but I would personally tackle this later on.<br>
&gt;<br>
&gt;<br>
&gt;</tt><br>
<br>
<pre style="margin: 0em;">again I need to think about this. However if this uses divisons by model
again it will perform poorly as the different models will take different
amounts of time to calculate so many processors will sit idle...
again if I am not undertsanding properly please accept my apologies
relax is very heavily layered and alot of names are repeated multiple
times it can be quite had to follow whatis going on in the code base ;-)
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">Idle time will be inevitable.  Especially when it comes to the 'all'
model-free minimisation instance (the optimisation of all model-free
model parameters for all residues together with all diffusion
parameters).  That cannot be avoided.</pre><br>
</blockquote><tt>I agree especially if you are going for the low hanging fruit. In the 
case of the all minimisation it might be possible to  parallelise it  
but the code would have to work at a much lower level and many messages 
would have to be passed. Whether it would work or not would depend on 
how frequently nodes had to update data between themselves.  Certainly 
it would only work well on a homogenous system or a fairly well balanced 
hetrogenous system (i.e. you would have to know the relative speed of 
each slave and weight the amount of work it did correctly)</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">Bye,</pre><br>
<pre style="margin: 0em;">Edward</pre><br>
<pre style="margin: 0em;">.</pre><br>
</blockquote><pre style="margin: 0em;">regards
gary</pre><br>
<pre style="margin: 0em;">--
-------------------------------------------------------------------
Dr Gary Thompson
Astbury Centre for Structural Molecular Biology,
University of Leeds, Astbury Building,
Leeds, LS2 9JT, West-Yorkshire, UK             Tel. +44-113-3433024
email: garyt@xxxxxxxxxxxxxxx                   Fax  +44-113-2331407
-------------------------------------------------------------------</pre><br>
<pre style="margin: 0em;"><br></pre><br>
<br>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="00114" href="msg00114.html">Re: relax, MPI, and Grid computing.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="00092" href="msg00092.html">relax and Grid computing.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00093" href="msg00093.html">Re: relax and Grid computing.</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00097" href="msg00097.html">relax, MPI, and Grid computing.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00107" href="msg00107.html">Re: relax, MPI, and Grid computing.</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00110" href="msg00110.html">Re: relax, MPI, and Grid computing.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer"></div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Tue Mar 20 16:20:31 2007</div>  
</body>
</html>
