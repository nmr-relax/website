<!-- MHonArc v2.6.10 -->
<!--X-Subject: Re: r3237 &#45; in /branches/multi_processor: multi/mpi4py_processor.py	relax -->
<!--X-From-R13: "Unel E. Fubzcfba" <tnelgNozo.yrrqf.np.hx> -->
<!--X-Date: Tue, 20 Mar 2007 00:22:08 +0100 -->
<!--X-Message-Id: 45FF1AF7.2000606@bmb.leeds.ac.uk -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: E1HTJY1&#45;0005VF&#45;O1@subversion.gna.org -->
<!--X-Reference: 45FEAC46.8020307@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703190923s4b09fab1vd958ed6001031b9c@mail.gmail.com -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax -- March 20, 2007 - 00:22</title>
<link rel="stylesheet" type="text/css" href="/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="https://gna.org/images/gna.theme/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00094" class="tabs">Index by Date</a> | <a href="threads.html#00094" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00093.html">Date Prev</a>] [<a href="msg00095.html">Date Next</a>] [<a href="msg00091.html">Thread Prev</a>] [<a href="msg00098.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: Edward d'Auvergne &lt;edward@xxxxxxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Date</em>: Mon, 19 Mar 2007 23:21:27 +0000</li>
<li class="menuitem">
<em>Cc</em>: relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00094.html">45FF1AF7.2000606@bmb.leeds.ac.uk</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;E1HTJY1-0005VF-O1@subversion.gna.org&gt;	&lt;<a href="msg00088.html">45FEAC46.8020307@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00091.html">7f080ed10703190923s4b09fab1vd958ed6001031b9c@mail.gmail.com</a>&gt;</li>
<li class="menuitem">
<em>User-agent</em>: Mozilla Thunderbird 1.0 (X11/20041206)</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Gary S. Thompson</strong> on March 20, 2007 - 00:22:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre style="margin: 0em;">Edward d'Auvergne wrote:</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">Hi,</pre><br>
<pre style="margin: 0em;">Just a quick point, it would be good to either start a new thread for
these types of questions or changing the subject field (unless you use
the Gmail web interface like I am at the moment).  People may miss
important discussions with such scary subject lines!</pre><br>
<pre style="margin: 0em;"><br>On 3/20/07, Gary S. Thompson &lt;garyt@xxxxxxxxxxxxxxx&gt; wrote:</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">garyt@xxxxxxxxxxxxxxx wrote:</pre><br>
<pre style="margin: 0em;">Dear Ed
d this is a good enough point to tell you how to run things and what to
install for the mpi version i am testing with</pre><br>
<tt>I have installed mpipy and lam
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>Are both, together with mpi4py, essential for MPI operation?
</tt></blockquote><tt><br>yes lam is an mpi implementation and mpi4py is the interface code 
between the c world of mpi and the python world</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">As MPI
is solely for those who are very serious and have access to clusters,
</pre></blockquote><tt><br>not true! mpi can run over ssh as well. For example lam has an ssh 
backend and this is what i am using for testing on my computer!</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">the user should be able handle installing these dependencies (or at
least be able to get someone to install it for them).</pre><br>
<br>
</blockquote><tt>lam or mpich is often available in a linux distributions software 
repository e.g 
<a  href="http://rpmfind.net/linux/rpm2html/search.php?query=lam-devel">http://rpmfind.net/linux/rpm2html/search.php?query=lam-devel</a> lists rpms 
for mandriva and a variety of other platforms. I chose lam to play with 
because it is the best performing library on our cluster, but have also 
used mpich. mpi4py has instructions  on building and seetting up a 
variety of  configurations</tt><br>
<br>
<blockquote class="blockquote"><blockquote class="blockquote"><tt>lam should  be available in your linux distribution ??? ;-) run 
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">I'm using Mandriva so I would assume that it is within it's 'contrib'
repositories.  You are talking about <a  href="http://www.lam-mpi.org/">http://www.lam-mpi.org/</a> aren't
you?  As OpenMPI (<a  href="http://www.open-mpi.org/">http://www.open-mpi.org/</a>) seems to be the future of
that project, wouldn't this be the better option?  It's more likely to
be supported in future linux distros.</pre><br>
</blockquote><tt>see above it shouldn't matter which mpi distribution you choose they all 
have almost  identical apis and mpi4py deals with that</tt><br>
<br>
<blockquote class="blockquote"><br>
<blockquote class="blockquote"><tt>mpi4py came from <a  href="http://www.python.org/pypi/mpi4py">http://www.python.org/pypi/mpi4py</a> (there is an mpi4py<br>
website but it is out of date, however, mpi4py is under recent 
development)
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>Do you think mpi4py 0.4 or below will be stable enough?  Are there 
alternatives?
</tt></blockquote><tt><br>I would use mpi4py 0.4.0rc4 from the cheese shop. This is that I have 
developed with and it seems to work _well___<br>
&lt;<a  href="http://cheeseshop.python.org/pypi/mpi4py/0.4.0rc4">http://cheeseshop.python.org/pypi/mpi4py/0.4.0rc4</a>&gt;</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br></pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">follow the instructions to install mpi4py</pre><br>
<pre style="margin: 0em;">create the file  'test_multi1.py'</pre><br>
<pre style="margin: 0em;">-------------------8&lt;-----------------------
import multi
cmd  =  multi.mpi4py_processor.Get_name_command()
self.relax.processor.run_command(cmd)
-------------------8&lt;-----------------------</pre><br>
<pre style="margin: 0em;">then type lamboot</pre><br>
<pre style="margin: 0em;">and to run the test type:</pre><br>
<pre style="margin: 0em;">mpirun -np 6 python relax --multi mpi4py test_mult1.py</pre><br>
<pre style="margin: 0em;">to get:</pre><br>
<pre style="margin: 0em;">                                  relax repository checkout</pre><br>
<tt>                          Protein dynamics by NMR relaxation data 
analysis</tt><br>
<br>
<pre style="margin: 0em;">                             Copyright (C) 2001-2006 Edward d'Auvergne</pre><br>
<pre style="margin: 0em;">This is free software which you are welcome to modify and redistribute
under the conditions of the
GNU General Public License (GPL).  This program, including all modules,
is licensed under the GPL
and comes with absolutely no warranty.  For details type 'GPL'.
Assistance in using this program
can be accessed by typing 'help'.</pre><br>
<tt>script = 'test_mult1.py'<br>
---------------------------------------------------------------------------------------------------- </tt><br>
<br>
<pre style="margin: 0em;">import sys</pre><br>
<tt><br>import multi<br>
cmd  =  multi.mpi4py_processor.Get_name_command()<br>
self.relax.processor.run_command(cmd)<br>
---------------------------------------------------------------------------------------------------- </tt><br>
<br>
<pre style="margin: 0em;">1 fbsdpcu156-9377
2 fbsdpcu156-9378
3 fbsdpcu156-9379
4 fbsdpcu156-9380
5 fbsdpcu156-9381</pre><br>
</blockquote><pre style="margin: 0em;"><br>I'll have to play with this tomorrow.</pre><br>
<br>
<blockquote class="blockquote"><tt>hope this is useful!
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">I'm starting to get a better idea of how this will be implemented!</pre><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">Now a question what is the best way to get an etrenally running relax
interpreter I can just fire commands at (for the slaves)?
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>The prompt based interface (as well as the script interface) is only<br>
one way of invoking relax.  An important question is how should we<br>
present relax to the user when using MPI.  Should the parent process<br>
present a functional interpreter or should operation be solely<br>
dictated by a script?  
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>I already have a prompt running on the master, my idea is that the relax 
use should see no difference (apart from perfomance) when using he 
parallel version</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">Or should a completely different mechanism of
operation be devised for the interface of the parent.  For the grid
computing code the parent UI is either the prompt or the script while
the slaves use the interface started by 'relax --thread'.  The slave
use none of the user functions and only really invoke the number
crunching code.
</pre></blockquote><tt><br>now this is what I couldn't follow. How much of the relax environment is 
essential for a slave/thread and if we don't want to do it all by 
compete pickles of relax data structures (how big is a complete relax 
data structure for a typical set of runs?) where shoudl i start</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br>For the MPI slaves (I'm assuming these are separate processes with
different PIDs running on different nodes) we should avoid the
standard UI interfaces as these are superfluous.  In this case a
simple MPI interface should probably be devised - it accepts the MPI
commands and returns data back to the parent.  Is this though stdin,
stdout, and stderr in MPI?  My knowledge of MPI is very limited.</pre><br>
</blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>1. via picked objects  which will integrate the results back into the  
master copy or<br>
2. as text strings which are printed with a  process number in front (so 
that  complete parallel  log files can be grepped out of the main output).</tt><br>
<br>
<tt>I don't currently assume that the compute node has available or useable 
or shared  disk space.  everything comes back to the master</tt><br>
<br>
<pre style="margin: 0em;">the place to look at in mpi4py_processor is</pre><br>
<tt>lines 59-71  which sends a command from the master and either prints or 
exceutes the resulting object (checks for exceptions, repeated feedback, 
and command completion still to come)</tt><br>
<br>
<pre style="margin: 0em;">       for i in range(1,MPI.size):
           if i != 0:
               MPI.COMM_WORLD.Send(buf=command,dest=i)
       for i in range(1,MPI.size):
           buf=[]
           if i !=0:
               elem = MPI.COMM_WORLD.Recv(source=i)
               if type(elem) == 'object':
                   elem.run(relax_instance, relax_instance.processor)
               else:
                   #FIXME can't cope with multiple lines
                   print i,elem</pre><br>
<tt>and lines 92-94 where all the command are recieved and executes on the 
slaves<br>
         
        while not self.do_quit:<br>
               command = MPI.COMM_WORLD.Recv(source=0)<br>
               command.run(self.relax_instance, 
self.relax_instance.processor)</tt><br>
<br>
<pre style="margin: 0em;">it is the intention that the command protocol is very minimal</pre><br>
<tt>0. send a command plus data to run on the remote slave as an object 
which will get its run method executed with the local relax and 
processor as aruments master:communicator.run_command<br>
1. the slave then writes a series of objects to the processsor method 
return_object<br>
2. master recieves data either<br>
  a. objects to execute on the master which will also be given the 
master relax and processor instances<br>
  b.string objects to print<br>
  c. a command completion indicator back from the slave (a well known 
object)<br>
  d. an exception (raising of a local exception ont master which will 
do stack trace printing for the master and the slave)<br>
  e. None  a void  return</tt><br>
<br>
<br>
<blockquote class="blockquote"><br>
<blockquote class="blockquote"><pre style="margin: 0em;">basically I just need to</pre><br>
<pre style="margin: 0em;">1. do some setup
2. send a command object</pre><br>
<tt>but without quitting at the end of the script
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">If the prompt and scripting UI interface are not used by the slaves,
this shouldn't be an issue.  The parent should hang and wait at the
'grid_search()' and 'minimise()' user functions until these complete.
No other code needs to be executed by MPI.</pre><br>
</blockquote><pre style="margin: 0em;">that is indeed the case</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">All that needs to be done is to send the minimal amount of data to the
slave (see the minimise() method of the specific_fns.model_free code
for the objects required in this case), run the specific optimisation
code, and then return the parameter vector and minimisation stats back
to the parent.</pre><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">do i modify interpreter.run to take a quit varaiable set to False  so
that run_script can be
run with quit = false?
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">Avoid the interpreter and wait at the optimisation steps - the only
serious number crunching code in relax.
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>I agree!  interpeters are not required on the slave just the relax data 
structures in a clean and useable state</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br>I hope this helps,</pre><br>
<pre style="margin: 0em;">Edward</pre><br>
<pre style="margin: 0em;">.</pre><br>
</blockquote><pre style="margin: 0em;">regards
gary</pre><br>
<pre style="margin: 0em;">One other quetion: how well behaved are the relax functions with not 
gratuitously modifying global state. e.g could I share one relax instance 
between several threads? The reason I ask is that in the case that they are 
well behaved  many of the data transfer operations in a threaded environment 
with a single  memory space would become noops ;-) nice!</pre><br>
<pre style="margin: 0em;"><br>--
-------------------------------------------------------------------
Dr Gary Thompson
Astbury Centre for Structural Molecular Biology,
University of Leeds, Astbury Building,
Leeds, LS2 9JT, West-Yorkshire, UK             Tel. +44-113-3433024
email: garyt@xxxxxxxxxxxxxxx                   Fax  +44-113-2331407
-------------------------------------------------------------------</pre><br>
<pre style="margin: 0em;"><br></pre><br>
<br>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="00099" href="msg00099.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00098" href="msg00098.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="00088" href="msg00088.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00091" href="msg00091.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer">You are on the <a href="http://gna.org">Gna!</a> mail server.</div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Tue Mar 20 11:40:52 2007</div>  
</body>
</html>
