<!-- MHonArc v2.6.16 -->
<!--X-Subject: Re: r3280 &#45; /branches/multi_processor/multi/ -->
<!--X-From-R13: "Unel E. Fubzcfba" <tnelgNozo.yrrqf.np.hx> -->
<!--X-Date: Fri, 04 May 2007 15:00:27 +0200 -->
<!--X-Message-Id: 463B2E31.30503@bmb.leeds.ac.uk -->
<!--X-Content-Type: multipart/alternative -->
<!--X-Reference: E1HjkOC&#45;0007zq&#45;7e@subversion.gna.org -->
<!--X-Derived: jpg3S9LoMqzqS.jpg -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: r3280 - /branches/multi_processor/multi/ -- May 04, 2007 - 15:00</title>
<link rel="stylesheet" type="text/css" href="/mail.gna.org/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="/mail.gna.org/images/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: r3280 - /branches/multi_processor/multi/</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00000" class="tabs">Index by Date</a> | <a href="threads.html#00000" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[Date Prev] [<a href="msg00001.html">Date Next</a>] [Thread Prev] [<a href="msg00016.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: garyt@xxxxxxxxxxxxxxx</li>
<li class="menuitem">
<em>Date</em>: Fri, 04 May 2007 13:59:29 +0100</li>
<li class="menuitem">
<em>Cc</em>: a.kalverda@xxxxxxxxxxx, Tom Burnley &lt;bmbbtb@xxxxxxxxxxxxxxx&gt;,	relax-devel@xxxxxxx, Neil Syme &lt;bmbnrs@xxxxxxxxxxx&gt;,	Steve Homans &lt;s.w.homans@xxxxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00000.html">463B2E31.30503@bmb.leeds.ac.uk</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;E1HjkOC-0007zq-7e@xxxxxxxxxxxxxxxxxx&gt;</li>
<li class="menuitem">
<em>User-agent</em>: Mozilla Thunderbird 1.0 (X11/20041206)</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Gary S. Thompson</strong> on May 04, 2007 - 15:00:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<table width="100%"><tr><td bgcolor="#ffffff" style="background-color: #ffffff; color: #000000; "><font color="#000000">



<a rel="nofollow" class="moz-txt-link-abbreviated" href="mailto:garyt@xxxxxxxxxxxxxxx">garyt@xxxxxxxxxxxxxxx</a> wrote:
<blockquote cite="" type="cite">
  <pre wrap="">Author: varioustoxins
Date: Fri May  4 01:07:07 2007
New Revision: 3280

URL: <a rel="nofollow" class="moz-txt-link-freetext" href="http://svn.gna.org/viewcvs/relax?rev=3280&view=rev">http://svn.gna.org/viewcvs/relax?rev=3280&amp;view=rev</a>
Log:
threaded multi processor support written, multi is now beta! 
multi now supports three substrates: mpi4py, uniprocessor, and threads

Added:
    branches/multi_processor/multi/prependStringIO.py
      - copied, changed from r3275, branches/multi_processor/multi/PrependStringIO.py
Removed:
    branches/multi_processor/multi/PrependStringIO.py
Modified:
    branches/multi_processor/multi/commands.py
    branches/multi_processor/multi/mpi4py_processor.py
    branches/multi_processor/multi/multi_processor.py
    branches/multi_processor/multi/processor.py
    branches/multi_processor/multi/uni_processor.py
  </pre>
</blockquote>
[snip]<br>
<br>
Following on from the above commit this is an overview of&nbsp; the multi
processor branch, a review of the state of the branch,&nbsp; how to use it,&nbsp;
outstanding items and changes required.<br>
<br>
Architecture <br>
--------------<br>
<br>
The processor extensions act as a wrapper around the core of relax and
with relativley minimal changes (see for example
multi.commands.MF_minimise_command and <br>
specific_fns.model_free.minimse()) which allows relax to distribute&nbsp;
distribute computational tasks in a mater slave manner on a&nbsp; number of
different processor 'fabrics'.&nbsp; The a fabric is thus&nbsp; defines a
mechanism of distributing computational tasks. The three fabrics
currently supported are <br>
<br>
- uni a simple single processor fabric that doesn't operate relax in
parallel and replicates the results that a normal relax tag 1.3.0
session would produce. This is the default fabric and is present to
provide a unified relax for both parallel and non parallel
architectures. (this may seem redundant as running the thread fabric&nbsp;
with 1 slave processor gives the same result; however, python can be
compiled without thread support and this provides an implementation for
these python configurations).<br>
- thread this implimentation runs calculations on parallel threads
within the same machine and is ideal for a shared memory processor such
as one of the latest workstaions with multiple chips with multiple cores<br>
-mpi4py this implimentation uses the mpi4py library to communicate
using&nbsp; MPI (message passing interface) to communicate between a cluster
of processes either on the same machine or on a disjoint set of
machines. This is one of the common methods&nbsp; used to link computers in
beowulf clusters<br>
<br>
there are other ways that processes can communicate , and the
architecture of the multi module is such that adding a different
processor fabric implimentation is relativley simple. (see for example
how multi.mpi4py_processor is written as thin venear over the top of
multi.multi_processor and have very few lines of code (~120) [note the
name of multi_processor.py will change soon to multi_processor_base] ).
processor fabrics (multi.uni_prcoessor, multi,mpi4py_processor) are
loaded dynamically as plugins depending on the command line option
--multi/-m and so new processor fabrics are easily created and loaded.<br>
<br>
Processor fabrics which are obvious targets for implimentation include <br>
<br>
- other implimentations using&nbsp; different python mpi libraries (pypar
etc)<br>
- use of ssh tunnels for parallel programming<br>
- use of the twisted frame work for communication
<a rel="nofollow" class="moz-txt-link-freetext" href="http://twistedmatrix.com/projects/">http://twistedmatrix.com/projects/</a><br>
- the parallel virtual machine (pvm) via pypvm&nbsp;
<a rel="nofollow" class="moz-txt-link-freetext" href="http://pypvm.sourceforge.net">http://pypvm.sourceforge.net</a><br>
<br>
compilation<br>
-------------<br>
<br>
How to get and use the current implimentation. The currrent
implimentation is in a branch of the relax project and can be accessed
with the follwing subversion command:<br>
<br>
<h4>Checkout over SVN protocol (TCP 3690):</h4>
<pre>svn co svn://svn.gna.org/svn/relax/branches/multi_processor relax_multi</pre>
<h4>Checkout over http:</h4>
<pre>svn co <a rel="nofollow" class="moz-txt-link-freetext" href="http://svn.gna.org/svn/relax/branches/multi_processor">http://svn.gna.org/svn/relax/branches/multi_processor</a> relax_multi</pre>
<br>
the implimentaton has no extra dependencies from a vanilla relax
installation [ <a rel="nofollow" class="moz-txt-link-freetext" href="/download.html">http://www.nmr-relax.com/download.html</a>]&nbsp; apart from a
requirement for mpi4py if you are going to use the mpi4py processor
fabric. mpi4py can be obtained from <font size="-1"><b>the python
cheese shop [</b><span class="a"><b>cheeseshop</b>.<b>python</b>.org/pypi/]
at <a rel="nofollow" class="moz-txt-link-freetext" href="http://cheeseshop.python.org/pypi/mpi4py">http://cheeseshop.python.org/pypi/mpi4py</a>. You will need to compile
it against an mpi implimentation (I used lam</span></font><font
 size="-1"><span class="a">: www.<b>lam</b>-<b>mpi</b>.org. Though
other mpi impilmentations should work I have not tried them)</span></font><br>
<font size="-1"><span class="a"></span></font><font size="-1"><span
 class="a"><br>
Three important&nbsp; points to note when compiling&nbsp; the mpi4py code&nbsp; are
that</span></font><br>
<font size="-1"><span class="a"></span></font><br>
<font size="-1"><span class="a"></span></font><font size="-1"><span
 class="a">1. the mpi4py code must be compiled against a copy of the
mpi libraries whicht are in a shared object. so for example for lam
when you compuile it&nbsp; you need to use </span></font><br>
<font size="-1"><span class="a">&nbsp;./configure </span></font>--enable-shared
before you us 'make' and 'make install' so that you get a
lib&lt;xxx&gt;.so library as well as a lib&lt;xxx&gt;.a&nbsp; ater
compilation where &lt;xxx&gt; is the name of you mpi library (mpich,
lam etc)<br>
2. I believe&nbsp; the code for you mpi installation and needs to be
position independant when compiled on x86_64 machines; so you need to
use the -FPIC flag<br>
3. step 2 precludes the use of compilation with the portland groups
compilers as they don't seem to cope well with shared objects
(allegedly and in my hands)<br>
4. [ok i lied] I have compiled the code for mpi4py and tested under
linux with both 32 bit and 64 bit processors (in my case I used a
single processor machine setup to run as a 6 task lam mpi box for basic
sanity testing, for real testing I used a 148 processor cluster). I
have not tried things out on windows or on osx so you mileage may vary<br>
5. [ok i lied alot, see douglas adams for examples of how to do this
sort of thing in real style;-)]&nbsp; I believe you mpi implimentation
should be compiled with the same compiler as was used for your python
installation<br>
<br>
<font size="-1"><span class="a"></span></font>command line and running
the code<br>
---------------------------------------<br>
<br>
The multi branch adds two command line switches -m/--multi and
-n/--processors<br>
<br>
&nbsp; -m MULTIPROCESSOR, --multi=MULTIPROCESSOR<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; set multi processor method<br>
&nbsp; -n N_PROCESSORS, --processors=N_PROCESSORS<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; set number of processors (may be ignored)<br>
<br>
--multi &lt;MULTIPROCESSOR&gt; specifies the multi processor
implimentations to use and &lt;MULTIPROCESSOR&gt; defaults to 'uni'&nbsp;
The name of the processor to use should be the same as the first part
of one of the processor impimentation files in multi ie the
correspondences are<br>
<br>
'-m uni' loads: multi.uni_processor.Uni_processor from uni_processor.py<br>
'-m thread'&nbsp; loads multi.thread_processor.Thread_processor from
thread_processor.py<br>
'-m mpi4py'&nbsp; loads multi.mpi4py_processor.Mpi4py_processor from
mpi4py_processor.py<br>
<br>
--processors&nbsp; sets the number of slave processors to use for
calculation (there is currently always one extra master processor that
allocates jobs and sevrices i/o for the thread and mpi4py processor
fabrics) and is only supported by the thread implimentation.
Uniprocessor always only has one processor and the mpi implimentations
use the number of processors allocated to them by the mpi environment.<br>
<br>
as an example of using the mpi4py version here are the commands I use
to run a 6 processor run [1 master and 5 slaves] on my linux box:<br>
<br>
lamboot<br>
mpirun -np 6 relax --multi mpi4py test_small.py<br>
lamhalt<br>
#lamclean if lam halt returns failure<br>
<br>
the lamhalt may give errors, however, sometimes if you don't stop and
start lam cleanly you can get strange results<br>
<br>
output:<br>
<br>
the processor implementation gives some&nbsp; feedback as to what prcoessor
you are running:<br>
<br>
M S&gt; script<br>
M S&gt;<br>
M S&gt;<br>
M S&gt;<br>
M S&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; relax repository checkout<br>
M S&gt;<br>
M S&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Protein dynamics by NMR relaxation
data analysis<br>
M S&gt;<br>
M S&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Copyright (C) 2001-2006 Edward
d'Auvergne<br>
M S&gt;<br>
M S&gt; This is free software which you are welcome to modify and
redistribute under the conditions of the<br>
M S&gt; GNU General Public License (GPL).&nbsp; This program, including all
modules, is licensed under the GPL<br>
M S&gt; and comes with absolutely no warranty.&nbsp; For details type
'GPL'.&nbsp; Assistance in using this program<br>
M S&gt; can be accessed by typing 'help'.<br>
M S&gt;<br>
M S&gt; processor = MPI running via mpi4py with 5 slave processors
&amp; 1 master, mpi version = 1.2<br>
M S&gt;<br>
M S&gt; script = 'test_small.py'<br>
M S&gt;
----------------------------------------------------------------------------------------------------<br>
<br>
<br>
note the processor =&nbsp; line<br>
<br>
<br>
another couple of things to note are that the output from the program
is prepended with some text indicating which stream and which
processors the output is coming from: The output prefix is divided into
two parts<br>
<br>
'processor' 'stream'&gt;&nbsp; [normal output line]<br>
<br>
where <br>
processor is either a number to identify the rank of the processor, or&nbsp;
a series of M's to indicate the master<br>
stream is either E or S for the error or output streams<br>
<br>
so here is another fragment<br>
<br>
1 S&gt; Hessian calls:&nbsp;&nbsp;&nbsp; 0<br>
1 S&gt; Warning:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; None<br>
1 S&gt;<br>
M S&gt; idle set set([1, 2])<br>
M S&gt; running_set set([2, 3, 4, 5])<br>
M S&gt;<br>
2 S&gt;<br>
2 S&gt;<br>
2 S&gt; Fitting to residue: 24 ALA<br>
2 S&gt; ~~~~~~~~~~~~~~~~~~~~~~~~~~<br>
2 S&gt;<br>
2 S&gt; Grid search<br>
2 S&gt; ~~~~~~~~~~~<br>
2 S&gt;<br>
2 S&gt; Searching the grid.<br>
2 S&gt; k: 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xk: array([ 0.<br>
<br>
<br>
in this case we finish a minimisation on processor 1 '1 S&gt;'<br>
then have some output from the master processor&nbsp; 'M S&gt;'<br>
and then some output from prcoessor 2 '2 S&gt;'<br>
<br>
when running under the threaded and mpi4py implimentations you may see
long gaps with no output and the output to the terminal can be quite
'jerky'. This is because the multiprcoessor implimentation uses a
threaded output queue to decouple the writing of output on the master
from the queuing of calculations on the slaves, as otherwise for
systems with slow io the rate of io on the mastewr can control the rate
of calculation!<br>
<br>
also note the std error stream is not currently used as race conditions
between writing to the&nbsp; stderr and stdout streams can lead to garbled
output.<br>
<br>
futher note that the implimentation includes a simple timer that gives
some bench marking as to the speed of calculation, this is the total
time that it takes for the master process to run<br>
<br>
M S&gt; relax&gt; state.save(file='save', dir=None, force=1,
compress_type=1)<br>
M S&gt; Opening the file 'save.bz2' for writing.<br>
M S&gt;<br>
M S&gt; overall runtime: 0:00:24<br>
<br>
<b>Interactive terminals:</b> the multi implementation still has an
interactive terminal. Tis maybe started by typing mpiexec -np 6
../relax --multi mpi4py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for example in the case of an mpi4py
session All io to the treminal takes place on the master processor, but
commands that are parallel still run across the whole cluster.<br>
<br>
<br>
<b>Exceptions</b>: exceptions from slave&nbsp; processors appear with
slightly different stack traces compared to normal exceptions:<br>
<br>
<br>
Traceback (most recent call last):<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py",
line 351, in run<br>
&nbsp;&nbsp;&nbsp; self.callback.init_master(self)<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/processor.py",
line 75, in default_init_master<br>
&nbsp;&nbsp;&nbsp; self.master.run()<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/relax_tests_chris/../relax",
line 177, in run<br>
&nbsp;&nbsp;&nbsp; self.interpreter.run()<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/prompt/interpreter.py",
line 216, in run<br>
&nbsp;&nbsp;&nbsp; run_script(intro=self.relax.intro_string, local=self.local,
script_file=self.relax.script_file, quit=1)<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/prompt/interpreter.py",
line 392, in run_script<br>
&nbsp;&nbsp;&nbsp; console.interact(intro, local, script_file, quit)<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/prompt/interpreter.py",
line 343, in interact_script<br>
&nbsp;&nbsp;&nbsp; execfile(script_file, local)<br>
&nbsp; File "test_small.py", line 54, in ?<br>
&nbsp;&nbsp;&nbsp; grid_search(name, inc=11)<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/prompt/minimisation.py",
line 147, in grid_search<br>
&nbsp;&nbsp;&nbsp; self.relax.processor.run_queue()<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py",
line 270, in run_queue<br>
&nbsp;&nbsp;&nbsp; self.run_command_queue(lqueue)<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py",
line 335, in run_command_queue<br>
&nbsp;&nbsp;&nbsp; result_queue.put(result)<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py",
line 109, in put<br>
&nbsp;&nbsp;&nbsp; super(Threaded_result_queue,self).put(job)<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py",
line 76, in put<br>
&nbsp;&nbsp;&nbsp; self.processor.process_result(job)<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py",
line 221, in process_result<br>
&nbsp;&nbsp;&nbsp; result.run(self,memo)<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/processor.py",
line 276, in run<br>
&nbsp;&nbsp;&nbsp; raise self.exception<br>
Capturing_exception:<br>
<br>
------------------------------------------------------------------------------------------------------------------------<br>
<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py",
line 381, in run<br>
&nbsp;&nbsp;&nbsp; command.run(self,completed)<br>
&nbsp; File
"/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/commands.py",
line 297, in run<br>
&nbsp;&nbsp;&nbsp; raise 'dummy'<br>
<br>
<br>
Nested Exception from sub processor<br>
Rank: 1&nbsp; Name: fbsdpcu156-pid31522<br>
Exception type: dummy (legacy string exception)<br>
Message: dummy<br>
<br>
------------------------------------------------------------------------------------------------------------------------<br>
<br>
<br>
here we have an exception 'dummy' which was raised at line 297, in the
run function /multi/commands.py on&nbsp; slave 1 processor node&nbsp; fbsdpcu156&nbsp;
process id 31522 and transferred back to line 276 of&nbsp; function run in
multi/processor.py on the master where it was raised again. <br>
<br>
Now some caveats <br>
1. not all exceptions can be handled by this mechanism as they
exceptions can only be handed back once communication between the
slaves has been setup. This can be a problem on some mpi
implimentations as they don't provide redirection of stdout back to the
master contolling trerminal.<br>
2. I have had a few cases where raising an exception has wedged the
whole multiproessor without any output. These can be quite hard to
debug as they are due to errors in the overrides I put on the io
streams! a pointer that may help is that&nbsp; using the
sys.settrace(traceit)&nbsp; as shown in processor.py will produce copious
output tracing&nbsp; (and a very slow program)<br>
3. not all exception states seem to be leading to an exit from the
program currently so you should monitor output from the program
carefully<br>
<br>
Speedups<br>
-----------<br>
<br>
the following calculations are currently parallelised<br>
<br>
1. model free minimisations across sets of residues with a&nbsp; fixed&nbsp;
difffusion tensor frame <br>
2. model free grid searches for the difffusion tensor frame<br>
3. monte carlo simulations<br>
<br>
in future it maybe possible also parallelise the minimisation of
modelfree calculations of the 'all' case where model fitting and the
tensor frame are optimised at the same time. However,this will require
modifications to the model free hessian gradient and cuntion
calculation routines and
development of a parallel newton line seach which are both major
undertakings. Indeed the problem may be fine grained enough that use of
c mpi and recoding of the hessian etc calculations for model free in c
is required<br>
<br>
speedups on all calculations with increasing numbers of processors
should be near perfect as alluded to in message<br>
<a rel="nofollow" class="moz-txt-link-freetext" href="/mail.gna.org/public/relax-devel/2007-04/msg00048.html">https://mail.gna.org/public/relax-devel/2007-04/msg00048.html</a> more
benchmarks will follow soon<br>
<br>
<font color="#000000">
<pre>processors&nbsp;&nbsp;&nbsp; 	min    	eff	mc	eff	grid	eff</pre>
<pre>1		18	100	80	100	134	100</pre>
<pre>2		9	100</pre>
<pre>4		5	90</pre>
<pre>8		3	75</pre>
<pre>16		1	112.5</pre>
<pre>32		1	56.25	8	31.25	4	104.6

</pre>
and the picture that speaks 1000 words<br>
<br>
</font><font color="#000000">
<pre>processors&nbsp;&nbsp;&nbsp; 	min    	eff	mc	eff	grid	eff</pre>
<pre>1		18	100	80	100	134	100</pre>
<pre>2		9	100</pre>
<pre>4		5	90</pre>
<pre>8		3	75</pre>
<pre>16		1	112.5</pre>
<pre>32		1	56.25	8	31.25	4	104.6

</pre>
and the picture that speaks 1000 words<br>
<br>
</font><font color="#000000"><img alt=""
 src="jpg3S9LoMqzqS.jpg" height="399"
 width="587"></font><br>
<font color="#000000"><br>
<br>
key top graph black line achieved runtimes<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; top graph red line expected runtimes with perfect scaling
efficency<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bottom graph scaling efficiency<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>
some notes<br>
<br>
<br>
0. data was collected on one of chris's small data sets containing 28
residues not all of which are active for minimisation columns<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; processors &nbsp;&nbsp;&nbsp; - no slave&nbsp; mpi processors <br>
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; min &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; - time for a minimisation of models
m1-m9 with a fixed diffusion tensor<br>
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; eff &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; - approximate parallel efficiency
expected runtime/ actual runtime<br>
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; mc &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; - 256 monte carlo calculations<br>
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; eff &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; - efficiency of the above<br>
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; grid &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; - a grid search on a anisotropic
diffusion tensor 6 steps<br>
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; eff &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; - efficency of the above<br>
&nbsp;&nbsp;&nbsp;&nbsp; tests were run on a cluster of opterons using gigabit ethernet and
mpi<br>
1. these results are crude wall times as measured by pythons time.time
function for the master but they do not include startup and shutdown
overhead<br>
2. these tests are single point measurements there are no statistics<br>
3. timings were rounded to 1 second, so for example we must consider
data points for&nbsp; more than 16 processors for the min run to be suspect<br>
</font><br>
<font color="#000000">key top graph black line achieved runtimes<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; top graph red line expected runtimes with perfect scaling
efficency<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bottom graph scaling efficiency<br>
<br>
note if you watch the output carefully you will see one difference
between the multiprocessor and uniprocessor runs of the grid search.
The grid search reports all cases of the search where the target
function has improved for each processor, rather than for the whole
grid search....<br>
<br>
Bugs missing freatures todos etc:<br>
-------------------------------------<br>
<br>
1. There is very little commenting<br>
2. some exceptions do not stop the interpreter properly and there may
still be some bugs that cause lockups on throwing exceptions&nbsp;&nbsp; <br>
3. there are no unit tests (though the amount of code that can be unit
tested is rather limited as for example writing mock objects for mpi
could be fun!)<br>
4. there are no documentation strings<br>
5. the command line handling need to be improved: we need to find the
current processor implimentation, load it and then ask it what command
line options it needs (this will also allow the simplification of the
handling of setting up the number of processors and allow
multiprocessor that need more command line arguments such as ssh
tunnels to get extra arguments) I will also have to design a way of
getting the help text for all the processor command line options
whether they are loaded or not<br>
6. there are many task comments littered around the code FIXME: TODO:
etc all of these except the ones labelled PY3K: will need to be
reviewed resolved and removed<br>
7. the relax class still has much code for the slave command setup
which needs to be removed as the multi module replaces it<br>
8. The Get_name_command hasn't been tested recently especially across&nbsp;
all of the current processor fabrics<br>
9. there needs to be a way of running the relax system test suite
againnst a list of processor fabrics<br>
10. code to control the use of batched command queueing and returning,
and the threaded output queue&nbsp; has been implimented but hasn't got an
interface to turn it on and off yet<br>
11.&nbsp; the command queuing code has an idea of&nbsp; how many grains there&nbsp;
should be per processor. This isn't under use control&nbsp; at the moment
(the grainyness contols how many batches of commands each processor
should see , take for example 3 slaves and 18 commands with a
grainyness of 1&nbsp; .&nbsp; On the task queue they&nbsp; would be divided up into 3
batched commands one for each processor with each batched command
containing 6 sub commands. With a grainyness of 3 there would be 9&nbsp;
batched commands with each batched command containing 2 commands). This
allow for some load balancing on more hetrogenous systems as the
batched commands are held in a queue and handed out to the slave
processors as the slaves become available.<br>
12. some of the output prefixing has off by 1 errors<br>
13. re segregation of&nbsp; the stdout and&nbsp; stderr streams back out into
their correct streams is not implimented; everything is reported on
stdout. This will require work for the uni_processor as well<br>
14.&nbsp; parellisation of hessian calculations and the all minimisation<br>
</font>15 . it would be good to give users control of which parts of
the program are parallelised during a run<br>
16 . uni processor could be implimented as a s subclass of&nbsp;
multi_processor<br>
17.&nbsp; true virtual classes are not implimented<br>
18.&nbsp; the stdio stream interceptors should be implimented as delegates
to StringIO rather than inheriting from StringIO which would also allow
for the use of cStringIO<br>
19. The master processor only does io and no calculations<br>
<br>
anyway thats it for now<br>
<br>
regards<br>
gary<br>
<br>
<br>
-------------------------------------------------------------------<br>
Dr Gary Thompson<br>
Astbury Centre for Structural Molecular Biology,<br>
University of Leeds, Astbury Building,<br>
Leeds, LS2 9JT, West-Yorkshire, UK Tel. +44-113-3433024<br>
email: <a rel="nofollow" class="moz-txt-link-abbreviated" href="mailto:garyt@xxxxxxxxxxxxxxx">garyt@xxxxxxxxxxxxxxx</a> Fax +44-113-2331407<br>
-------------------------------------------------------------------<br>
<br>


</font></td></tr></table>
<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="00017" href="msg00017.html">The multi-processor branch.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00016" href="msg00016.html">Great work with the multi-processor branch!</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer"></div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Fri May 11 01:21:00 2007</div>  
</body>
</html>
