<!-- MHonArc v2.6.10 -->
<!--X-Subject: Re: [Fwd: Re: multi processing] -->
<!--X-From-R13: "Sqjneq q'Ohiretar" <rqjneqNaze&#45;erynk.pbz> -->
<!--X-Date: Thu, 06 Jul 2006 08:27:11 +0200 -->
<!--X-Message-Id: 7f080ed10607052326y3b91018dq14e16c54fdcf2d3b@mail.gmail.com -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: 44463C74.7050108@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10604222117l7320c264la84902aefe8ec299@mail.gmail.com -->
<!--X-Reference: 4497C69F.6040701@bmb.leeds.ac.uk -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: [Fwd: Re: multi processing] -- July 06, 2006 - 08:27</title>
<link rel="stylesheet" type="text/css" href="/mail.gna.org/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="/mail.gna.org/images/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: [Fwd: Re: multi processing]</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00002" class="tabs">Index by Date</a> | <a href="threads.html#00002" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00001.html">Date Prev</a>] [<a href="msg00003.html">Date Next</a>] [<a href="msg00000.html">Thread Prev</a>] [<a href="msg00003.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Date</em>: Thu, 6 Jul 2006 16:26:32 +1000</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00002.html">7f080ed10607052326y3b91018dq14e16c54fdcf2d3b@mail.gmail.com</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;44463C74.7050108@bmb.leeds.ac.uk&gt;	&lt;7f080ed10604222117l7320c264la84902aefe8ec299@mail.gmail.com&gt;	&lt;4497C69F.6040701@bmb.leeds.ac.uk&gt;</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Edward d'Auvergne</strong> on July 06, 2006 - 08:27:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre style="margin: 0em;">Sorry about the late response, you probably know what writing up is
like!  I've created the 1.3 development branch as the MPI changes are
likely to be very disruptive.  Gary, feel free to create your private
branch by copying the code from the 1.3 line.  You can use a command
such as:</pre><br>
<pre style="margin: 0em;">$ svn cp svn+ssh://varioustoxins@xxxxxxxxxxx/svn/relax/1.3
svn+ssh://varioustoxins@xxxxxxxxxxx/svn/relax/branches/mpi</pre><br>
<pre style="margin: 0em;">to make the branch.  Don't forget to subscribe to the relax-commits
mailing list so that you get emails of each change that is made.  With
the email you will receive a diff of the changes.  By subscribing you
will also receive emails of changes others may make to your branch.
That might occur if you ask for help or if other non-MPI parts of the
code are modified to better sit side-by-side with the MPI code.  For
collaborative development it is very important to be on that list.  I
have a few more points below.</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">&gt;&gt;Thats generally the idea I had, i.e. a fairly course grained approach.
&gt;&gt;My thought was to add constructs to the top level commands (if needed)
&gt;&gt;to allow allow subsets of a set of calculations to be run from a script.
&gt;&gt;i.e. part of a grid search or a few monte carlo runs or a subset of
&gt;&gt;minimisations for a set of residues. Then the real script would generate
&gt;&gt;the required subscripts plus embedded data on the fly. I think this
&gt;&gt;provides a considerable degree of flexibility. Thus for instance our
&gt;&gt;cluster which runs grid engine needs a master script to start all the
&gt;&gt;sub processes rather than a set of separate password less ssh logons
&gt;&gt;which a cluster of workstations would require. In general I thought that
&gt;&gt;catching failures other than a failure to start is not required...
&gt;&gt;
&gt;&gt;
&gt;
&gt;Is your idea similar to having the runs themselves threaded so instead
&gt;of looping over them you run them simultaneously?  I don't know too
&gt;much about clustering.  What is the interface by which data and
&gt;instructions are sent and returned from the nodes?  And do you know if
&gt;there are python wrappings?
&gt;
&gt;</pre><br>
<pre style="margin: 0em;">so the idea is to take the low hanging fruit for the moment and only
parallelise the things that will naturally run for the same amounts of time</pre><br>
<pre style="margin: 0em;">e.g. divide up sets of monte carlo simulations into parts, run
minimisations on subsets of residues that share the same model and
tensor frame etc
</pre></blockquote><pre style="margin: 0em;"><br>That would be the easiest way to start things off.  The MPI overhead
of threading the grid search will probably be too much anyway.
Individual grid searches can be parallelised, however it would be best
not to split up an individual grid search yet.</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">as to how to send data, scripts and results: I would write an interface
class and then allow differnt instances of the class to deal with
communication differently to support different transport mehtods e.g.
ssh logins vs mpi sessions (or something which hasn't been invented yet)
</pre></blockquote><pre style="margin: 0em;"><br>We could set it up so that there is a clear separation of the
threading code, the MPI code, the SSH code, etc.  That way both the
MPI and SSH code can use the same threads, just in a different way.
That may allow an efficient dual, yet separate, implementation of
clustering and grid computing.</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">transfer of data will use cpickles in my case with an mpi backend to
keep compute nodes available to prevent queing problems (you don't want
to resubmit to the batch queue each time you calculate a subpart of the
problem....)
</pre></blockquote><pre style="margin: 0em;"><br>Sound's like a good way to transfer the data to the nodes.  It would
be good to only send the minimal amount of data required.  For example
the data which is rounded up in the function 'self.minimise()' in
'specific_fns/model_free.py' and sent to the model-free minimisation
code in 'maths_fns/'.</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">&gt;&gt;&gt;SSH tunnels is probably not the best option for your system.  Do you
&gt;&gt;&gt;know anything about MPI?
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;I have read about MPI but have not implimented anything __YET__;-). Also
&gt;&gt;I have compiled some MPI based programs. It seems to a bit of a pig and
&gt;&gt;I don't think the low hanging fruit necessarily require that degree of
&gt;&gt;fine grained distribution...
&gt;&gt;
&gt;&gt;
&gt;
&gt;I haven't used MPI either.  There may be much better protocols
&gt;implemented for Python.
&gt;
&gt;</pre><br>
<pre style="margin: 0em;">actually after looking at the problem in our local implementation we
will need mpi and I have the mpi from  from scientific working on my
computer.   However,  as alluded to above mpi will only be a  dependancy
for  a particular transport methods not the overall scheme</pre><br>
<pre style="margin: 0em;">&gt;
&gt;
&gt;&gt;&gt;There are a number of options available for
&gt;&gt;&gt;distributed calculations, but it will need to have a clean and stable
&gt;&gt;&gt;Python interface.
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;obviously a stable interface with as little change to the current top
&gt;&gt;level functions and as little suprise as possible is to be desired. I
&gt;&gt;thought it might be a good idea  to have some form of facade, so  that
&gt;&gt;the various forms of coarse grained multi processing looks the same,
&gt;&gt;whichever one you are using. The idea would be only to have the setup
&gt;&gt;and dispach code different.
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;
&gt;It would probably be best to use some existing standard protocol
&gt;rather than inventing a relax specific system.
&gt;
&gt;</pre><br>
<pre style="margin: 0em;">I think the interface of scripts plus data provides all you need,  the
actual methodology  in the   transport method can be private...</pre><br>
<pre style="margin: 0em;">so for example:</pre><br>
<pre style="margin: 0em;">1. create a clustre with a transport layer</pre><br>
<pre style="margin: 0em;">top level script:</pre><br>
<pre style="margin: 0em;">init_parallel()
                                                   # override relax
commands as needed
</pre></blockquote><pre style="margin: 0em;"><br>It's not overriding, your just selecting a different UI for the MPI
relax instances.  See the new figure 9.1 in the manual as to where the
MPI UI would fit in.</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">cluster= create_cluster(name='test')
                                     # the cluster to use you can have
more than one...
</pre></blockquote><pre style="margin: 0em;"><br>Would that feature be of any use?</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">mpi-transport=create_transport(name='name',method='mpi-local',....)
              # a transport layer all extra keyword arguments are for
configurateion
processor-set=create_processor(transport=mpi-transport,nprocessors=30,...)
     # a particular set of processors using a partuicular transport
method, with a particular weight
cluster_add_processor(processor-set, weight=1.0)
                        # add it to the pool of available processors</pre><br>
</blockquote><pre style="margin: 0em;"><br>All the above could probably be merged into a single simple user
function.  All the hard work can be done behind the scenes.</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">normal relax setup ...</pre><br>
<pre style="margin: 0em;">minimise('newton',run=name,cluster=cluster)
                           # one extra argument
</pre></blockquote><pre style="margin: 0em;"><br>The underlying 'generic' code should be able to determine the selected
UI and make the decisions itself.  The user need not worry about these
things, the less the user has to do the better.</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br>2. internally</pre><br>
<pre style="margin: 0em;">class transport(object):                                            #
just knows howto setup a connection to a bunch ot prosessors and
communicate with them</pre><br>
<pre style="margin: 0em;">  def __init__(self):
    pass</pre><br>
<pre style="margin: 0em;">  def start(self,nprocessors,**kw):                           # setup
for calculation, returns processor-set for this particuar connection</pre><br>
<pre style="margin: 0em;">pass
# kw arguments from create_processor</pre><br>
<pre style="margin: 0em;"> def shutdown(self,aprocessor-set):
         # end all calculations and shutdown
    pass</pre><br>
<pre style="margin: 0em;">  def setupData(self,processor-set,data,nodes=None):                #
send setup data, in my case I would pickle it to an in memory file and
then put it in a
</pre></blockquote><pre style="margin: 0em;"><br>Have a look at section 9.2.3 of the manual in the current sources for
relax object naming conventions.</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;"><br># numpy byte array for transport over numerics mpi layer,  if node is
None send it to everyone
     pass</pre><br>
<pre style="margin: 0em;">  def calculate(self,processor-set,node,script,callback, tag):
                               # run the script on the node x and call
completion callback with tag when complete
    pass</pre><br>
<pre style="margin: 0em;">  def getData(self,processor-set,node=None):
    pass</pre><br>
<pre style="margin: 0em;">  def status(self,processor-set,node=None):
                        # test for status of  a particular calculation
    pass</pre><br>
<pre style="margin: 0em;">  def
cancel(self,processor-set,node=None):
# give up calculation on a particular node
     pass</pre><br>
<pre style="margin: 0em;"><br>class cluster(object):</pre><br>
<pre style="margin: 0em;">   def __init__(self):
     pass</pre><br>
<pre style="margin: 0em;">  def start(self):
      pass</pre><br>
<pre style="margin: 0em;">  def getDivisions(self,nproblems):       # get a list of of size for
'divisions' of  the problems to send to each element of each processor
set based on weights and  number of processors
     pass</pre><br>
<pre style="margin: 0em;">  def shutdown(self):
      pass</pre><br>
<pre style="margin: 0em;"><br>  def setupData(self,data):                             # send setup data
      pass</pre><br>
<pre style="margin: 0em;"><br>  def calculate(self,division,scripts):                           # run
the script on all nodes
    pass</pre><br>
<pre style="margin: 0em;"><br>  def getData(self,division)                                          #
get results
     pass</pre><br>
<pre style="margin: 0em;"><br></pre><br>
<tt>.... anyway i think the idea is fairly clear
</tt></blockquote><pre style="margin: 0em;"><br>It sounds good.  We should determine clean interfaces between the MPI
code, threading code, etc.  Don't be scared about throwing out all the
threading code I've already implemented, it's a bit buggy and starting
from scratch might be best.  Andrew, what do you think about the
threading/MPI+SSH interface idea?</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">&gt;&gt;&gt;Which ever system is decided upon, threading inside
&gt;&gt;&gt;the program will probably be necessary so that each thread can be sent
&gt;&gt;&gt;to a different machine.  This requires calculations which can be
&gt;&gt;&gt;parallelised.  As minimisation is an iterative process with each
&gt;&gt;&gt;iteration requiring the results of the previous, and as it's not the
&gt;&gt;&gt;most CPU intensive part anyway, I can't see too many gains in
&gt;&gt;&gt;modifying that code.
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;Agreed
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;&gt;I've already parallelised the Monte Carlo
&gt;&gt;&gt;simulations for the threading code as those calculations are the most
&gt;&gt;&gt;obvious target.
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;They are a time hog
&gt;&gt;
&gt;&gt;
&gt;
&gt;Grid searching model m8 {S2, tf, S2f, ts, Rex} probably beats the
&gt;total of the MC sims (unless the data is dodgy).
&gt;
&gt;
&gt;
&gt;&gt;&gt;But all residue specific calculations could be
&gt;&gt;&gt;parellelised as well.  This is probably where you can get the best
&gt;&gt;&gt;speed ups.
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;Yes that and grid searches seem obvious candidates
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;
&gt;I was thinking more along the lines of splitting the residues rather
&gt;than the grid search increments.  These increments could be threaded
&gt;however the approach would need to be conservative.  I'm planning on
&gt;eventually splitting out the minimisation code as a separate project
&gt;on Gna! as a Python optimisation library.  The optimisers in Scipy are
&gt;useless!
&gt;
&gt;</pre><br>
<pre style="margin: 0em;">I think whichever divisons are equal and fit the best are what is
required, though residues would be the obvious first candidate followed
by grid steps
</pre></blockquote><pre style="margin: 0em;"><br>I personally think that solely the calls to the code in 'maths_fns'
should be where the split occurs.  For model-free analysis this is
within 'specific_fns/model_free.py' in the 'self.minimise()' function.
  The setup on line 2318 (self.mf = Mf(...)) and then the call to the
code on lines 2360 or 2362 (results = generic_minimise(...)) is the
natural break which should be targeted.</pre><br>
<pre style="margin: 0em;">We should try to write smaller messages, this thread is getting a bit
to fat.  Good luck with the coding Gary.</pre><br>
<pre style="margin: 0em;">Edward</pre><br>
<br>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<!--X-Follow-Ups-End-->
<!--X-References-->
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer"></div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Thu Jul 06 09:00:56 2006</div>  
</body>
</html>
