<!-- MHonArc v2.6.16 -->
<!--X-Subject: Re: The multi&#45;processor branch. -->
<!--X-From-R13: "Sqjneq q'Ohiretar" <rqjneq.qnhiretarNtznvy.pbz> -->
<!--X-Date: Tue, 29 May 2007 10:23:31 +0200 -->
<!--X-Message-Id: 7f080ed10705290122m275f385fx82450871973657a5@mail.gmail.com -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: E1HjkOC&#45;0007zq&#45;7e@subversion.gna.org -->
<!--X-Reference: 463B2E31.30503@bmb.leeds.ac.uk -->
<!--X-Reference: 1178838900.6484.88.camel@localhost -->
<!--X-Reference: 464D5F8B.3010009@bmb.leeds.ac.uk -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: The multi-processor branch. -- May 29, 2007 - 10:23</title>
<link rel="stylesheet" type="text/css" href="/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="https://gna.org/images/gna.theme/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: The multi-processor branch.</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00032" class="tabs">Index by Date</a> | <a href="threads.html#00032" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00031.html">Date Prev</a>] [<a href="msg00033.html">Date Next</a>] [<a href="msg00027.html">Thread Prev</a>] [<a href="msg00033.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: &quot;Gary S. Thompson&quot; &lt;garyt@xxxxxxxxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Date</em>: Tue, 29 May 2007 10:22:51 +0200</li>
<li class="menuitem">
<em>Cc</em>: relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00032.html">7f080ed10705290122m275f385fx82450871973657a5@mail.gmail.com</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;E1HjkOC-0007zq-7e@xxxxxxxxxxxxxxxxxx&gt;	&lt;463B2E31.30503@xxxxxxxxxxxxxxx&gt; &lt;<a href="msg00017.html">1178838900.6484.88.camel@localhost</a>&gt;	&lt;464D5F8B.3010009@xxxxxxxxxxxxxxx&gt;</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Edward d'Auvergne</strong> on May 29, 2007 - 10:23:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre style="margin: 0em;">
On 5/18/07, Gary S. Thompson &lt;garyt@xxxxxxxxxxxxxxx&gt; wrote:
</pre><blockquote class="blockquote"><pre style="margin: 0em;">
Edward d'Auvergne wrote:

&gt;On Fri, 2007-05-04 at 13:59 +0100, Gary S. Thompson wrote:
</pre></blockquote><pre style="margin: 0em;">

[snip]

</pre><blockquote class="blockquote"><pre style="margin: 0em;">
&gt;If using '-np 6', shouldn't the number of slaves be 6?
&gt;
nope there needs to be one processor which is the master and you just
tell  mpi how many prcoessors you want ( I will investigate running jobs
on the master in a thread at some point (maybe never depending ;-)) but
this places extra requirements on the mpi implimentation and is thus a
special case, I can give more details if you want me to)
</pre></blockquote><pre style="margin: 0em;">

I would personally avoid running one of the calculations on the master.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">
&gt;&gt;when running under the threaded and mpi4py implimentations you may see
&gt;&gt;long gaps with no output and the output to the terminal can be quite
&gt;&gt;'jerky'. This is because the multiprcoessor implimentation uses a
&gt;&gt;threaded output queue to decouple the writing of output on the master
&gt;&gt;from the queuing of calculations on the slaves, as otherwise for
&gt;&gt;systems with slow io the rate of io on the mastewr can control the
&gt;&gt;rate of calculation!
&gt;&gt;
&gt;&gt;
&gt;
&gt;I'll have to test this later and see if I can cosmetically minimise the
&gt;jerkyness.
&gt;
&gt;
&gt;
Your can't! Well ok you can but there are 'implications'. The jerkyness
is intrinsic to the  batching up of results from Slave_commands, so you
can switch off the batching of results from the Slave_processors but
this will put more stress on ther master and the interprocessor
communication fabric. If you want to return string results one line at a
time the design also allows you to do this, but again you stress the
master processor and the interprocessor communication fabric so possibly
slowing the overall calculation. Note also that what works well for a
computer with fast interprcoess interconnects will not work well on a
computer with a slow communication fabric. Anyway the message overall is
if you block/slow the master you can end up slowing the whole
multiprocessor....
</pre></blockquote><pre style="margin: 0em;">

I was thinking of storing stdout and stderr in a buffer on the master
and having a thread handle the screen output.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">
&gt;&gt;also note the std error stream is not currently used as race
&gt;&gt;conditions between writing to the  stderr and stdout streams can lead
&gt;&gt;to garbled output.
&gt;&gt;
&gt;&gt;
&gt;
&gt;This will definitely need to be fixed prior to merging into the 1.3
&gt;line.  Stdout and stderr separation is quite important.
&gt;
&gt;
&gt;
&gt;
Indeed this is true and what I intend to do is to reintroduce an output
streem that splits output on the master based on what the lines prefix
is. This is all down to efficiency again, i vcould retuirn each line of
text as it is output to the output stream on the slave, however, so
there are not lots of objects to send between  processors I join the
streams together with the tags for identification of where the lien came
from.. The intention was to give the user the choice to split them again
at the other  end but, I still haven't had time to write that code.
</pre></blockquote><pre style="margin: 0em;">

It's a pity MPI doesn't have the ability to keep the streams separate.
Maybe it would be better if the slave keep its output in a buffer and
then sent it all at once at the end.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">
&gt;&gt;Now some caveats
&gt;&gt;1. not all exceptions can be handled by this mechanism as they
&gt;&gt;exceptions can only be handed back once communication between the
&gt;&gt;slaves has been setup. This can be a problem on some mpi
&gt;&gt;implimentations as they don't provide redirection of stdout back to
&gt;&gt;the master contolling trerminal.
&gt;&gt;
&gt;&gt;
&gt;
&gt;There's probably not much that can be done there.
&gt;
&gt;

yes what I am looking at is putting output to a file one per processor
in this case

 (this won't work in all cases as some clusters don't have disk storage?)
</pre></blockquote><pre style="margin: 0em;">

How about storing stdout and stderr in buffers in memory?


</pre><blockquote class="blockquote"><pre style="margin: 0em;">
&gt;&gt;2. I have had a few cases where raising an exception has wedged the
&gt;&gt;whole multiproessor without any output. These can be quite hard to
&gt;&gt;debug as they are due to errors in the overrides I put on the io
&gt;&gt;streams! a pointer that may help is that  using the
&gt;&gt;sys.settrace(traceit)  as shown in processor.py will produce copious
&gt;&gt;output tracing  (and a very slow program)
&gt;&gt;
&gt;&gt;
&gt;
&gt;The sorting out and separation of the IO streams may cause this problem
&gt;to disappear.
&gt;
&gt;
nope this may be  to do with exceptions being thrown on remote
proceessors and the master processor waiing infinitley long for
communication from dea processors...
</pre></blockquote><pre style="margin: 0em;">

Ah, this will be a significant issue in a grid computing setup where a
person in the building may turn their computer, which is part of the
grid, off!  Fault tolerance is what made my old threading code such a
nightmare.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">
&gt;&gt;in future it maybe possible also parallelise the minimisation of
&gt;&gt;modelfree calculations of the 'all' case where model fitting and the
&gt;&gt;tensor frame are optimised at the same time. However,this will require
&gt;&gt;modifications to the model free hessian gradient and cuntion
&gt;&gt;calculation routines and development of a parallel newton line seach
&gt;&gt;which are both major undertakings.
&gt;&gt;
&gt;&gt;
&gt;
&gt;These are possible targets for parallelisation but I would very strongly
&gt;recommend against working at this position.  And adding optimisation
&gt;algorithms would require very careful testing.  From my experience with
&gt;optimisation in the model-free space, I would probably bet that the
&gt;algorithm will fail for certain model-free motions (not many algorithms
&gt;find all minima in such a convoluted space).  The place to target is the
&gt;following three functions:
&gt;       maths_fns.mf.Mf.func_all()
&gt;       maths_fns.mf.Mf.dfunc_all()
&gt;       maths_fns.mf.Mf.d2func_all()
&gt;
&gt;Specifically the loop over all residues (to be renamed to all spin
&gt;systems in the 1.3 line) to create the value, gradient, and Hessian
&gt;would be the ideal spot to parallelise!
&gt;
&gt;
&gt;
indeed this is what I thought and neil is working on it. One side note is 
that (if such a thing existed) a line search which is  adventitous and looked 
a superset of newton positions would work ;-) again it would have to be 
tested but all such things have to betested to some degree
</pre></blockquote><pre style="margin: 0em;">

See Chapter 4 of my PhD thesis at
<a  rel="nofollow" href="http://eprints.infodiv.unimelb.edu.au/archive/00002799/">http://eprints.infodiv.unimelb.edu.au/archive/00002799/</a> to see why
almost all local optimisation algorithms fail in the model-free space.
Out of 31 algorithms tested, only two worked sufficiently for
model-free analysis - Newton optimisation with the Backtraking step
length algorithm and GMW81 Hessian modification, and simplex
optimisation (which was slower).  Both required constraints using the
Method of Multipliers algorithm, also known as the Augmented
Lagrangian.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">
note are tests for the cases for where lm failed in the test suite for
relax?
</pre></blockquote><pre style="margin: 0em;">

lm failed?  The model-free optimisation system tests are points where
some algorithms struggle.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">
&gt;&gt; Indeed the problem may be fine grained enough that use of c mpi and
&gt;&gt;recoding of the hessian etc calculations for model free in c is
&gt;&gt;required
&gt;&gt;
&gt;&gt;
&gt;
&gt;This conversion should significantly speed up calculations anyway.  I
&gt;will do this one day.
&gt;
&gt;
&gt;
&gt;
the later we do this the better c is a bind. I still think pyrex which
compiles what almost lloks like python to c woould be a good thing to
look at ;-) I might try an prototype something for you to look at at
some point
</pre></blockquote><pre style="margin: 0em;">

I don't like pyrex at all.  I have already tried to use it on the
model-free function code - the output sucked!  Most of the code are
simple functions of less that 5 lines.  The hard part would be to port
'maths_fns/mf.py', which shouldn't be too hard anyway, and the rest
would be very basic.

Cheers,

Edward


</pre>
<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="00000" href="msg00000.html">Re: r3280 - /branches/multi_processor/multi/</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00017" href="msg00017.html">The multi-processor branch.</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00027" href="msg00027.html">Re: The multi-processor branch.</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer">You are on the <a href="http://gna.org">Gna!</a> mail server.</div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Tue May 29 16:20:28 2007</div>  
</body>
</html>
