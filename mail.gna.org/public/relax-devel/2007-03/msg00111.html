<!-- MHonArc v2.6.10 -->
<!--X-Subject: Re: r3237 &#45; in /branches/multi_processor: multi/mpi4py_processor.py	relax -->
<!--X-From-R13: "Unel E. Fubzcfba" <tnelgNozo.yrrqf.np.hx> -->
<!--X-Date: Tue, 20 Mar 2007 15:52:54 +0100 -->
<!--X-Message-Id: 45FFF51D.1010406@bmb.leeds.ac.uk -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: E1HTJY1&#45;0005VF&#45;O1@subversion.gna.org -->
<!--X-Reference: 45FEAC46.8020307@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703190923s4b09fab1vd958ed6001031b9c@mail.gmail.com -->
<!--X-Reference: 45FF1AF7.2000606@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703200306q376f2120va9ab5f5fa49ec6d3@mail.gmail.com -->
<!--X-Reference: 45FFC71E.9040102@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703200601h1a9f94atd1281cc2c82254b7@mail.gmail.com -->
<!--X-Reference: 45FFE881.6050703@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703200717x5b3028f8q5311724600a34de2@mail.gmail.com -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax -- March 20, 2007 - 15:52</title>
<link rel="stylesheet" type="text/css" href="/mail.gna.org/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="/mail.gna.org/images/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00111" class="tabs">Index by Date</a> | <a href="threads.html#00111" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00110.html">Date Prev</a>] [<a href="msg00112.html">Date Next</a>] [<a href="msg00108.html">Thread Prev</a>] [<a href="msg00113.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: Edward d'Auvergne &lt;edward@xxxxxxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Date</em>: Tue, 20 Mar 2007 14:52:13 +0000</li>
<li class="menuitem">
<em>Cc</em>: relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00111.html">45FFF51D.1010406@bmb.leeds.ac.uk</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;E1HTJY1-0005VF-O1@subversion.gna.org&gt;	&lt;<a href="msg00088.html">45FEAC46.8020307@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00091.html">7f080ed10703190923s4b09fab1vd958ed6001031b9c@mail.gmail.com</a>&gt;	&lt;<a href="msg00094.html">45FF1AF7.2000606@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00098.html">7f080ed10703200306q376f2120va9ab5f5fa49ec6d3@mail.gmail.com</a>&gt;	&lt;<a href="msg00101.html">45FFC71E.9040102@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00103.html">7f080ed10703200601h1a9f94atd1281cc2c82254b7@mail.gmail.com</a>&gt;	&lt;<a href="msg00106.html">45FFE881.6050703@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00108.html">7f080ed10703200717x5b3028f8q5311724600a34de2@mail.gmail.com</a>&gt;</li>
<li class="menuitem">
<em>User-agent</em>: Mozilla Thunderbird 1.0 (X11/20041206)</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Gary S. Thompson</strong> on March 20, 2007 - 15:52:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre style="margin: 0em;">Edward d'Auvergne wrote:</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">On 3/21/07, Gary S. Thompson &lt;garyt@xxxxxxxxxxxxxxx&gt; wrote:</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">Edward d'Auvergne wrote:</pre><br>
<pre style="margin: 0em;">&gt; My personal experience with the coding of the grid computing code, I
&gt; would assume that there are a number of differences.  For example the
&gt; algorithm relax currently uses to handle computers of different
&gt; speeds.</pre><br>
<pre style="margin: 0em;">This should not make a difference you just divide more finely and weight
the size of the job by computer (you can even send off the finer grained
task one at a time and then send new ones as tasks are completed... the
same way as you do in the thread code)
</pre></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>The model-free problem places a lower bound on the minimum granularity<br>
size.  That limit is the optimisation instance.  
</tt></blockquote><pre style="margin: 0em;"><br>so an optimisation instance is one of the minimsation instances from  
here?</pre><br>
<pre style="margin: 0em;">There are 4 different classes of model-free model supported by relax.
These are found by the 'determine_param_set_type()' method and are
handled differently by the 'minimise()' method and the 'maths_fns.mf'
module.  The four types are:
  'mf' - one model-free model.
  'local_tm' - one model-free model together with the local tm parameter.
  'diff' - solely the diffusion tensor (the model-free model
parameters are held constant).
  'all' - all model-free models of all spin systems together with
the Brownian rotational diffusion parameters.</pre><br>
<pre style="margin: 0em;">The minimise() method loops over the minimisation instances which for
the four models total n, n, 1, and 1 instances respectively.</pre><br>
<blockquote class="blockquote"><tt>For almost all<br>
minimisation techniques (the grid search excluded) parallelisation is<br>
not possible.  Say you have a calculation which would take 20 min on a<br>
3 GHz machine but it is sent to a 500 MHz old clunker in the basement.<br>
These are serious performance issues in a grid that add up - they<br>
need to be handled properly and not solely by making the calculations<br>
more fine grained.  
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>no I wasn't  suggesting that fine graining is the answer but to allow 
for various speeds of machines the way you do in the thread code you 
have to fine grain more so that a slow mchine can do one work unit in 
the same time it takes a fast machine to do three ..</tt><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">And a couple of those old machines in addition to
the fast ones could significantly speed up model-free analysis.</pre><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">&gt; Grid computing is designed for a heterogeneous environment
&gt; whereas MPI is not.  I'm not saying one is superior to the other but
&gt; that they have different applications in different computing
&gt; environments.</pre><br>
<tt><br>mpi can cope with a hetrogeneous environment as explained above
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>Not well though.  And a likely scenario - a Windows user turning off<br>
their machine when they go home rather than just logging out - is not<br>
something MPI is designed for.  
</tt></blockquote><br>
<blockquote class="blockquote"><tt>It is a tool which can be fitted into<br>
these situations, but in this case it isn't the best tool for the job.<br>
Grid computing is the best tool.  
</tt></blockquote><pre style="margin: 0em;"><br></pre><br>
<tt>Indeed and that is why the framework is designed to integrate a number 
of different possibilities ;-)</tt><br>
<br>
<pre style="margin: 0em;"><br>regards
gary</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">In the situation of a roughly
homogeneous fault-free environment or a cluster, grid computing is not
the best tool, MPI is.</pre><br>
<pre style="margin: 0em;">Cheers,</pre><br>
<pre style="margin: 0em;">Edward</pre><br>
<pre style="margin: 0em;">.</pre><br>
</blockquote><pre style="margin: 0em;"><br></pre><br>
<pre style="margin: 0em;">--
-------------------------------------------------------------------
Dr Gary Thompson
Astbury Centre for Structural Molecular Biology,
University of Leeds, Astbury Building,
Leeds, LS2 9JT, West-Yorkshire, UK             Tel. +44-113-3433024
email: garyt@xxxxxxxxxxxxxxx                   Fax  +44-113-2331407
-------------------------------------------------------------------</pre><br>
<pre style="margin: 0em;"><br></pre><br>
<br>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="00113" href="msg00113.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="00088" href="msg00088.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00091" href="msg00091.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00094" href="msg00094.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00098" href="msg00098.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00101" href="msg00101.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00103" href="msg00103.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00106" href="msg00106.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00108" href="msg00108.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer"></div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Tue Mar 20 16:20:32 2007</div>  
</body>
</html>
