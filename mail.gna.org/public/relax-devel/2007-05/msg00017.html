<!-- MHonArc v2.6.16 -->
<!--X-Subject: The multi&#45;processor branch. -->
<!--X-From-R13: Sqjneq q'Ohiretar <rqjneq.qnhiretarNtznvy.pbz> -->
<!--X-Date: Fri, 11 May 2007 01:16:57 +0200 -->
<!--X-Message-Id: 1178838900.6484.88.camel@localhost -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: E1HjkOC&#45;0007zq&#45;7e@subversion.gna.org -->
<!--X-Reference: 463B2E31.30503@bmb.leeds.ac.uk -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>The multi-processor branch. -- May 11, 2007 - 01:16</title>
<link rel="stylesheet" type="text/css" href="/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="https://gna.org/images/gna.theme/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />The multi-processor branch.</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00017" class="tabs">Index by Date</a> | <a href="threads.html#00017" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00016.html">Date Prev</a>] [<a href="msg00018.html">Date Next</a>] [<a href="msg00031.html">Thread Prev</a>] [<a href="msg00027.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: &quot;Gary S. Thompson&quot; &lt;garyt@xxxxxxxxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Date</em>: Fri, 11 May 2007 01:15:00 +0200</li>
<li class="menuitem">
<em>Cc</em>: relax-devel@xxxxxxx, Tom Burnley &lt;bmbbtb@xxxxxxxxxxxxxxx&gt;,	a.kalverda@xxxxxxxxxxx, Neil Syme &lt;bmbnrs@xxxxxxxxxxx&gt;,	Steve Homans &lt;s.w.homans@xxxxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00017.html">1178838900.6484.88.camel@localhost</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;E1HjkOC-0007zq-7e@xxxxxxxxxxxxxxxxxx&gt;	&lt;463B2E31.30503@xxxxxxxxxxxxxxx&gt;</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Edward d'Auvergne</strong> on May 11, 2007 - 01:16:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre style="margin: 0em;">On Fri, 2007-05-04 at 13:59 +0100, Gary S. Thompson wrote:

[cut]

</pre><blockquote class="blockquote"><pre style="margin: 0em;">output:

the processor implementation gives some  feedback as to what prcoessor
you are running:

M S&gt; script
M S&gt;
M S&gt;
M S&gt;
M S&gt;                                      relax repository checkout
M S&gt;
M S&gt;                           Protein dynamics by NMR relaxation data
analysis
M S&gt;
M S&gt;                              Copyright (C) 2001-2006 Edward
d'Auvergne
M S&gt;
M S&gt; This is free software which you are welcome to modify and
redistribute under the conditions of the
M S&gt; GNU General Public License (GPL).  This program, including all
modules, is licensed under the GPL
M S&gt; and comes with absolutely no warranty.  For details type 'GPL'.
Assistance in using this program
M S&gt; can be accessed by typing 'help'.
M S&gt;
M S&gt; processor = MPI running via mpi4py with 5 slave processors &amp; 1
master, mpi version = 1.2
M S&gt;
M S&gt; script = 'test_small.py'
M S&gt;
----------------------------------------------------------------------------------------------------

</pre></blockquote><pre style="margin: 0em;">If using '-np 6', shouldn't the number of slaves be 6?


</pre><blockquote class="blockquote"><pre style="margin: 0em;">note the processor =  line
</pre></blockquote><pre style="margin: 0em;">

The processor line is in line with the 'script = ...' notation and that
information would be useful if the output is caught and stored in a log
file.  I like that touch.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">another couple of things to note are that the output from the program
is prepended with some text indicating which stream and which
processors the output is coming from: The output prefix is divided
into two parts

'processor' 'stream'&gt;  [normal output line]

where 
processor is either a number to identify the rank of the processor, or
a series of M's to indicate the master
stream is either E or S for the error or output streams

so here is another fragment

1 S&gt; Hessian calls:    0
1 S&gt; Warning:          None
1 S&gt;
M S&gt; idle set set([1, 2])
M S&gt; running_set set([2, 3, 4, 5])
M S&gt;
2 S&gt;
2 S&gt;
2 S&gt; Fitting to residue: 24 ALA
2 S&gt; ~~~~~~~~~~~~~~~~~~~~~~~~~~
2 S&gt;
2 S&gt; Grid search
2 S&gt; ~~~~~~~~~~~
2 S&gt;
2 S&gt; Searching the grid.
2 S&gt; k: 0       xk: array([ 0.


in this case we finish a minimisation on processor 1 '1 S&gt;'
then have some output from the master processor  'M S&gt;'
and then some output from prcoessor 2 '2 S&gt;'
</pre></blockquote><pre style="margin: 0em;">

This output is very useful.  I would prefer though if the notation:

[M S] relax&gt;

rather than:

M S&gt; relax&gt;

The text '1 S&gt;' looks like a prompt when it is not whereas [1 S] is less
ambiguous.

On another note, I strongly believe that for ordinary operation this
output is not necessary.  The user doesn't need to know which slave
process the code has executed on.  All the user will care about is that
the calculation has occurred successfully.  Nevertheless this output is
very useful for programming and debugging purposes.  I propose that it
is shown when the --debug flag is passed to relax and suppressed
otherwise.  Importantly, that way the MPI or threading mode of operation
will look very similar to the normal uni-processor operation.  Or maybe
a verbosity flag to relax could be added to activate this printout?


</pre><blockquote class="blockquote"><pre style="margin: 0em;">when running under the threaded and mpi4py implimentations you may see
long gaps with no output and the output to the terminal can be quite
'jerky'. This is because the multiprcoessor implimentation uses a
threaded output queue to decouple the writing of output on the master
from the queuing of calculations on the slaves, as otherwise for
systems with slow io the rate of io on the mastewr can control the
rate of calculation!
</pre></blockquote><pre style="margin: 0em;">

I'll have to test this later and see if I can cosmetically minimise the
jerkyness.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">also note the std error stream is not currently used as race
conditions between writing to the  stderr and stdout streams can lead
to garbled output.
</pre></blockquote><pre style="margin: 0em;">

This will definitely need to be fixed prior to merging into the 1.3
line.  Stdout and stderr separation is quite important.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">futher note that the implimentation includes a simple timer that gives
some bench marking as to the speed of calculation, this is the total
time that it takes for the master process to run

M S&gt; relax&gt; state.save(file='save', dir=None, force=1,
compress_type=1)
M S&gt; Opening the file 'save.bz2' for writing.
M S&gt;
M S&gt; overall runtime: 0:00:24
</pre></blockquote><pre style="margin: 0em;">

What triggers the time printout?  Does it occur at a specific location?
Does it occur multiple times during execution?


</pre><blockquote class="blockquote"><pre style="margin: 0em;">Interactive terminals: the multi implementation still has an
interactive terminal. Tis maybe started by typing mpiexec -np
6 ../relax --multi mpi4py      for example in the case of an mpi4py
session All io to the treminal takes place on the master processor,
but commands that are parallel still run across the whole cluster.
</pre></blockquote><pre style="margin: 0em;">

Perfect (although I already knew this)!


</pre><blockquote class="blockquote"><pre style="margin: 0em;">Exceptions: exceptions from slave  processors appear with slightly
different stack traces compared to normal exceptions:


Traceback (most recent call last):
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 351, in run
    self.callback.init_master(self)
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/processor.py&quot;, 
line 75, in default_init_master
    self.master.run()
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/relax_tests_chris/../relax&quot;,
 line 177, in run
    self.interpreter.run()
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/prompt/interpreter.py&quot;,
 line 216, in run
    run_script(intro=self.relax.intro_string, local=self.local,
script_file=self.relax.script_file, quit=1)
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/prompt/interpreter.py&quot;,
 line 392, in run_script
    console.interact(intro, local, script_file, quit)
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/prompt/interpreter.py&quot;,
 line 343, in interact_script
    execfile(script_file, local)
  File &quot;test_small.py&quot;, line 54, in ?
    grid_search(name, inc=11)
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/prompt/minimisation.py&quot;,
 line 147, in grid_search
    self.relax.processor.run_queue()
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 270, in run_queue
    self.run_command_queue(lqueue)
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 335, in run_command_queue
    result_queue.put(result)
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 109, in put
    super(Threaded_result_queue,self).put(job)
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 76, in put
    self.processor.process_result(job)
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 221, in process_result
    result.run(self,memo)
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/processor.py&quot;, 
line 276, in run
    raise self.exception
Capturing_exception:

------------------------------------------------------------------------------------------------------------------------

  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/multi_processor.py&quot;,
 line 381, in run
    command.run(self,completed)
  File
&quot;/nmr/jessy/garyt/projects/relax_branch/branch_multi1/multi/commands.py&quot;, 
line 297, in run
    raise 'dummy'


Nested Exception from sub processor
Rank: 1  Name: fbsdpcu156-pid31522
Exception type: dummy (legacy string exception)
Message: dummy

------------------------------------------------------------------------------------------------------------------------


here we have an exception 'dummy' which was raised at line 297, in the
run function /multi/commands.py on  slave 1 processor node  fbsdpcu156
process id 31522 and transferred back to line 276 of  function run in
multi/processor.py on the master where it was raised again. 
</pre></blockquote><pre style="margin: 0em;">

I think that the error printout should be made to resemble the standard
Python printout.  For example if you raise a RelaxError with the text
'hello', the printout could look like:

    raise RelaxError, 'hello'
Nested Exception from sub processor
Rank: 1  Name: fbsdpcu156-pid31522
RelaxError: hello

Hence the exception type is not separated from its message.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">Now some caveats 
1. not all exceptions can be handled by this mechanism as they
exceptions can only be handed back once communication between the
slaves has been setup. This can be a problem on some mpi
implimentations as they don't provide redirection of stdout back to
the master contolling trerminal.
</pre></blockquote><pre style="margin: 0em;">

There's probably not much that can be done there.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">2. I have had a few cases where raising an exception has wedged the
whole multiproessor without any output. These can be quite hard to
debug as they are due to errors in the overrides I put on the io
streams! a pointer that may help is that  using the
sys.settrace(traceit)  as shown in processor.py will produce copious
output tracing  (and a very slow program)
</pre></blockquote><pre style="margin: 0em;">

The sorting out and separation of the IO streams may cause this problem
to disappear.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">3. not all exception states seem to be leading to an exit from the
program currently so you should monitor output from the program
carefully
</pre></blockquote><pre style="margin: 0em;">

Do you know why this is happening?


</pre><blockquote class="blockquote"><pre style="margin: 0em;">Speedups
-----------

the following calculations are currently parallelised

1. model free minimisations across sets of residues with a  fixed
difffusion tensor frame 
2. model free grid searches for the difffusion tensor frame
3. monte carlo simulations
</pre></blockquote><pre style="margin: 0em;">

This is great work!


</pre><blockquote class="blockquote"><pre style="margin: 0em;">in future it maybe possible also parallelise the minimisation of
modelfree calculations of the 'all' case where model fitting and the
tensor frame are optimised at the same time. However,this will require
modifications to the model free hessian gradient and cuntion
calculation routines and development of a parallel newton line seach
which are both major undertakings.
</pre></blockquote><pre style="margin: 0em;">

These are possible targets for parallelisation but I would very strongly
recommend against working at this position.  And adding optimisation
algorithms would require very careful testing.  From my experience with
optimisation in the model-free space, I would probably bet that the
algorithm will fail for certain model-free motions (not many algorithms
find all minima in such a convoluted space).  The place to target is the
following three functions:
        maths_fns.mf.Mf.func_all()
        maths_fns.mf.Mf.dfunc_all()
        maths_fns.mf.Mf.d2func_all()

Specifically the loop over all residues (to be renamed to all spin
systems in the 1.3 line) to create the value, gradient, and Hessian
would be the ideal spot to parallelise!


</pre><blockquote class="blockquote"><pre style="margin: 0em;"> Indeed the problem may be fine grained enough that use of c mpi and
recoding of the hessian etc calculations for model free in c is
required
</pre></blockquote><pre style="margin: 0em;">

This conversion should significantly speed up calculations anyway.  I
will do this one day.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">speedups on all calculations with increasing numbers of processors
should be near perfect as alluded to in message
<a  rel="nofollow" href="https://mail.gna.org/public/relax-devel/2007-04/msg00048.html">https://mail.gna.org/public/relax-devel/2007-04/msg00048.html</a> more
benchmarks will follow soon

processors            min     eff     mc      eff     grid    eff
1             18      100     80      100     134     100
2             9       100
4             5       90
8             3       75
16            1       112.5
32            1       56.25   8       31.25   4       104.6

and the picture that speaks 1000 words

processors            min     eff     mc      eff     grid    eff
1             18      100     80      100     134     100
2             9       100
4             5       90
8             3       75
16            1       112.5
32            1       56.25   8       31.25   4       104.6

and the picture that speaks 1000 words




key top graph black line achieved runtimes
        top graph red line expected runtimes with perfect scaling
efficency
        bottom graph scaling efficiency
        
some notes


0. data was collected on one of chris's small data sets containing 28
residues not all of which are active for minimisation columns
        processors     - no slave  mpi processors 
        min                    - time for a minimisation of models
m1-m9 with a fixed diffusion tensor
        eff                     - approximate parallel efficiency
expected runtime/ actual runtime
        mc                     - 256 monte carlo calculations
        eff                     - efficiency of the above
        grid                   - a grid search on a anisotropic
diffusion tensor 6 steps
        eff                     - efficency of the above
     tests were run on a cluster of opterons using gigabit ethernet
and mpi
1. these results are crude wall times as measured by pythons time.time
function for the master but they do not include startup and shutdown
overhead
2. these tests are single point measurements there are no statistics
3. timings were rounded to 1 second, so for example we must consider
data points for  more than 16 processors for the min run to be suspect

key top graph black line achieved runtimes
        top graph red line expected runtimes with perfect scaling
efficency
        bottom graph scaling efficiency

note if you watch the output carefully you will see one difference
between the multiprocessor and uniprocessor runs of the grid search.
The grid search reports all cases of the search where the target
function has improved for each processor, rather than for the whole
grid search....
</pre></blockquote><pre style="margin: 0em;">

I wonder what is causing the Monte Carlo simulations to not be 100%
efficient.  Of all the code in relax, I would see this as the most
amenable for parallelisation.  Each simulation can be queued to a
different slave.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">Bugs missing freatures todos etc:
-------------------------------------

1. There is very little commenting
</pre></blockquote><pre style="margin: 0em;">

This is quite important for the future maintainability of the code.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">2. some exceptions do not stop the interpreter properly and there may
still be some bugs that cause lockups on throwing exceptions
</pre></blockquote><pre style="margin: 0em;">

Yikes.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">3. there are no unit tests (though the amount of code that can be unit
tested is rather limited as for example writing mock objects for mpi
could be fun!)
</pre></blockquote><pre style="margin: 0em;">

Most parts of the code could be tested relatively easily.  The unit test
framework makes this job quite easy.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">4. there are no documentation strings
</pre></blockquote><pre style="margin: 0em;">

I would recommend compiling the API documentation using scons and then
looking at the HTML output.  That output should very clearly show what
docstrings are missing or deficient.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">5. the command line handling need to be improved: we need to find the
current processor implimentation, load it and then ask it what command
line options it needs (this will also allow the simplification of the
handling of setting up the number of processors and allow
multiprocessor that need more command line arguments such as ssh
tunnels to get extra arguments) I will also have to design a way of
getting the help text for all the processor command line options
whether they are loaded or not
</pre></blockquote><pre style="margin: 0em;">

I don't follow.  The user shouldn't be asked a question by relax.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">6. there are many task comments littered around the code FIXME: TODO:
etc all of these except the ones labelled PY3K: will need to be
reviewed resolved and removed
7. the relax class still has much code for the slave command setup
which needs to be removed as the multi module replaces it
8. The Get_name_command hasn't been tested recently especially across
all of the current processor fabrics
9. there needs to be a way of running the relax system test suite
againnst a list of processor fabrics
</pre></blockquote><pre style="margin: 0em;">

I disagree.  The unit tests should test all functions (except those
requiring missing dependancies).  The system/functional tests should be
run in the current mode of operation.  Therefore to test the operation
of relax against the different processor fabrics you would run the test
suite multiple times in those different modes.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">10. code to control the use of batched command queueing and returning,
and the threaded output queue  has been implimented but hasn't got an
interface to turn it on and off yet
11.  the command queuing code has an idea of  how many grains there
should be per processor. This isn't under use control  at the moment
(the grainyness contols how many batches of commands each processor
should see , take for example 3 slaves and 18 commands with a
grainyness of 1  .  On the task queue they  would be divided up into 3
batched commands one for each processor with each batched command
containing 6 sub commands. With a grainyness of 3 there would be 9
batched commands with each batched command containing 2 commands).
This allow for some load balancing on more hetrogenous systems as the
batched commands are held in a queue and handed out to the slave
processors as the slaves become available.
12. some of the output prefixing has off by 1 errors
13. re segregation of  the stdout and  stderr streams back out into
their correct streams is not implimented; everything is reported on
stdout. This will require work for the uni_processor as well
</pre></blockquote><pre style="margin: 0em;">

I think that the two should never be combined.  This makes it
challenging with the treading, but it is doable (I did it with my
ancient threading code).


</pre><blockquote class="blockquote"><pre style="margin: 0em;">14.  parellisation of hessian calculations and the all minimisation
</pre></blockquote><pre style="margin: 0em;">

See above.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">15 . it would be good to give users control of which parts of the
program are parallelised during a run
</pre></blockquote><pre style="margin: 0em;">

I don't know if this is important or useful from the user's perspective.


</pre><blockquote class="blockquote"><pre style="margin: 0em;">16 . uni processor could be implimented as a s subclass of
multi_processor
17.  true virtual classes are not implimented
</pre></blockquote><pre style="margin: 0em;">

What is a virtual class?


</pre><blockquote class="blockquote"><pre style="margin: 0em;">18.  the stdio stream interceptors should be implimented as delegates
to StringIO rather than inheriting from StringIO which would also
allow for the use of cStringIO
19. The master processor only does io and no calculations
</pre></blockquote><pre style="margin: 0em;">

Is that not how it currently works?


</pre><blockquote class="blockquote"><pre style="margin: 0em;">anyway thats it for now
</pre></blockquote><pre style="margin: 0em;">

Again I have to say that this code is looking really good.  This should
be released as a proper relax release.  I would look at the instructions
for creating a relax release.  A tag needs to be created and the source
and binary packages created.  For that you will need to create a GPG key
pair specifically for relax and then send me your public key.  Then you
will be able to package and upload signed files to the download site.  I
can then check the packages and sign them with the relax key.

Cheers,

Edward



</pre>
<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="00027" href="msg00027.html">Re: The multi-processor branch.</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="00000" href="msg00000.html">Re: r3280 - /branches/multi_processor/multi/</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer">You are on the <a href="http://gna.org">Gna!</a> mail server.</div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Sat May 26 19:00:36 2007</div>  
</body>
</html>
