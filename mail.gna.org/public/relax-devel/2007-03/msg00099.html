<!-- MHonArc v2.6.10 -->
<!--X-Subject: Re: r3237 &#45; in /branches/multi_processor: multi/mpi4py_processor.py	relax -->
<!--X-From-R13: "Sqjneq q'Ohiretar" <rqjneqNaze&#45;erynk.pbz> -->
<!--X-Date: Tue, 20 Mar 2007 11:23:35 +0100 -->
<!--X-Message-Id: 7f080ed10703200322ua51bfcexc4e26556a623a0d0@mail.gmail.com -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: E1HTJY1&#45;0005VF&#45;O1@subversion.gna.org -->
<!--X-Reference: 45FEAC46.8020307@bmb.leeds.ac.uk -->
<!--X-Reference: 7f080ed10703190923s4b09fab1vd958ed6001031b9c@mail.gmail.com -->
<!--X-Reference: 45FF1AF7.2000606@bmb.leeds.ac.uk -->
<!--X-Head-End-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax -- March 20, 2007 - 11:23</title>
<link rel="stylesheet" type="text/css" href="/mail.gna.org/archives-color-gna.css"> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<h2><img src="/mail.gna.org/images/mail.orig.png" width="48" height="48"
alt="mail" class="pageicon" />Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</h2>
<br />
<div class="topmenu">
<a href="../" class="tabs">Others Months</a> | <a href="index.html#00099" class="tabs">Index by Date</a> | <a href="threads.html#00099" class="tabs">Thread Index</a><br />
<span class="smaller">&gt;&gt;&nbsp;&nbsp;
[<a href="msg00098.html">Date Prev</a>] [<a href="msg00100.html">Date Next</a>] [<a href="msg00113.html">Thread Prev</a>] [<a href="msg00100.html">Thread Next</a>]
</div>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h3><a name="header" href="#header">Header</a></h3>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul class="headdata">
<li class="menuitem">
<em>To</em>: &quot;Gary S. Thompson&quot; &lt;garyt@xxxxxxxxxxxxxxx&gt;</li>
<li class="menuitem">
<em>Date</em>: Tue, 20 Mar 2007 21:22:45 +1100</li>
<li class="menuitem">
<em>Cc</em>: relax-devel@xxxxxxx</li>
<li class="menuitem">
<em>Dkim-signature</em>: a=rsa-sha1; c=relaxed/relaxed; d=gmail.com; s=beta;	h=domainkey-signature:received:received:message-id:date:from:sender:to:subject:cc:in-reply-to:mime-version:content-type:content-transfer-encoding:content-disposition:references:x-google-sender-auth;	b=gCixyo/CbpnaukzMlbg2pBRw5LhJ2YmilmQ9EWybhVX3Ew0Xg1tU7+F/lZl6Dik1Qq0A/E94mIybbbGkstd7EI6KQDxqmMw1GmRQrU+nM2wRTTHL0OfRGZC5KpH91I+wvPivCatIr4ksgkm26TLCS7bQu7nhYVXZnQiCL6Hq8A8=</li>
<li class="menuitem">
<em>Message-id</em>: &lt;<a href="msg00099.html">7f080ed10703200322ua51bfcexc4e26556a623a0d0@mail.gmail.com</a>&gt;</li>
<li class="menuitem">
<em>References</em>: &lt;E1HTJY1-0005VF-O1@subversion.gna.org&gt;	&lt;<a href="msg00088.html">45FEAC46.8020307@bmb.leeds.ac.uk</a>&gt;	&lt;<a href="msg00091.html">7f080ed10703190923s4b09fab1vd958ed6001031b9c@mail.gmail.com</a>&gt;	&lt;<a href="msg00094.html">45FF1AF7.2000606@bmb.leeds.ac.uk</a>&gt;</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
</div><!-- end headdata -->
<br />
<h3><a name="content" href="#content">Content</a></h3>
<div class="postedby">Posted by <strong>Edward d'Auvergne</strong> on March 20, 2007 - 11:23:</div>
<div class="msgdata">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<tt>On 3/20/07, Gary S. Thompson &lt;garyt@xxxxxxxxxxxxxxx&gt; wrote:
</tt><blockquote class="blockquote"><pre style="margin: 0em;">Edward d'Auvergne wrote:</pre><br>
<pre style="margin: 0em;">&gt; Hi,
&gt;
&gt; Just a quick point, it would be good to either start a new thread for
&gt; these types of questions or changing the subject field (unless you use
&gt; the Gmail web interface like I am at the moment).  People may miss
&gt; important discussions with such scary subject lines!
&gt;
&gt;
&gt; On 3/20/07, Gary S. Thompson &lt;garyt@xxxxxxxxxxxxxxx&gt; wrote:
&gt;
&gt;&gt; garyt@xxxxxxxxxxxxxxx wrote:
</pre></blockquote><pre style="margin: 0em;"><br>[snip]</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">&gt;&gt; Now a question what is the best way to get an etrenally running relax
&gt;&gt; interpreter I can just fire commands at (for the slaves)?
&gt;
&gt;
&gt; The prompt based interface (as well as the script interface) is only
&gt; one way of invoking relax.  An important question is how should we
&gt; present relax to the user when using MPI.  Should the parent process
&gt; present a functional interpreter or should operation be solely
&gt; dictated by a script?</pre><br>
<pre style="margin: 0em;"><br>I already have a prompt running on the master, my idea is that the relax
use should see no difference (apart from perfomance) when using he
parallel version
</pre></blockquote><pre style="margin: 0em;"><br>I've just played around with that and it does look like the best
option for user flexibility.</pre><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">&gt; Or should a completely different mechanism of
&gt; operation be devised for the interface of the parent.  For the grid
&gt; computing code the parent UI is either the prompt or the script while
&gt; the slaves use the interface started by 'relax --thread'.  The slave
&gt; use none of the user functions and only really invoke the number
&gt; crunching code.</pre><br>
<pre style="margin: 0em;">now this is what I couldn't follow. How much of the relax environment is
essential for a slave/thread and if we don't want to do it all by
compete pickles of relax data structures (how big is a complete relax
data structure for a typical set of runs?) where shoudl i start
</pre></blockquote><pre style="margin: 0em;"><br>My post at <a  href="https://mail.gna.org/public/relax-devel/2007-03/msg00097.html">https://mail.gna.org/public/relax-devel/2007-03/msg00097.html</a>
(Message-id: &lt;1174384147.29205.20.camel@xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&gt;)
hopefully answers these questions.</pre><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">&gt; For the MPI slaves (I'm assuming these are separate processes with
&gt; different PIDs running on different nodes) we should avoid the
&gt; standard UI interfaces as these are superfluous.  In this case a
&gt; simple MPI interface should probably be devised - it accepts the MPI
&gt; commands and returns data back to the parent.  Is this though stdin,
&gt; stdout, and stderr in MPI?  My knowledge of MPI is very limited.
&gt;</pre><br>
<pre style="margin: 0em;"><br>1. via picked objects  which will integrate the results back into the
master copy or
</pre></blockquote><pre style="margin: 0em;"><br>If implemented at the 'minimise_mpi()' model-free method, then this
would probably be the best option.</pre><br>
<blockquote class="blockquote"><pre style="margin: 0em;">2. as text strings which are printed with a  process number in front (so
that  complete parallel  log files can be grepped out of the main output).
</pre></blockquote><pre style="margin: 0em;"><br>However if you see advantages with this option, then maybe this is better.</pre><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;"> I don't currently assume that the compute node has available or useable
or shared  disk space.  everything comes back to the master
</pre></blockquote><pre style="margin: 0em;"><br>I expected MPI to operate in this way.  This means that none of the
grid computing code will be of use to you.</pre><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">the place to look at in mpi4py_processor is</pre><br>
<pre style="margin: 0em;">lines 59-71  which sends a command from the master and either prints or
exceutes the resulting object (checks for exceptions, repeated feedback,
and command completion still to come)</pre><br>
<pre style="margin: 0em;">        for i in range(1,MPI.size):
            if i != 0:
                MPI.COMM_WORLD.Send(buf=command,dest=i)
        for i in range(1,MPI.size):
            buf=[]
            if i !=0:
                elem = MPI.COMM_WORLD.Recv(source=i)
                if type(elem) == 'object':
                    elem.run(relax_instance, relax_instance.processor)
                else:
                    #FIXME can't cope with multiple lines
                    print i,elem</pre><br>
<pre style="margin: 0em;">and lines 92-94 where all the command are recieved and executes on the
slaves</pre><br>
<pre style="margin: 0em;">         while not self.do_quit:
                command = MPI.COMM_WORLD.Recv(source=0)
                command.run(self.relax_instance,
self.relax_instance.processor)</pre><br>
<pre style="margin: 0em;">it is the intention that the command protocol is very minimal</pre><br>
<pre style="margin: 0em;">0. send a command plus data to run on the remote slave as an object
which will get its run method executed with the local relax and
processor as aruments master:communicator.run_command
</pre></blockquote><pre style="margin: 0em;"><br>Maybe the run method should be part of the 'minimise()' model-free method?</pre><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">1. the slave then writes a series of objects to the processsor method
return_object
</pre></blockquote><pre style="margin: 0em;"><br>The model-free parameter vector and optimisation stats?</pre><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">2. master recieves data either
   a. objects to execute on the master which will also be given the
master relax and processor instances
</pre></blockquote><pre style="margin: 0em;"><br>Unpack the minimisation data and place it into the appropriate
location in the relax data storage object?</pre><br>
<br>
<blockquote class="blockquote"><tt>   b.string objects to print
</tt></blockquote><pre style="margin: 0em;"><br>The optimisation print outs?</pre><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">   c. a command completion indicator back from the slave (a well known
object)
</pre></blockquote><pre style="margin: 0em;"><br>Does the slave then die?</pre><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">   d. an exception (raising of a local exception ont master which will
do stack trace printing for the master and the slave)
</pre></blockquote><pre style="margin: 0em;"><br>Ah, I didn't think of that one!</pre><br>
<br>
<blockquote class="blockquote"><tt>   e. None  a void  return
</tt></blockquote><pre style="margin: 0em;"><br>???</pre><br>
<br>
<blockquote class="blockquote"><pre style="margin: 0em;">&gt;&gt; do i modify interpreter.run to take a quit varaiable set to False  so
&gt;&gt; that run_script can be
&gt;&gt; run with quit = false?
&gt;
&gt;
&gt; Avoid the interpreter and wait at the optimisation steps - the only
&gt; serious number crunching code in relax.</pre><br>
<pre style="margin: 0em;"><br>I agree!  interpeters are not required on the slave just the relax data
structures in a clean and useable state
</pre></blockquote><pre style="margin: 0em;"><br>If you're working at the level of the model-free 'minimise()'
function, don't bother with the relax data structures!  See my
previous post mentioned above.</pre><br>
<br>
<blockquote class="blockquote"><tt>One other quetion: how well behaved are the relax functions with not gratuitously modifying global state. e.g could I share one relax instance between several threads? The reason I ask is that in the case that they are well behaved  many of the data transfer operations in a threaded environment with a single  memory space would become noops ;-) nice!
</tt></blockquote><pre style="margin: 0em;"><br>I wouldn't share the state.  Again if you work at the 'minimise()'
model-free method level, copying it and renaming it to
'minimise_mpi()', that new function could be made to not touch the
relax data storage object.  Maybe there should be a
'minimise_mpi_master()' that contains the setup code and a
'minimise_mpi_slave()' which contains the optimisation code and the
unpacking code.  This should be very simple to copy and modify from
the current code!</pre><br>
<pre style="margin: 0em;">Cheers,</pre><br>
<pre style="margin: 0em;">Edward</pre><br>
<br>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div><!-- end msgdata -->
<br />
<h3><a name="related" href="#related">Related Messages</a></h3>
<div class="relateddata">
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="00100" href="msg00100.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="00088" href="msg00088.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
<li><strong><a name="00091" href="msg00091.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Edward d'Auvergne</li></ul></li>
<li><strong><a name="00094" href="msg00094.html">Re: r3237 - in /branches/multi_processor: multi/mpi4py_processor.py	relax</a></strong>
<ul><li><em>From:</em> Gary S. Thompson</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
</div><!-- end relateddata -->
<!-- NoBotLinksApartFromRelatedMessages -->

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
<div class="footer"></div><br />
<div class="right">Powered by <a href="http://www.mhonarc.org">MHonArc</a>, Updated Tue Mar 20 16:20:32 2007</div>  
</body>
</html>
